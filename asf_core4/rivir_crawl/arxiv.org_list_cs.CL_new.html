<!DOCTYPE html>
<html lang="en">

<head>  <title>Computation and Language  </title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="apple-touch-icon" sizes="180x180" href="/static/browse/0.3.4/images/icons/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/static/browse/0.3.4/images/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/static/browse/0.3.4/images/icons/favicon-16x16.png">
  <link rel="manifest" href="/static/browse/0.3.4/images/icons/site.webmanifest">
  <link rel="mask-icon" href="/static/browse/0.3.4/images/icons/safari-pinned-tab.svg" color="#5bbad5">
  <meta name="msapplication-TileColor" content="#da532c">
  <meta name="theme-color" content="#ffffff">
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/arXiv.css?v=20241206" />
  <link rel="stylesheet" type="text/css" media="print" href="/static/browse/0.3.4/css/arXiv-print.css?v=20200611" />
  <link rel="stylesheet" type="text/css" media="screen" href="/static/browse/0.3.4/css/browse_search.css" />
  <script language="javascript" src="/static/browse/0.3.4/js/accordion.js" ></script>
  <script language="javascript" src="/static/browse/0.3.4/js/optin-modal.js?v=20250819"></script>
  
  <script src="/static/browse/0.3.4/js/mathjaxToggle.min.js" type="text/javascript"></script>
  <script type="text/javascript" language="javascript">mathjaxToggle();</script>
</head>

<body  class="with-cu-identity">
  
  
  <div class="flex-wrap-footer">
    <header>
      <a href="#content" class="is-sr-only">Skip to main content</a>
      <!-- start desktop header -->
      <div class="columns is-vcentered is-hidden-mobile" id="cu-identity">
        <div class="column" id="cu-logo">
          <a href="https://www.cornell.edu/"><img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University" /></a>
        </div><!-- /from April 7 at 1:00 AM to May 29 at 21:40 --><!-- /from May 2 at 1:00 AM to May 5 at 9:45 AM --><div class="column" id="support-ack">
          <span id="support-ack-url">We gratefully acknowledge support from the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors.</span>
          <a href="https://info.arxiv.org/about/donate.html" class="btn-header-donate">Donate</a>
        </div>
      </div>

      <div id="header" class="is-hidden-mobile">
<a aria-hidden="true" tabindex="-1" href="/IgnoreMe"></a>
<div class="header-breadcrumbs">
  <a href="/"><img src="/static/browse/0.3.4/images/arxiv-logo-one-color-white.svg" alt="arxiv logo" style="height:40px;"/></a> <span>&gt;</span>
  <a href="/list/cs.CL/recent">cs.CL</a>
  </div>

        <div class="columns is-vcentered is-mobile" style="justify-content: flex-end;">
        </div>

          <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
     </div><!-- /end desktop header -->

      <div class="mobile-header">
        <div class="columns is-mobile">
          <div class="column logo-arxiv"><a href="https://arxiv.org/"><img src="/static/browse/0.3.4/images/arxiv-logomark-small-white.svg" alt="arXiv logo" style="height:60px;" /></a></div>
          <div class="column logo-cornell"><a href="https://www.cornell.edu/">
            <picture>
              <source media="(min-width: 501px)"
                srcset="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg  400w"
                sizes="400w" />
              <source srcset="/static/browse/0.3.4/images/icons/cu/cornell_seal_simple_black.svg 2x" />
              <img src="/static/browse/0.3.4/images/icons/cu/cornell-reduced-white-SMALL.svg" alt="Cornell University Logo" />
            </picture>
          </a></div>
          <div class="column nav" id="toggle-container" role="menubar">
            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-white"><title>open search</title><path d="M505 442.7L405.3 343c-4.5-4.5-10.6-7-17-7H372c27.6-35.3 44-79.7 44-128C416 93.1 322.9 0 208 0S0 93.1 0 208s93.1 208 208 208c48.3 0 92.7-16.4 128-44v16.3c0 6.4 2.5 12.5 7 17l99.7 99.7c9.4 9.4 24.6 9.4 33.9 0l28.3-28.3c9.4-9.4 9.4-24.6.1-34zM208 336c-70.7 0-128-57.2-128-128 0-70.7 57.2-128 128-128 70.7 0 128 57.2 128 128 0 70.7-57.2 128-128 128z"/></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <form class="mobile-search-form" method="GET" action="https://arxiv.org/search">
                <div class="field has-addons">
                  <input class="input" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
                  <input type="hidden" name="source" value="header">
                  <input type="hidden" name="searchtype" value="all">
                  <button class="button">GO</button>
                </div>
              </form>
            </div>

            <button class="toggle-control"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-white" role="menu"><title>open navigation menu</title><path d="M16 132h416c8.837 0 16-7.163 16-16V76c0-8.837-7.163-16-16-16H16C7.163 60 0 67.163 0 76v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16zm0 160h416c8.837 0 16-7.163 16-16v-40c0-8.837-7.163-16-16-16H16c-8.837 0-16 7.163-16 16v40c0 8.837 7.163 16 16 16z"/ ></svg></button>
            <div class="mobile-toggle-block toggle-target">
              <nav class="mobile-menu" aria-labelledby="mobilemenulabel">
                <h2 id="mobilemenulabel">quick links</h2>
                <ul>
                    <li><a href="https://arxiv.org/login">Login</a></li>
                    <li><a href="https://info.arxiv.org/help">Help Pages</a></li>
                    <li><a href="https://info.arxiv.org/about">About</a></li>
                </ul>
              </nav>
            </div>
          </div>
        </div>
      </div><!-- /end mobile-header -->
    </header>

    <main>
      <div id="content">
<div id='content-inner'>
<div id='dlpage'>
  <h1>Computation and Language</h1>

<ul>
  <li><a href="#item0">New submissions</a></li>
  <li><a href="#item67">Cross-lists</a></li>
  <li><a href="#item88">Replacements</a></li>
</ul>

<p>See <a id="recent-cs.CL" aria-labelledby="recent-cs.CL" href="/list/cs.CL/recent">recent</a> articles</p>
<h3>Showing new listings for Thursday, 1 January 2026</h3>

      <div class='paging'>Total of 145 entries 
    </div>
    <div class='morefewer'>Showing up to 2000 entries per page:
          <a href=/list/cs.CL/new?skip=0&amp;show=1000 rel="nofollow">
      fewer</a>
 |
          <span style="color: #454545">more</span>
 |
          <span style="color: #454545">all</span>

    </div>




      <dl id='articles'>
    <h3>New submissions (showing 66 of 66 entries)</h3>


    <dt>
      <a name='item1'>[1]</a>
      <a href ="/abs/2512.23710" title="Abstract" id="2512.23710">
        arXiv:2512.23710
      </a>
      
        [<a href="/pdf/2512.23710" title="Download PDF" id="pdf-2512.23710" aria-labelledby="pdf-2512.23710">pdf</a>, <a href="https://arxiv.org/html/2512.23710v1" title="View HTML" id="html-2512.23710" aria-labelledby="html-2512.23710" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23710" title="Other formats" id="oth-2512.23710" aria-labelledby="oth-2512.23710">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Enriching Historical Records: An OCR and AI-Driven Approach for Database Integration
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abedi,+Z">Zahra Abedi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=van+Dijk,+R+M">Richard M.K. van Dijk</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wijnholds,+G">Gijs Wijnholds</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Verhoef,+T">Tessa Verhoef</a></div>


        <div class='list-journal-ref'><span class='descriptor'>Journal-ref:</span>
          Computational Linguistics in the Netherlands Journal 14 (2025) 401-420
        </div>

        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          This research digitizes and analyzes the Leidse hoogleraren en lectoren 1575-1815 books written between 1983 and 1985, which contain biographic data about professors and curators of Leiden University. It addresses the central question: how can we design an automated pipeline that integrates OCR, LLM-based interpretation, and database linking to harmonize data from historical document images with existing high-quality database records? We applied OCR techniques, generative AI decoding constraints that structure data extraction, and database linkage methods to process typewritten historical records into a digital format. OCR achieved a Character Error Rate (CER) of 1.08 percent and a Word Error Rate (WER) of 5.06 percent, while JSON extraction from OCR text achieved an average accuracy of 63 percent and, based on annotated OCR, 65 percent. This indicates that generative AI somewhat corrects low OCR performance. Our record linkage algorithm linked annotated JSON files with 94% accuracy and OCR-derived JSON files with 81%. This study contributes to digital humanities research by offering an automated pipeline for interpreting digitized historical documents, addressing challenges like layout variability and terminology differences, and exploring the applicability and strength of an advanced generative AI model.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item2'>[2]</a>
      <a href ="/abs/2512.23711" title="Abstract" id="2512.23711">
        arXiv:2512.23711
      </a>
      
        [<a href="/pdf/2512.23711" title="Download PDF" id="pdf-2512.23711" aria-labelledby="pdf-2512.23711">pdf</a>, <a href="https://arxiv.org/html/2512.23711v1" title="View HTML" id="html-2512.23711" aria-labelledby="html-2512.23711" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23711" title="Other formats" id="oth-2512.23711" aria-labelledby="oth-2512.23711">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          CAT: A Metric-Driven Framework for Analyzing the Consistency-Accuracy Relation of LLMs under Controlled Input Variations
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cavalin,+P">Paulo Cavalin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sanctos,+C">Cassia Sanctos</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Grave,+M">Marcelo Grave</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pinhanez,+C">Claudio Pinhanez</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Primerano,+Y">Yago Primerano</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          We introduce \textsc{CAT}, a framework designed to evaluate and visualize the \emph{interplay} of \emph{accuracy} and \emph{response consistency} of Large Language Models (LLMs) under controllable input variations, using multiple-choice (MC) benchmarks as a case study. Current evaluation practices primarily focus on model capabilities such as accuracy or benchmark scores and, more recently, measuring consistency is being considered an essential property for deploying LLMs in high-stake, real-world applications. We argue in this paper that although both dimensions should still be evaluated independently, their inter-dependency also need to be considered for a more nuanced evaluation of LLMs. At the core of \textsc{CAT} are the \emph{Consistency-Accuracy Relation (CAR)} curves, which visualize how model accuracy varies with increasing consistency requirements, as defined by the \emph{Minimum-Consistency Accuracy (MCA)} metric. We further propose the \emph{Consistency-Oriented Robustness Estimate (CORE)} index, a global metric that combines the area and shape of the CAR curve to quantify the trade-off between accuracy and consistency. We present a practical demonstration of our framework across a diverse set of generalist and domain-specific LLMs, evaluated on multiple MC benchmarks. We also outline how \textsc{CAT} can be extended beyond MC tasks to support long-form, open-ended evaluations through adaptable scoring functions.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item3'>[3]</a>
      <a href ="/abs/2512.23712" title="Abstract" id="2512.23712">
        arXiv:2512.23712
      </a>
      
        [<a href="/pdf/2512.23712" title="Download PDF" id="pdf-2512.23712" aria-labelledby="pdf-2512.23712">pdf</a>, <a href="https://arxiv.org/html/2512.23712v1" title="View HTML" id="html-2512.23712" aria-labelledby="html-2512.23712" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23712" title="Other formats" id="oth-2512.23712" aria-labelledby="oth-2512.23712">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          STED and Consistency Scoring: A Framework for Evaluating LLM Structured Output Reliability
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+G">Guanghui Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+J">Jinze Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xing Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+D">Dayuan Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Y">Yin Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deb,+T">Tomal Deb</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+X">Xuefeng Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+P">Peiyang He</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Large Language Models (LLMs) are increasingly deployed for structured data generation, yet output consistency remains critical for production applications. We introduce a comprehensive framework for evaluating and improving consistency in LLM-generated structured outputs. Our approach combines: (1) STED (Semantic Tree Edit Distance), a novel similarity metric balancing semantic flexibility with structural strictness when comparing JSON outputs, and (2) a consistency scoring framework aggregating multiple STED measurements across repeated generations to quantify reliability. Through systematic experiments on synthetic datasets with controlled schema, expression, and semantic variations, we demonstrate STED achieves superior performance ($0.86-0.90$ similarity for semantic equivalents, $0.0$ for structural breaks) compared to existing metrics including TED, BERTScore, and DeepDiff. Applying our framework to benchmark six LLMs reveals significant variations: Claude-3.7-Sonnet demonstrates exceptional consistency, maintaining near-perfect structural reliability even at high temperatures ($T=0.9$), while models like Claude-3-Haiku and Nova-Pro exhibit substantial degradation requiring careful tuning. Our framework enables practical applications including targeted model selection for structured tasks, iterative prompt refinement for reproducible results, and diagnostic analysis to identify inconsistency root causes. This work provides theoretical foundations and practical tools for ensuring reliable structured output generation in LLM-based production systems.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item4'>[4]</a>
      <a href ="/abs/2512.23713" title="Abstract" id="2512.23713">
        arXiv:2512.23713
      </a>
      
        [<a href="/pdf/2512.23713" title="Download PDF" id="pdf-2512.23713" aria-labelledby="pdf-2512.23713">pdf</a>, <a href="https://arxiv.org/html/2512.23713v1" title="View HTML" id="html-2512.23713" aria-labelledby="html-2512.23713" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23713" title="Other formats" id="oth-2512.23713" aria-labelledby="oth-2512.23713">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          PyBangla at BLP-2025 Task 2: Enhancing Bangla-to-Python Code Generation with Iterative Self-Correction and Multilingual Agents
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Islam,+J">Jahidul Islam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ataullha,+M">Md Ataullha</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Azad,+S">Saiful Azad</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          6 Pages
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          LLMs excel at code generation from English prompts, but this progress has not extended to low-resource languages. We address Bangla-to-Python code generation by introducing BanglaCodeAct, an agent-based framework that leverages multi-agent prompting and iterative self-correction. Unlike prior approaches relying on task-specific fine-tuning, BanglaCodeAct employs an open-source multilingual LLM within a Thought-Code-Observation loop, enabling dynamic generation, testing, and refinement of code from Bangla instructions. We benchmark several small-parameter open-source LLMs and evaluate their effectiveness on the mHumanEval dataset for Bangla NL2Code. Our results show that Qwen3-8B, when deployed with BanglaCodeAct, achieves the best performance, with pass@1 accuracy of 94.0\% on the development set and 71.6\% on the blind test set. These results establish a new benchmark for Bangla-to-Python translation and highlight the potential of agent-based reasoning for reliable code generation in low-resource languages. Experimental scripts are publicly available at <a href="http://github.com/jahidulzaid/PyBanglaCodeActAgent" rel="external noopener nofollow" class="link-external link-http">this http URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item5'>[5]</a>
      <a href ="/abs/2512.23714" title="Abstract" id="2512.23714">
        arXiv:2512.23714
      </a>
      
        [<a href="/pdf/2512.23714" title="Download PDF" id="pdf-2512.23714" aria-labelledby="pdf-2512.23714">pdf</a>, <a href="https://arxiv.org/html/2512.23714v1" title="View HTML" id="html-2512.23714" aria-labelledby="html-2512.23714" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23714" title="Other formats" id="oth-2512.23714" aria-labelledby="oth-2512.23714">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          PharmaShip: An Entity-Centric, Reading-Order-Supervised Benchmark for Chinese Pharmaceutical Shipping Documents
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+T">Tingwei Xie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+T">Tianyi Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Y">Yonghong Song</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          5 pages, 4 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          We present PharmaShip, a real-world Chinese dataset of scanned pharmaceutical shipping documents designed to stress-test pre-trained text-layout models under noisy OCR and heterogeneous templates. PharmaShip covers three complementary tasks-sequence entity recognition (SER), relation extraction (RE), and reading order prediction (ROP)-and adopts an entity-centric evaluation protocol to minimize confounds across architectures. We benchmark five representative baselines spanning pixel-aware and geometry-aware families (LiLT, LayoutLMv3-base, GeoLayoutLM and their available RORE-enhanced variants), and standardize preprocessing, splits, and optimization. Experiments show that pixels and explicit geometry provide complementary inductive biases, yet neither alone is sufficient: injecting reading-order-oriented regularization consistently improves SER and EL and yields the most robust configuration, while longer positional coverage stabilizes late-page predictions and reduces truncation artifacts. ROP is accurate at the word level but challenging at the segment level, reflecting boundary ambiguity and long-range crossings. PharmaShip thus establishes a controlled, reproducible benchmark for safety-critical document understanding in the pharmaceutical domain and highlights sequence-aware constraints as a transferable bias for structure modeling. We release the dataset at <a href="https://github.com/KevinYuLei/PharmaShip" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item6'>[6]</a>
      <a href ="/abs/2512.23716" title="Abstract" id="2512.23716">
        arXiv:2512.23716
      </a>
      
        [<a href="/pdf/2512.23716" title="Download PDF" id="pdf-2512.23716" aria-labelledby="pdf-2512.23716">pdf</a>, <a href="/format/2512.23716" title="Other formats" id="oth-2512.23716" aria-labelledby="oth-2512.23716">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Noise-Driven Persona Formation in Reflexive Neural Language Generation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shigemura,+T">Toshiyuki Shigemura</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          324 pages, 9 figures (Figure 7 intentionally skipped), with Appendices A-I. This manuscript presents a computational framework for noise-driven persona formation in neural language generation, analyzing 152 generation cycles using GPT-5.1 with stochastic noise seeds generated by Microsoft Copilot. Primary category: cs.CL
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          This paper introduces the Luca-Noise Reflex Protocol (LN-RP), a computational framework for analyzing noise-driven persona emergence in large language models. By injecting stochastic noise seeds into the initial generation state, we observe nonlinear transitions in linguistic behavior across 152 generation cycles. Our results reveal three stable persona modes with distinct entropy signatures, and demonstrate that external noise sources can reliably induce phase transitions in reflexive generation dynamics. Quantitative evaluation confirms consistent persona retention and significant differences across modes (p &lt; 0.01). The protocol provides a reproducible method for studying reflexive generation, emergent behavior, and longrange linguistic coherence in LLMs.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item7'>[7]</a>
      <a href ="/abs/2512.23717" title="Abstract" id="2512.23717">
        arXiv:2512.23717
      </a>
      
        [<a href="/pdf/2512.23717" title="Download PDF" id="pdf-2512.23717" aria-labelledby="pdf-2512.23717">pdf</a>, <a href="https://arxiv.org/html/2512.23717v1" title="View HTML" id="html-2512.23717" aria-labelledby="html-2512.23717" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23717" title="Other formats" id="oth-2512.23717" aria-labelledby="oth-2512.23717">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          HarmTransform: Transforming Explicit Harmful Queries into Stealthy via Multi-Agent Debate
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+S">Shenzhe Zhu</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Large language models (LLMs) are equipped with safety mechanisms to detect and block harmful queries, yet current alignment approaches primarily focus on overtly dangerous content and overlook more subtle threats. However, users can often disguise harmful intent through covert rephrasing that preserves malicious objectives while appearing benign, which creates a significant gap in existing safety training data. To address this limitation, we introduce HarmTransform, a multi-agent debate framework for systematically transforming harmful queries into stealthier forms while preserving their underlying harmful intent. Our framework leverages iterative critique and refinement among multiple agents to generate high-quality, covert harmful query transformations that can be used to improve future LLM safety alignment. Experiments demonstrate that HarmTransform significantly outperforms standard baselines in producing effective query transformations. At the same time, our analysis reveals that debate acts as a double-edged sword: while it can sharpen transformations and improve stealth, it may also introduce topic shifts and unnecessary complexity. These insights highlight both the promise and the limitations of multi-agent debate for generating comprehensive safety training data.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item8'>[8]</a>
      <a href ="/abs/2512.23722" title="Abstract" id="2512.23722">
        arXiv:2512.23722
      </a>
      
        [<a href="/pdf/2512.23722" title="Download PDF" id="pdf-2512.23722" aria-labelledby="pdf-2512.23722">pdf</a>, <a href="https://arxiv.org/html/2512.23722v1" title="View HTML" id="html-2512.23722" aria-labelledby="html-2512.23722" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23722" title="Other formats" id="oth-2512.23722" aria-labelledby="oth-2512.23722">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Emergent World Beliefs: Exploring Transformers in Stochastic Games
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kamel,+A">Adam Kamel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rastogi,+T">Tanish Rastogi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+M">Michael Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ranganathan,+K">Kailash Ranganathan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+K">Kevin Zhu</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Accepted at NeurIPS 2025 Mechanistic Interpretability Workshop
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Transformer-based large language models (LLMs) have demonstrated strong reasoning abilities across diverse fields, from solving programming challenges to competing in strategy-intensive games such as chess. Prior work has shown that LLMs can develop emergent world models in games of perfect information, where internal representations correspond to latent states of the environment. In this paper, we extend this line of investigation to domains of incomplete information, focusing on poker as a canonical partially observable Markov decision process (POMDP). We pretrain a GPT-style model on Poker Hand History (PHH) data and probe its internal activations. Our results demonstrate that the model learns both deterministic structure, such as hand ranks, and stochastic features, such as equity, without explicit instruction. Furthermore, by using primarily nonlinear probes, we demonstrated that these representations are decodeable and correlate with theoretical belief states, suggesting that LLMs are learning their own representation of the stochastic environment of Texas Hold&#39;em Poker.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item9'>[9]</a>
      <a href ="/abs/2512.23732" title="Abstract" id="2512.23732">
        arXiv:2512.23732
      </a>
      
        [<a href="/pdf/2512.23732" title="Download PDF" id="pdf-2512.23732" aria-labelledby="pdf-2512.23732">pdf</a>, <a href="https://arxiv.org/html/2512.23732v1" title="View HTML" id="html-2512.23732" aria-labelledby="html-2512.23732" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23732" title="Other formats" id="oth-2512.23732" aria-labelledby="oth-2512.23732">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          When in Doubt, Deliberate: Confidence-Based Routing to Expert Debate for Sexism Detection
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Alajmi,+A">Anwar Alajmi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pergola,+G">Gabriele Pergola</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Sexist content online increasingly appears in subtle, context-dependent forms that evade traditional detection methods. Its interpretation often depends on overlapping linguistic, psychological, legal, and cultural dimensions, which produce mixed and sometimes contradictory signals, even in annotated datasets. These inconsistencies, combined with label scarcity and class imbalance, result in unstable decision boundaries and cause fine-tuned models to overlook subtler, underrepresented forms of harm. Together, these limitations point to the need for a design that explicitly addresses the combined effects of (i) underrepresentation, (ii) noise, and (iii) conceptual ambiguity in both data and model predictions. To address these challenges, we propose a two-stage framework that unifies (i) targeted training procedures to adapt supervision to scarce and noisy data with (ii) selective, reasoning-based inference to handle ambiguous or borderline cases. Our training setup applies class-balanced focal loss, class-aware batching, and post-hoc threshold calibration to mitigate label imbalance and noisy supervision. At inference time, a dynamic routing mechanism classifies high-confidence cases directly and escalates uncertain instances to a novel \textit{Collaborative Expert Judgment} (CEJ) module, which prompts multiple personas and consolidates their reasoning through a judge model. Our approach achieves state-of-the-art results across several benchmarks, with a +2.72\% improvement in F1 on the EXIST 2025 Task 1.1, and a gains of +4.48\% and +1.30\% on the EDOS Tasks A and B, respectively.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item10'>[10]</a>
      <a href ="/abs/2512.23739" title="Abstract" id="2512.23739">
        arXiv:2512.23739
      </a>
      
        [<a href="/pdf/2512.23739" title="Download PDF" id="pdf-2512.23739" aria-labelledby="pdf-2512.23739">pdf</a>, <a href="https://arxiv.org/html/2512.23739v1" title="View HTML" id="html-2512.23739" aria-labelledby="html-2512.23739" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23739" title="Other formats" id="oth-2512.23739" aria-labelledby="oth-2512.23739">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Break Out the Silverware -- Semantic Understanding of Stored Household Items
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Levi-Richter,+M">Michaela Levi-Richter</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mirsky,+R">Reuth Mirsky</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Glickman,+O">Oren Glickman</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Poster presented at the Israeli Seminar on Computational Linguistics 2025
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV); Robotics (cs.RO)
        </div>

        <p class='mathjax'>
          ``Bring me a plate.&#39;&#39; For domestic service robots, this simple command reveals a complex challenge: inferring where everyday items are stored, often out of sight in drawers, cabinets, or closets. Despite advances in vision and manipulation, robots still lack the commonsense reasoning needed to complete this task. We introduce the Stored Household Item Challenge, a benchmark task for evaluating service robots&#39; cognitive capabilities: given a household scene and a queried item, predict its most likely storage location.
<br>Our benchmark includes two datasets: (1) a real-world evaluation set of 100 item-image pairs with human-annotated ground truth from participants&#39; kitchens, and (2) a development set of 6,500 item-image pairs annotated with storage polygons over public kitchen images. These datasets support realistic modeling of household organization and enable comparative evaluation across agent architectures.
<br>To begin tackling this challenge, we introduce NOAM (Non-visible Object Allocation Model), a hybrid agent pipeline that combines structured scene understanding with large language model inference. NOAM converts visual input into natural language descriptions of spatial context and visible containers, then prompts a language model (e.g., GPT-4) to infer the most likely hidden storage location. This integrated vision-language agent exhibits emergent commonsense reasoning and is designed for modular deployment within broader robotic systems.
<br>We evaluate NOAM against baselines including random selection, vision-language pipelines (Grounding-DINO + SAM), leading multimodal models (e.g., Gemini, GPT-4o, Kosmos-2, LLaMA, Qwen), and human performance. NOAM significantly improves prediction accuracy and approaches human-level results, highlighting best practices for deploying cognitively capable agents in domestic environments.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item11'>[11]</a>
      <a href ="/abs/2512.23765" title="Abstract" id="2512.23765">
        arXiv:2512.23765
      </a>
      
        [<a href="/pdf/2512.23765" title="Download PDF" id="pdf-2512.23765" aria-labelledby="pdf-2512.23765">pdf</a>, <a href="https://arxiv.org/html/2512.23765v1" title="View HTML" id="html-2512.23765" aria-labelledby="html-2512.23765" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23765" title="Other formats" id="oth-2512.23765" aria-labelledby="oth-2512.23765">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Entropy-Aware Speculative Decoding Toward Improved LLM Reasoning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+T">Tiancheng Su</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M">Meicong Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+G">Guoxiu He</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Speculative decoding (SD) accelerates large language model (LLM) reasoning by using a small draft model to generate candidate tokens, which the target LLM either accepts directly or regenerates upon rejection. However, excessive alignment between the draft and target models constrains SD to the performance of the target LLM. To address this limitation, we propose Entropy-Aware Speculative Decoding (EASD), a training-free enhancement. Building on standard SD, EASD incorporates a dynamic entropy-based penalty. At each decoding step, we employ the entropy of the sampling distribution to quantify model uncertainty. When both models exhibit high entropy with substantial overlap among their top-N predictions, the corresponding token is rejected and re-sampled by the target LLM. This penalty prevents low-confidence errors from propagating. By incorporating draft-model verification, EASD enables the possibility of surpassing the target model&#39;s inherent performance. Experiments across multiple reasoning benchmarks demonstrate that EASD consistently outperforms existing SD methods and, in most cases, surpasses the target LLM itself. We further prove that the efficiency of EASD is comparable to that of SD. The code can be found in the Supplementary Materials.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item12'>[12]</a>
      <a href ="/abs/2512.23808" title="Abstract" id="2512.23808">
        arXiv:2512.23808
      </a>
      
        [<a href="/pdf/2512.23808" title="Download PDF" id="pdf-2512.23808" aria-labelledby="pdf-2512.23808">pdf</a>, <a href="https://arxiv.org/html/2512.23808v1" title="View HTML" id="html-2512.23808" aria-labelledby="html-2512.23808" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23808" title="Other formats" id="oth-2512.23808" aria-labelledby="oth-2512.23808">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MiMo-Audio: Audio Language Models are Few-Shot Learners
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiaomi+LLM-Core+Team">Xiaomi LLM-Core Team</a>: <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+D">Dong Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+G">Gang Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+J">Jinlong Xue</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+K">Kai Fang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+L">Liang Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+R">Rui Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ren,+S">Shuhuai Ren</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S">Shuo Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+T">Tao Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhuang,+W">Weiji Zhuang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xin Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+X">Xingchen Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+Y">Yihan Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+Y">Yongzhe He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cici">Cici</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+B">Bowen Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+C">Chengxuan Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+C">Chong Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+C">Chun Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+H">Heyu Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jiawei Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+L">Lei Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+M">Menghang Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P">Peidian Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Q">Qiying Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+S">Sirui Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+W">Weimin Xiong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+W">Wenshan Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+W">Wenyu Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Y">Yilin Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Y">Yixin Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian,+Y">Yuanyuan Tian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+Y">Yue Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+Y">Yue Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">Zihan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yue,+Z">Zihao Yue</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+B">Bangjun Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xia,+B">Bingquan Xia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+B">Bofei Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+B">Bowen Ye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+C">Can Cai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C">Chang Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+C">Chenhong He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C">Chunan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+D">Dawei Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+D">Duo Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+F">Fengyuan Shi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+G">Guoan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H">Hailin Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lv,+H">Hanglong Lv</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H">Hanyu Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tian,+H">Hao Tian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+H">Heng Qu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+H">Hongshen Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+H">Houbin Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+H">Huaqiu Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Duo,+J">Jiangshan Duo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zuo,+J">Jianguang Zuo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+J">Jianyu Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+J">Jiebao Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+J">Jinhao Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+J">Jun Shi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+J">Junhao Hu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bao,+K">Kainan Bao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+K">Kang Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+L">Linghao Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M">Meng Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+N">Nuo Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+P">Peng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Q">Qianli Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Q">Qiantong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+R">Rang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S">Shaohui Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+S">Shengfan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S">Shicheng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+S">Shihua Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+S">Shijie Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+S">Shimao Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+S">Shuhao Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W">Weikun Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+W">Wenhan Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+X">Xiangwei Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yong,+X">Xing Yong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xing Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X">Xu Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Y">Yifan Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Y">Yihao Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Y">Yingbo Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+Y">Yizhao Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+Y">Yu Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tu,+Y">Yu Tu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yudong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Z">Zhaojun Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+Z">Zhengju Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z">Zhenru Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+Z">Zhichao Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Z">Zhipeng Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Z">Zhixian Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Z">Zihan Jiang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)
        </div>

        <p class='mathjax'>
          Existing audio language models typically rely on task-specific fine-tuning to accomplish particular audio tasks. In contrast, humans are able to generalize to new audio tasks with only a few examples or simple instructions. GPT-3 has shown that scaling next-token prediction pretraining enables strong generalization capabilities in text, and we believe this paradigm is equally applicable to the audio domain. By scaling MiMo-Audio&#39;s pretraining data to over one hundred million of hours, we observe the emergence of few-shot learning capabilities across a diverse set of audio tasks. We develop a systematic evaluation of these capabilities and find that MiMo-Audio-7B-Base achieves SOTA performance on both speech intelligence and audio understanding benchmarks among open-source models. Beyond standard metrics, MiMo-Audio-7B-Base generalizes to tasks absent from its training data, such as voice conversion, style transfer, and speech editing. MiMo-Audio-7B-Base also demonstrates powerful speech continuation capabilities, capable of generating highly realistic talk shows, recitations, livestreaming and debates. At the post-training stage, we curate a diverse instruction-tuning corpus and introduce thinking mechanisms into both audio understanding and generation. MiMo-Audio-7B-Instruct achieves open-source SOTA on audio understanding benchmarks (MMSU, MMAU, MMAR, MMAU-Pro), spoken dialogue benchmarks (Big Bench Audio, MultiChallenge Audio) and instruct-TTS evaluations, approaching or surpassing closed-source models. Model checkpoints and full evaluation suite are available at <a href="https://github.com/XiaomiMiMo/MiMo-Audio" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item13'>[13]</a>
      <a href ="/abs/2512.23813" title="Abstract" id="2512.23813">
        arXiv:2512.23813
      </a>
      
        [<a href="/pdf/2512.23813" title="Download PDF" id="pdf-2512.23813" aria-labelledby="pdf-2512.23813">pdf</a>, <a href="https://arxiv.org/html/2512.23813v1" title="View HTML" id="html-2512.23813" aria-labelledby="html-2512.23813" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23813" title="Other formats" id="oth-2512.23813" aria-labelledby="oth-2512.23813">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          StressRoBERTa: Cross-Condition Transfer Learning from Depression, Anxiety, and PTSD to Stress Detection
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Alqahtani,+A">Amal Alqahtani</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kayi,+E">Efsun Kayi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Diab,+M">Mona Diab</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          The prevalence of chronic stress represents a significant public health concern, with social media platforms like Twitter serving as important venues for individuals to share their experiences. This paper introduces StressRoBERTa, a cross-condition transfer learning approach for automatic detection of self-reported chronic stress in English tweets. The investigation examines whether continual training on clinically related conditions (depression, anxiety, PTSD), disorders with high comorbidity with chronic stress, improves stress detection compared to general language models and broad mental health models. RoBERTa is continually trained on the Stress-SMHD corpus (108M words from users with self-reported diagnoses of depression, anxiety, and PTSD) and fine-tuned on the SMM4H 2022 Task 8 dataset. StressRoBERTa achieves 82% F1-score, outperforming the best shared task system (79% F1) by 3 percentage points. The results demonstrate that focused cross-condition transfer from stress-related disorders (+1% F1 over vanilla RoBERTa) provides stronger representations than general mental health training. Evaluation on Dreaddit (81% F1) further demonstrates transfer from clinical mental health contexts to situational stress discussions.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item14'>[14]</a>
      <a href ="/abs/2512.23835" title="Abstract" id="2512.23835">
        arXiv:2512.23835
      </a>
      
        [<a href="/pdf/2512.23835" title="Download PDF" id="pdf-2512.23835" aria-labelledby="pdf-2512.23835">pdf</a>, <a href="https://arxiv.org/html/2512.23835v1" title="View HTML" id="html-2512.23835" aria-labelledby="html-2512.23835" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23835" title="Other formats" id="oth-2512.23835" aria-labelledby="oth-2512.23835">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Explaining News Bias Detection: A Comparative SHAP Analysis of Transformer Model Decision Mechanisms
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ghosh,+H">Himel Ghosh</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          10 pages, 8 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Human-Computer Interaction (cs.HC)
        </div>

        <p class='mathjax'>
          Automated bias detection in news text is heavily used to support journalistic analysis and media accountability, yet little is known about how bias detection models arrive at their decisions or why they fail. In this work, we present a comparative interpretability study of two transformer-based bias detection models: a bias detector fine-tuned on the BABE dataset and a domain-adapted pre-trained RoBERTa model fine-tuned on the BABE dataset, using SHAP-based explanations. We analyze word-level attributions across correct and incorrect predictions to characterize how different model architectures operationalize linguistic bias. Our results show that although both models attend to similar categories of evaluative language, they differ substantially in how these signals are integrated into predictions. The bias detector model assigns stronger internal evidence to false positives than to true positives, indicating a misalignment between attribution strength and prediction correctness and contributing to systematic over-flagging of neutral journalistic content. In contrast, the domain-adaptive model exhibits attribution patterns that better align with prediction outcomes and produces 63\% fewer false positives. We further demonstrate that model errors arise from distinct linguistic mechanisms, with false positives driven by discourse-level ambiguity rather than explicit bias cues. These findings highlight the importance of interpretability-aware evaluation for bias detection systems and suggest that architectural and training choices critically affect both model reliability and deployment suitability in journalistic contexts.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item15'>[15]</a>
      <a href ="/abs/2512.23836" title="Abstract" id="2512.23836">
        arXiv:2512.23836
      </a>
      
        [<a href="/pdf/2512.23836" title="Download PDF" id="pdf-2512.23836" aria-labelledby="pdf-2512.23836">pdf</a>, <a href="https://arxiv.org/html/2512.23836v1" title="View HTML" id="html-2512.23836" aria-labelledby="html-2512.23836" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23836" title="Other formats" id="oth-2512.23836" aria-labelledby="oth-2512.23836">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Retrieval Augmented Question Answering: When Should LLMs Admit Ignorance?
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+D">Dingmin Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+J">Ji Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+S">Shankar Kumar</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          The success of expanded context windows in Large Language Models (LLMs) has driven increased use of broader context in retrieval-augmented generation. We investigate the use of LLMs for retrieval augmented question answering. While longer contexts make it easier to incorporate targeted knowledge, they introduce more irrelevant information that hinders the model&#39;s generation process and degrades its performance. To address the issue, we design an adaptive prompting strategy which involves splitting the retrieved information into smaller chunks and sequentially prompting a LLM to answer the question using each chunk. Adjusting the chunk size allows a trade-off between incorporating relevant information and reducing irrelevant information. Experimental results on three open-domain question answering datasets demonstrate that the adaptive strategy matches the performance of standard prompting while using fewer tokens. Our analysis reveals that when encountering insufficient information, the LLM often generates incorrect answers instead of declining to respond, which constitutes a major source of error. This finding highlights the need for further research into enhancing LLMs&#39; ability to effectively decline requests when faced with inadequate information.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item16'>[16]</a>
      <a href ="/abs/2512.23837" title="Abstract" id="2512.23837">
        arXiv:2512.23837
      </a>
      
        [<a href="/pdf/2512.23837" title="Download PDF" id="pdf-2512.23837" aria-labelledby="pdf-2512.23837">pdf</a>, <a href="/format/2512.23837" title="Other formats" id="oth-2512.23837" aria-labelledby="oth-2512.23837">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Adversarial Lens: Exploiting Attention Layers to Generate Adversarial Examples for Evaluation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dhole,+K">Kaustubh Dhole</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Recent advances in mechanistic interpretability suggest that intermediate attention layers encode token-level hypotheses that are iteratively refined toward the final output. In this work, we exploit this property to generate adversarial examples directly from attention-layer token distributions. Unlike prompt-based or gradient-based attacks, our approach leverages model-internal token predictions, producing perturbations that are both plausible and internally consistent with the model&#39;s own generation process. We evaluate whether tokens extracted from intermediate layers can serve as effective adversarial perturbations for downstream evaluation tasks. We conduct experiments on argument quality assessment using the ArgQuality dataset, with LLaMA-3.1-Instruct-8B serving as both the generator and evaluator. Our results show that attention-based adversarial examples lead to measurable drops in evaluation performance while remaining semantically similar to the original inputs. However, we also observe that substitutions drawn from certain layers and token positions can introduce grammatical degradation, limiting their practical effectiveness. Overall, our findings highlight both the promise and current limitations of using intermediate-layer representations as a principled source of adversarial examples for stress-testing LLM-based evaluation pipelines.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item17'>[17]</a>
      <a href ="/abs/2512.23848" title="Abstract" id="2512.23848">
        arXiv:2512.23848
      </a>
      
        [<a href="/pdf/2512.23848" title="Download PDF" id="pdf-2512.23848" aria-labelledby="pdf-2512.23848">pdf</a>, <a href="https://arxiv.org/html/2512.23848v1" title="View HTML" id="html-2512.23848" aria-labelledby="html-2512.23848" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23848" title="Other formats" id="oth-2512.23848" aria-labelledby="oth-2512.23848">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Integrating Domain Knowledge for Financial QA: A Multi-Retriever RAG Approach with LLMs
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yukun Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Droguett,+S+E">Stefan Elbl Droguett</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jain,+S">Samyak Jain</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Computational Engineering, Finance, and Science (cs.CE); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          This research project addresses the errors of financial numerical reasoning Question Answering (QA) tasks due to the lack of domain knowledge in finance. Despite recent advances in Large Language Models (LLMs), financial numerical questions remain challenging because they require specific domain knowledge in finance and complex multi-step numeric reasoning. We implement a multi-retriever Retrieval Augmented Generators (RAG) system to retrieve both external domain knowledge and internal question contexts, and utilize the latest LLM to tackle these tasks. Through comprehensive ablation experiments and error analysis, we find that domain-specific training with the SecBERT encoder significantly contributes to our best neural symbolic model surpassing the FinQA paper&#39;s top model, which serves as our baseline. This suggests the potential superior performance of domain-specific training. Furthermore, our best prompt-based LLM generator achieves the state-of-the-art (SOTA) performance with significant improvement (&gt;7%), yet it is still below the human expert performance. This study highlights the trade-off between hallucinations loss and external knowledge gains in smaller models and few-shot examples. For larger models, the gains from external facts typically outweigh the hallucination loss. Finally, our findings confirm the enhanced numerical reasoning capabilities of the latest LLM, optimized for few-shot learning.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item18'>[18]</a>
      <a href ="/abs/2512.23941" title="Abstract" id="2512.23941">
        arXiv:2512.23941
      </a>
      
        [<a href="/pdf/2512.23941" title="Download PDF" id="pdf-2512.23941" aria-labelledby="pdf-2512.23941">pdf</a>, <a href="https://arxiv.org/html/2512.23941v1" title="View HTML" id="html-2512.23941" aria-labelledby="html-2512.23941" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23941" title="Other formats" id="oth-2512.23941" aria-labelledby="oth-2512.23941">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Disentangling Learning from Judgment: Representation Learning for Open Response Analytics
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Borchers,+C">Conrad Borchers</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Patel,+M">Manit Patel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+S+M">Seiyon M. Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Botelho,+A+F">Anthony F. Botelho</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Short research paper accepted at Learning Analytics and Knowledge (LAK &#39;26)
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)
        </div>

        <p class='mathjax'>
          Open-ended responses are central to learning, yet automated scoring often conflates what students wrote with how teachers grade. We present an analytics-first framework that separates content signals from rater tendencies, making judgments visible and auditable via analytics. Using de-identified ASSISTments mathematics responses, we model teacher histories as dynamic priors and derive text representations from sentence embeddings, incorporating centering and residualization to mitigate prompt and teacher confounds. Temporally-validated linear models quantify the contributions of each signal, and a projection surfaces model disagreements for qualitative inspection. Results show that teacher priors heavily influence grade predictions; the strongest results arise when priors are combined with content embeddings (AUC~0.815), while content-only models remain above chance but substantially weaker (AUC~0.626). Adjusting for rater effects sharpens the residual content representation, retaining more informative embedding dimensions and revealing cases where semantic evidence supports understanding as opposed to surface-level differences in how students respond. The contribution presents a practical pipeline that transforms embeddings from mere features into learning analytics for reflection, enabling teachers and researchers to examine where grading practices align (or conflict) with evidence of student reasoning and learning.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item19'>[19]</a>
      <a href ="/abs/2512.23959" title="Abstract" id="2512.23959">
        arXiv:2512.23959
      </a>
      
        [<a href="/pdf/2512.23959" title="Download PDF" id="pdf-2512.23959" aria-labelledby="pdf-2512.23959">pdf</a>, <a href="https://arxiv.org/html/2512.23959v1" title="View HTML" id="html-2512.23959" aria-labelledby="html-2512.23959" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23959" title="Other formats" id="oth-2512.23959" aria-labelledby="oth-2512.23959">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Improving Multi-step RAG with Hypergraph-based Memory for Long-Context Complex Relational Modeling
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+C">Chulun Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C">Chunkang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+G">Guoxin Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Meng,+F">Fandong Meng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+J">Jie Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lam,+W">Wai Lam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+M">Mo Yu</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          21 pages
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Multi-step retrieval-augmented generation (RAG) has become a widely adopted strategy for enhancing large language models (LLMs) on tasks that demand global comprehension and intensive reasoning. Many RAG systems incorporate a working memory module to consolidate retrieved information. However, existing memory designs function primarily as passive storage that accumulates isolated facts for the purpose of condensing the lengthy inputs and generating new sub-queries through deduction. This static nature overlooks the crucial high-order correlations among primitive facts, the compositions of which can often provide stronger guidance for subsequent steps. Therefore, their representational strength and impact on multi-step reasoning and knowledge evolution are limited, resulting in fragmented reasoning and weak global sense-making capacity in extended contexts. We introduce HGMem, a hypergraph-based memory mechanism that extends the concept of memory beyond simple storage into a dynamic, expressive structure for complex reasoning and global understanding. In our approach, memory is represented as a hypergraph whose hyperedges correspond to distinct memory units, enabling the progressive formation of higher-order interactions within memory. This mechanism connects facts and thoughts around the focal problem, evolving into an integrated and situated knowledge structure that provides strong propositions for deeper reasoning in subsequent steps. We evaluate HGMem on several challenging datasets designed for global sense-making. Extensive experiments and in-depth analyses show that our method consistently improves multi-step RAG and substantially outperforms strong baseline systems across diverse tasks.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item20'>[20]</a>
      <a href ="/abs/2512.23966" title="Abstract" id="2512.23966">
        arXiv:2512.23966
      </a>
      
        [<a href="/pdf/2512.23966" title="Download PDF" id="pdf-2512.23966" aria-labelledby="pdf-2512.23966">pdf</a>, <a href="https://arxiv.org/html/2512.23966v1" title="View HTML" id="html-2512.23966" aria-labelledby="html-2512.23966" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23966" title="Other formats" id="oth-2512.23966" aria-labelledby="oth-2512.23966">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Efficient Context Scaling with LongCat ZigZag Attention
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+C">Chen Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+Y">Yang Bai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jiahuan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gui,+A">Anchun Gui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+K">Keheng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+F">Feifan Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+G">Guanyu Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Y">Yuwei Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bu,+D">Defei Bu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+L">Li Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jing,+H">Haihang Jing</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+H">Hongyin Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X">Xin Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+X">Xiangzhou Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+F">Fengcun Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Weng,+R">Rongxiang Weng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qian,+Y">Yulei Qian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+Y">Yifan Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+Y">Yerui Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">Jingang Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+Y">Yuchen Xie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+X">Xunliang Cai</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          10 pages, 3 figures, 3 tables
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          We introduce LongCat ZigZag Attention (LoZA), which is a sparse attention scheme designed to transform any existing full-attention models into sparse versions with rather limited compute budget. In long-context scenarios, LoZA can achieve significant speed-ups both for prefill-intensive (e.g., retrieval-augmented generation) and decode-intensive (e.g., tool-integrated reasoning) cases. Specifically, by applying LoZA to LongCat-Flash during mid-training, we serve LongCat-Flash-Exp as a long-context foundation model that can swiftly process up to 1 million tokens, enabling efficient long-term reasoning and long-horizon agentic capabilities.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item21'>[21]</a>
      <a href ="/abs/2512.23971" title="Abstract" id="2512.23971">
        arXiv:2512.23971
      </a>
      
        [<a href="/pdf/2512.23971" title="Download PDF" id="pdf-2512.23971" aria-labelledby="pdf-2512.23971">pdf</a>, <a href="https://arxiv.org/html/2512.23971v1" title="View HTML" id="html-2512.23971" aria-labelledby="html-2512.23971" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23971" title="Other formats" id="oth-2512.23971" aria-labelledby="oth-2512.23971">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          CEC-Zero: Zero-Supervision Character Error Correction with Self-Generated Rewards
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z">Zhiming Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+K">Kai Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Sophie Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+P">Peilai Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+C">Canran Xiao</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          AAAI&#39;26 poster
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Large-scale Chinese spelling correction (CSC) remains critical for real-world text processing, yet existing LLMs and supervised methods lack robustness to novel errors and rely on costly annotations. We introduce CEC-Zero, a zero-supervision reinforcement learning framework that addresses this by enabling LLMs to correct their own mistakes. CEC-Zero synthesizes errorful inputs from clean text, computes cluster-consensus rewards via semantic similarity and candidate agreement, and optimizes the policy with PPO. It outperforms supervised baselines by 10--13 F$_1$ points and strong LLM fine-tunes by 5--8 points across 9 benchmarks, with theoretical guarantees of unbiased rewards and convergence. CEC-Zero establishes a label-free paradigm for robust, scalable CSC, unlocking LLM potential in noisy text pipelines.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item22'>[22]</a>
      <a href ="/abs/2512.23988" title="Abstract" id="2512.23988">
        arXiv:2512.23988
      </a>
      
        [<a href="/pdf/2512.23988" title="Download PDF" id="pdf-2512.23988" aria-labelledby="pdf-2512.23988">pdf</a>, <a href="https://arxiv.org/html/2512.23988v1" title="View HTML" id="html-2512.23988" aria-labelledby="html-2512.23988" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23988" title="Other formats" id="oth-2512.23988" aria-labelledby="oth-2512.23988">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Fantastic Reasoning Behaviors and Where to Find Them: Unsupervised Discovery of the Reasoning Process
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">Zhenyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Shujian Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lambert,+J">John Lambert</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+W">Wenxuan Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zhangyang Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M">Mingqing Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hard,+A">Andrew Hard</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mathews,+R">Rajiv Mathews</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L">Lun Wang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Despite the growing reasoning capabilities of recent large language models (LLMs), their internal mechanisms during the reasoning process remain underexplored. Prior approaches often rely on human-defined concepts (e.g., overthinking, reflection) at the word level to analyze reasoning in a supervised manner. However, such methods are limited, as it is infeasible to capture the full spectrum of potential reasoning behaviors, many of which are difficult to define in token space. In this work, we propose an unsupervised framework (namely, RISE: Reasoning behavior Interpretability via Sparse auto-Encoder) for discovering reasoning vectors, which we define as directions in the activation space that encode distinct reasoning behaviors. By segmenting chain-of-thought traces into sentence-level &#39;steps&#39; and training sparse auto-encoders (SAEs) on step-level activations, we uncover disentangled features corresponding to interpretable behaviors such as reflection and backtracking. Visualization and clustering analyses show that these behaviors occupy separable regions in the decoder column space. Moreover, targeted interventions on SAE-derived vectors can controllably amplify or suppress specific reasoning behaviors, altering inference trajectories without retraining. Beyond behavior-specific disentanglement, SAEs capture structural properties such as response length, revealing clusters of long versus short reasoning traces. More interestingly, SAEs enable the discovery of novel behaviors beyond human supervision. We demonstrate the ability to control response confidence by identifying confidence-related vectors in the SAE decoder space. These findings underscore the potential of unsupervised latent discovery for both interpreting and controllably steering reasoning in LLMs.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item23'>[23]</a>
      <a href ="/abs/2512.24000" title="Abstract" id="2512.24000">
        arXiv:2512.24000
      </a>
      
        [<a href="/pdf/2512.24000" title="Download PDF" id="pdf-2512.24000" aria-labelledby="pdf-2512.24000">pdf</a>, <a href="https://arxiv.org/html/2512.24000v1" title="View HTML" id="html-2512.24000" aria-labelledby="html-2512.24000" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24000" title="Other formats" id="oth-2512.24000" aria-labelledby="oth-2512.24000">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          WISE: Web Information Satire and Fakeness Evaluation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chhetri,+G">Gaurab Chhetri</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Das,+S">Subasish Das</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chowdhury,+T+I">Tausif Islam Chowdhury</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          This is the author&#39;s preprint. Accepted to WEB&amp;GRAPH 2026 (co-located with WSDM 2026), Boise, Idaho, USA, Feb 26, 2026. Final version will appear in WSDM 2026 Companion Proceedings. Conf: <a href="https://wsdm-conference.org/2026/" rel="external noopener nofollow" class="link-external link-https">this https URL</a> Workshop: <a href="https://aiimlab.org/events/WSDM_2026_WEB_and_GRAPH_2026_Workshop_on_Web_and_Graphs_Responsible_Intelligence_and_Social_Media.html" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Distinguishing fake or untrue news from satire or humor poses a unique challenge due to their overlapping linguistic features and divergent intent. This study develops WISE (Web Information Satire and Fakeness Evaluation) framework which benchmarks eight lightweight transformer models alongside two baseline models on a balanced dataset of 20,000 samples from Fakeddit, annotated as either fake news or satire. Using stratified 5-fold cross-validation, we evaluate models across comprehensive metrics including accuracy, precision, recall, F1-score, ROC-AUC, PR-AUC, MCC, Brier score, and Expected Calibration Error. Our evaluation reveals that MiniLM, a lightweight model, achieves the highest accuracy (87.58%) among all models, while RoBERTa-base achieves the highest ROC-AUC (95.42%) and strong accuracy (87.36%). DistilBERT offers an excellent efficiency-accuracy trade-off with 86.28\% accuracy and 93.90\% ROC-AUC. Statistical tests confirm significant performance differences between models, with paired t-tests and McNemar tests providing rigorous comparisons. Our findings highlight that lightweight models can match or exceed baseline performance, offering actionable insights for deploying misinformation detection systems in real-world, resource-constrained settings.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item24'>[24]</a>
      <a href ="/abs/2512.24014" title="Abstract" id="2512.24014">
        arXiv:2512.24014
      </a>
      
        [<a href="/pdf/2512.24014" title="Download PDF" id="pdf-2512.24014" aria-labelledby="pdf-2512.24014">pdf</a>, <a href="https://arxiv.org/html/2512.24014v1" title="View HTML" id="html-2512.24014" aria-labelledby="html-2512.24014" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24014" title="Other formats" id="oth-2512.24014" aria-labelledby="oth-2512.24014">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          iCLP: Large Language Model Reasoning with Implicit Cognition Latent Planning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+S">Sijia Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Niu,+D">Di Niu</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          9 pages, 6 figures. The source code is publicly available at <a href="https://github.com/AgenticFinLab/latent-planning" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Large language models (LLMs), when guided by explicit textual plans, can perform reliable step-by-step reasoning during problem-solving. However, generating accurate and effective textual plans remains challenging due to LLM hallucinations and the high diversity of task-specific questions. To address this, we draw inspiration from human Implicit Cognition (IC), the subconscious process by which decisions are guided by compact, generalized patterns learned from past experiences without requiring explicit verbalization. We propose iCLP, a novel framework that enables LLMs to adaptively generate latent plans (LPs), which are compact encodings of effective reasoning instructions. iCLP first distills explicit plans from existing step-by-step reasoning trajectories. It then learns discrete representations of these plans via a vector-quantized autoencoder coupled with a codebook. Finally, by fine-tuning LLMs on paired latent plans and corresponding reasoning steps, the models learn to perform implicit planning during reasoning. Experimental results on mathematical reasoning and code generation tasks demonstrate that, with iCLP, LLMs can plan in latent space while reasoning in language space. This approach yields significant improvements in both accuracy and efficiency and, crucially, demonstrates strong cross-domain generalization while preserving the interpretability of chain-of-thought reasoning.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item25'>[25]</a>
      <a href ="/abs/2512.24058" title="Abstract" id="2512.24058">
        arXiv:2512.24058
      </a>
      
        [<a href="/pdf/2512.24058" title="Download PDF" id="pdf-2512.24058" aria-labelledby="pdf-2512.24058">pdf</a>, <a href="https://arxiv.org/html/2512.24058v1" title="View HTML" id="html-2512.24058" aria-labelledby="html-2512.24058" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24058" title="Other formats" id="oth-2512.24058" aria-labelledby="oth-2512.24058">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Beyond Hallucinations: A Composite Score for Measuring Reliability in Open-Source Large Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Salla,+R+K">Rohit Kumar Salla</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Saravanan,+M">Manoj Saravanan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kota,+S+R">Shrikar Reddy Kota</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          5 pages, 4 tables, accepted at AAAI 2026
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Large Language Models (LLMs) like LLaMA, Mistral, and Gemma are increasingly used in decision-critical domains such as healthcare, law, and finance, yet their reliability remains uncertain. They often make overconfident errors, degrade under input shifts, and lack clear uncertainty estimates. Existing evaluations are fragmented, addressing only isolated aspects. We introduce the Composite Reliability Score (CRS), a unified framework that integrates calibration, robustness, and uncertainty quantification into a single interpretable metric. Through experiments on ten leading open-source LLMs across five QA datasets, we assess performance under baselines, perturbations, and calibration methods. CRS delivers stable model rankings, uncovers hidden failure modes missed by single metrics, and highlights that the most dependable systems balance accuracy, robustness, and calibrated uncertainty.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item26'>[26]</a>
      <a href ="/abs/2512.24092" title="Abstract" id="2512.24092">
        arXiv:2512.24092
      </a>
      
        [<a href="/pdf/2512.24092" title="Download PDF" id="pdf-2512.24092" aria-labelledby="pdf-2512.24092">pdf</a>, <a href="https://arxiv.org/html/2512.24092v1" title="View HTML" id="html-2512.24092" aria-labelledby="html-2512.24092" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24092" title="Other formats" id="oth-2512.24092" aria-labelledby="oth-2512.24092">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          HY-MT1.5 Technical Report
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+M">Mao Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z">Zheng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+T">Tao Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+M">Mingyang Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+D">Di Wang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          In this report, we introduce our latest translation models, HY-MT1.5-1.8B and HY-MT1.5-7B, a new family of machine translation models developed through a holistic training framework tailored for high-performance translation. Our methodology orchestrates a multi-stage pipeline that integrates general and MT-oriented pre-training, supervised fine-tuning, on-policy distillation, and reinforcement learning. HY-MT1.5-1.8B, the 1.8B-parameter model demonstrates remarkable parameter efficiency, comprehensively outperforming significantly larger open-source baselines (e.g., Tower-Plus-72B, Qwen3-32B) and mainstream commercial APIs (e.g., Microsoft Translator, Doubao Translator) in standard Chinese-foreign and English-foreign tasks. It achieves approximately 90% of the performance of ultra-large proprietary models such as Gemini-3.0-Pro, while marginally trailing Gemini-3.0-Pro on WMT25 and Mandarin-minority language benchmarks, it maintains a substantial lead over other competing models. Furthermore, HY-MT1.5-7B establishes a new state-of-the-art for its size class, achieving 95% of Gemini-3.0-Pro&#39;s performance on Flores-200 and surpassing it on the challenging WMT25 and Mandarin-minority language test sets. Beyond standard translation, the HY-MT1.5 series supports advanced constraints, including terminology intervention, context-aware translation, and format preservation. Extensive empirical evaluations confirm that both models offer highly competitive, robust solutions for general and specialized translation tasks within their respective parameter scales.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item27'>[27]</a>
      <a href ="/abs/2512.24098" title="Abstract" id="2512.24098">
        arXiv:2512.24098
      </a>
      
        [<a href="/pdf/2512.24098" title="Download PDF" id="pdf-2512.24098" aria-labelledby="pdf-2512.24098">pdf</a>, <a href="https://arxiv.org/html/2512.24098v1" title="View HTML" id="html-2512.24098" aria-labelledby="html-2512.24098" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24098" title="Other formats" id="oth-2512.24098" aria-labelledby="oth-2512.24098">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Training a Huggingface Model on AWS Sagemaker (Without Tears)
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+L">Liling Tan</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          The development of Large Language Models (LLMs) has primarily been driven by resource-rich research groups and industry partners. Due to the lack of on-premise computing resources required for increasingly complex models, many researchers are turning to cloud services like AWS SageMaker to train Hugging Face models. However, the steep learning curve of cloud platforms often presents a barrier for researchers accustomed to local environments. Existing documentation frequently leaves knowledge gaps, forcing users to seek fragmented information across the web. This demo paper aims to democratize cloud adoption by centralizing the essential information required for researchers to successfully train their first Hugging Face model on AWS SageMaker from scratch.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item28'>[28]</a>
      <a href ="/abs/2512.24143" title="Abstract" id="2512.24143">
        arXiv:2512.24143
      </a>
      
        [<a href="/pdf/2512.24143" title="Download PDF" id="pdf-2512.24143" aria-labelledby="pdf-2512.24143">pdf</a>, <a href="https://arxiv.org/html/2512.24143v1" title="View HTML" id="html-2512.24143" aria-labelledby="html-2512.24143" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24143" title="Other formats" id="oth-2512.24143" aria-labelledby="oth-2512.24143">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Activation Steering for Masked Diffusion Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shnaidman,+A">Adi Shnaidman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feiglin,+E">Erin Feiglin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yaari,+O">Osher Yaari</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mentel,+E">Efrat Mentel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Levi,+A">Amit Levi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lapid,+R">Raz Lapid</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Masked diffusion language models (MDLMs) generate text through an iterative denoising process. They have recently gained attention due to mask-parallel decoding and competitive performance with autoregressive large language models. However, effective mechanisms for inference-time control and steering in MDLMs remain largely unexplored. We present an activation-steering framework for MDLMs that computes layer-wise steering vectors from a single forward pass using contrastive examples, without simulating the denoising trajectory. These directions are applied at every reverse-diffusion step, yielding an efficient inference-time control mechanism. Experiments on LLaDA-8B-Instruct demonstrate reliable modulation of high-level attributes, with ablations examining the effects of steering across transformer sub-modules and token scope (prompt vs.\ response).
        </p>
      </div>
    </dd>
    <dt>
      <a name='item29'>[29]</a>
      <a href ="/abs/2512.24149" title="Abstract" id="2512.24149">
        arXiv:2512.24149
      </a>
      
        [<a href="/pdf/2512.24149" title="Download PDF" id="pdf-2512.24149" aria-labelledby="pdf-2512.24149">pdf</a>, <a href="https://arxiv.org/html/2512.24149v1" title="View HTML" id="html-2512.24149" aria-labelledby="html-2512.24149" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24149" title="Other formats" id="oth-2512.24149" aria-labelledby="oth-2512.24149">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Large Emotional World Model
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+C">Changhao Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yazhou Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+H">Hui Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+C">Chang Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+P">Peng Zhang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          World Models serve as tools for understanding the current state of the world and predicting its future dynamics, with broad application potential across numerous fields. As a key component of world knowledge, emotion significantly influences human decision-making. While existing Large Language Models (LLMs) have shown preliminary capability in capturing world knowledge, they primarily focus on modeling physical-world regularities and lack systematic exploration of emotional factors. In this paper, we first demonstrate the importance of emotion in understanding the world by showing that removing emotionally relevant information degrades reasoning performance. Inspired by theory of mind, we further propose a Large Emotional World Model (LEWM). Specifically, we construct the Emotion-Why-How (EWH) dataset, which integrates emotion into causal relationships and enables reasoning about why actions occur and how emotions drive future world states. Based on this dataset, LEWM explicitly models emotional states alongside visual observations and actions, allowing the world model to predict both future states and emotional transitions. Experimental results show that LEWM more accurately predicts emotion-driven social behaviors while maintaining comparable performance to general world models on basic tasks.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item30'>[30]</a>
      <a href ="/abs/2512.24157" title="Abstract" id="2512.24157">
        arXiv:2512.24157
      </a>
      
        [<a href="/pdf/2512.24157" title="Download PDF" id="pdf-2512.24157" aria-labelledby="pdf-2512.24157">pdf</a>, <a href="https://arxiv.org/html/2512.24157v1" title="View HTML" id="html-2512.24157" aria-labelledby="html-2512.24157" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24157" title="Other formats" id="oth-2512.24157" aria-labelledby="oth-2512.24157">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Training Report of TeleChat3-MoE
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+X">Xinzhang Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">Chao Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Z">Zhihao Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Z">Zhuo Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+X">Xuncheng Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H">Haoran Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+L">Lei Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+D">Dongdong He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+L">Luobin Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+K">Kaizhe Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+H">Han Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zihan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+Y">Yitong Yao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+S">Sishi Xiong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+W">Wenmin Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+H">Haowei He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+K">Kaidong Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Y">Yu Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+R">Ruiyu Fang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Y">Yuhao Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yingyan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+X">Xiaohui Hu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+X">Xi Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jingqi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">Yanwei Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Q">Qingli Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+X">Xinyu Shi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Niu,+J">Junhao Niu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+C">Chengnuo Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+Y">Yao Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+R">Ruiwen Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+F">Fengkai Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pu,+L">Luwen Pu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jia,+K">Kaipeng Jia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+F">Fubei Yao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Y">Yuyao Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+X">Xuewei He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Z">Zhuoru Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+R">Ruiting Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+R">Rui Xue</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+Q">Qiyi Xie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">Jie Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+Z">Zilu Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">Zhaoxi Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+Z">Zhilong Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yanhan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yin Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xue,+Y">Yanlei Xue</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+Z">Zhu Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+T">Teng Su</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+X">Xin Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+S">Shuangyong Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yongxiang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xuelong Li</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          TeleChat3-MoE is the latest series of TeleChat large language models, featuring a Mixture-of-Experts (MoE) architecture with parameter counts ranging from 105 billion to over one trillion,trained end-to-end on Ascend NPU cluster. This technical report mainly presents the underlying training infrastructure that enables reliable and efficient scaling to frontier model sizes. We detail systematic methodologies for operator-level and end-to-end numerical accuracy verification, ensuring consistency across hardware platforms and distributed parallelism strategies. Furthermore, we introduce a suite of performance optimizations, including interleaved pipeline scheduling, attention-aware data scheduling for long-sequence training,hierarchical and overlapped communication for expert parallelism, and DVM-based operator fusion. A systematic parallelization framework, leveraging analytical estimation and integer linear programming, is also proposed to optimize multi-dimensional parallelism configurations. Additionally, we present methodological approaches to cluster-level optimizations, addressing host- and device-bound bottlenecks during large-scale training tasks. These infrastructure advancements yield significant throughput improvements and near-linear scaling on clusters comprising thousands of devices, providing a robust foundation for large-scale language model development on hardware ecosystems.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item31'>[31]</a>
      <a href ="/abs/2512.24181" title="Abstract" id="2512.24181">
        arXiv:2512.24181
      </a>
      
        [<a href="/pdf/2512.24181" title="Download PDF" id="pdf-2512.24181" aria-labelledby="pdf-2512.24181">pdf</a>, <a href="https://arxiv.org/html/2512.24181v1" title="View HTML" id="html-2512.24181" aria-labelledby="html-2512.24181" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24181" title="Other formats" id="oth-2512.24181" aria-labelledby="oth-2512.24181">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MedKGI: Iterative Differential Diagnosis with Medical Knowledge Graphs and Information-Guided Inquiring
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Q">Qipeng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sheng,+R">Rui Sheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yafei Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+H">Huamin Qu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+Y">Yushi Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+M">Min Zhu</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Recent advancements in Large Language Models (LLMs) have demonstrated significant promise in clinical diagnosis. However, current models struggle to emulate the iterative, diagnostic hypothesis-driven reasoning of real clinical scenarios. Specifically, current LLMs suffer from three critical limitations: (1) generating hallucinated medical content due to weak grounding in verified knowledge, (2) asking redundant or inefficient questions rather than discriminative ones that hinder diagnostic progress, and (3) losing coherence over multi-turn dialogues, leading to contradictory or inconsistent conclusions. To address these challenges, we propose MedKGI, a diagnostic framework grounded in clinical practices. MedKGI integrates a medical knowledge graph (KG) to constrain reasoning to validated medical ontologies, selects questions based on information gain to maximize diagnostic efficiency, and adopts an OSCE-format structured state to maintain consistent evidence tracking across turns. Experiments on clinical benchmarks show that MedKGI outperforms strong LLM baselines in both diagnostic accuracy and inquiry efficiency, improving dialogue efficiency by 30% on average while maintaining state-of-the-art accuracy.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item32'>[32]</a>
      <a href ="/abs/2512.24235" title="Abstract" id="2512.24235">
        arXiv:2512.24235
      </a>
      
        [<a href="/pdf/2512.24235" title="Download PDF" id="pdf-2512.24235" aria-labelledby="pdf-2512.24235">pdf</a>, <a href="/format/2512.24235" title="Other formats" id="oth-2512.24235" aria-labelledby="oth-2512.24235">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          LAILA: A Large Trait-Based Dataset for Arabic Automated Essay Scoring
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bashendy,+M">May Bashendy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Massoud,+W">Walid Massoud</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Eltanbouly,+S">Sohaila Eltanbouly</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Albatarni,+S">Salam Albatarni</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sayed,+M">Marwan Sayed</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abir,+A">Abrar Abir</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bouamor,+H">Houda Bouamor</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Elsayed,+T">Tamer Elsayed</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Automated Essay Scoring (AES) has gained increasing attention in recent years, yet research on Arabic AES remains limited due to the lack of publicly available datasets. To address this, we introduce LAILA, the largest publicly available Arabic AES dataset to date, comprising 7,859 essays annotated with holistic and trait-specific scores on seven dimensions: relevance, organization, vocabulary, style, development, mechanics, and grammar. We detail the dataset design, collection, and annotations, and provide benchmark results using state-of-the-art Arabic and English models in prompt-specific and cross-prompt settings. LAILA fills a critical need in Arabic AES research, supporting the development of robust scoring systems.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item33'>[33]</a>
      <a href ="/abs/2512.24259" title="Abstract" id="2512.24259">
        arXiv:2512.24259
      </a>
      
        [<a href="/pdf/2512.24259" title="Download PDF" id="pdf-2512.24259" aria-labelledby="pdf-2512.24259">pdf</a>, <a href="https://arxiv.org/html/2512.24259v1" title="View HTML" id="html-2512.24259" aria-labelledby="html-2512.24259" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24259" title="Other formats" id="oth-2512.24259" aria-labelledby="oth-2512.24259">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Tracing the Flow of Knowledge From Science to Technology Using Deep Learning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rose,+M+E">Michael E. Rose</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ghosh,+M">Mainak Ghosh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Erhardt,+S">Sebastian Erhardt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C">Cheng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Buunk,+E">Erik Buunk</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Harhoff,+D">Dietmar Harhoff</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          4 tables, 7 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          We develop a language similarity model suitable for working with patents and scientific publications at the same time. In a horse race-style evaluation, we subject eight language (similarity) models to predict credible Patent-Paper Citations. We find that our Pat-SPECTER model performs best, which is the SPECTER2 model fine-tuned on patents. In two real-world scenarios (separating patent-paper-pairs and predicting patent-paper-pairs) we demonstrate the capabilities of the Pat-SPECTER. We finally test the hypothesis that US patents cite papers that are semantically less similar than in other large jurisdictions, which we posit is because of the duty of candor. The model is open for the academic community and practitioners alike.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item34'>[34]</a>
      <a href ="/abs/2512.24265" title="Abstract" id="2512.24265">
        arXiv:2512.24265
      </a>
      
        [<a href="/pdf/2512.24265" title="Download PDF" id="pdf-2512.24265" aria-labelledby="pdf-2512.24265">pdf</a>, <a href="https://arxiv.org/html/2512.24265v1" title="View HTML" id="html-2512.24265" aria-labelledby="html-2512.24265" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24265" title="Other formats" id="oth-2512.24265" aria-labelledby="oth-2512.24265">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Joint Selection for Large-Scale Pre-Training Data via Policy Gradient-based Mask Learning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+Z">Ziqing Fan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xian,+Y">Yuqiao Xian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+Y">Yan Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+L">Li Shen</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          A fine-grained data recipe is crucial for pre-training large language models, as it can significantly enhance training efficiency and model performance. One important ingredient in the recipe is to select samples based on scores produced by defined rules, LLM judgment, or statistical information in embeddings, which can be roughly categorized into quality and diversity metrics. Due to the high computational cost when applied to trillion-scale token pre-training datasets such as FineWeb and DCLM, these two or more types of metrics are rarely considered jointly in a single selection process. However, in our empirical study, selecting samples based on quality metrics exhibit severe diminishing returns during long-term pre-training, while selecting on diversity metrics removes too many valuable high-quality samples, both of which limit pre-trained LLMs&#39; capabilities. Therefore, we introduce DATAMASK, a novel and efficient joint learning framework designed for large-scale pre-training data selection that can simultaneously optimize multiple types of metrics in a unified process, with this study focusing specifically on quality and diversity metrics. DATAMASK approaches the selection process as a mask learning problem, involving iterative sampling of data masks, computation of policy gradients based on predefined objectives with sampled masks, and updating of mask sampling logits. Through policy gradient-based optimization and various acceleration enhancements, it significantly reduces selection time by 98.9% compared to greedy algorithm, enabling our study to explore joint learning within trillion-scale tokens. With DATAMASK, we select a subset of about 10% from the 15 trillion-token FineWeb dataset, termed FineWeb-Mask. Evaluated across 12 diverse tasks, we achieves significant improvements of 3.2% on a 1.5B dense model and 1.9% on a 7B MoE model.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item35'>[35]</a>
      <a href ="/abs/2512.24289" title="Abstract" id="2512.24289">
        arXiv:2512.24289
      </a>
      
        [<a href="/pdf/2512.24289" title="Download PDF" id="pdf-2512.24289" aria-labelledby="pdf-2512.24289">pdf</a>, <a href="https://arxiv.org/html/2512.24289v1" title="View HTML" id="html-2512.24289" aria-labelledby="html-2512.24289" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24289" title="Other formats" id="oth-2512.24289" aria-labelledby="oth-2512.24289">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Automated Analysis of Sustainability Reports: Using Large Language Models for the Extraction and Prediction of EU Taxonomy-Compliant KPIs
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schmoll,+J">Jonathan Schmoll</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jatowt,+A">Adam Jatowt</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          The manual, resource-intensive process of complying with the EU Taxonomy presents a significant challenge for companies. While Large Language Models (LLMs) offer a path to automation, research is hindered by a lack of public benchmark datasets. To address this gap, we introduce a novel, structured dataset from 190 corporate reports, containing ground-truth economic activities and quantitative Key Performance Indicators (KPIs). We use this dataset to conduct the first systematic evaluation of LLMs on the core compliance workflow. Our results reveal a clear performance gap between qualitative and quantitative tasks. LLMs show moderate success in the qualitative task of identifying economic activities, with a multi-step agentic framework modestly enhancing precision. Conversely, the models comprehensively fail at the quantitative task of predicting financial KPIs in a zero-shot setting. We also discover a paradox, where concise metadata often yields superior performance to full, unstructured reports, and find that model confidence scores are poorly calibrated. We conclude that while LLMs are not ready for full automation, they can serve as powerful assistive tools for human experts. Our dataset provides a public benchmark for future research.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item36'>[36]</a>
      <a href ="/abs/2512.24297" title="Abstract" id="2512.24297">
        arXiv:2512.24297
      </a>
      
        [<a href="/pdf/2512.24297" title="Download PDF" id="pdf-2512.24297" aria-labelledby="pdf-2512.24297">pdf</a>, <a href="https://arxiv.org/html/2512.24297v1" title="View HTML" id="html-2512.24297" aria-labelledby="html-2512.24297" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24297" title="Other formats" id="oth-2512.24297" aria-labelledby="oth-2512.24297">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Figure It Out: Improving the Frontier of Reasoning with Active Visual Thinking
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+M">Meiqi Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Meng,+F">Fandong Meng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+J">Jie Zhou</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Complex reasoning problems often involve implicit spatial, geometric, and structural relationships that are not explicitly encoded in text. While recent reasoning models have achieved strong performance across many domains, purely text-based reasoning struggles to represent global structural constraints in complex settings. In this paper, we introduce FIGR, which integrates active visual thinking into multi-turn reasoning via end-to-end reinforcement learning. FIGR externalizes intermediate structural hypotheses by constructing visual representations during problem solving. By adaptively regulating when and how visual reasoning should be invoked, FIGR enables more stable and coherent reasoning over global structural properties that are difficult to capture from text alone. Experiments on challenging mathematical reasoning benchmarks demonstrate that FIGR outperforms strong text-only chain-of-thought baselines. In particular, FIGR improves the base model by 13.12% on AIME 2025 and 11.00% on BeyondAIME, highlighting the effectiveness of figure-guided multimodal reasoning in enhancing the stability and reliability of complex reasoning.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item37'>[37]</a>
      <a href ="/abs/2512.24314" title="Abstract" id="2512.24314">
        arXiv:2512.24314
      </a>
      
        [<a href="/pdf/2512.24314" title="Download PDF" id="pdf-2512.24314" aria-labelledby="pdf-2512.24314">pdf</a>, <a href="https://arxiv.org/html/2512.24314v1" title="View HTML" id="html-2512.24314" aria-labelledby="html-2512.24314" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24314" title="Other formats" id="oth-2512.24314" aria-labelledby="oth-2512.24314">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          QianfanHuijin Technical Report: A Novel Multi-Stage Training Paradigm for Finance Industrial LLMs
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S">Shupeng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+W">Weipeng Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+L">Linyun Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+C">Chen Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S">Shaofei Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+Z">Zhendong Tan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhong,+H">Hanjun Zhong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng,+Y">Yucheng Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+C">Chenghao Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+M">Mengyue Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+D">Daxiang Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+J">Jianmin Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+Y">Yunting Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+A">Annan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+D">Danyu Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">Jingnan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+L">Licen Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+D">Dawei Yin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+D">Dou Shen</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Domain-specific enhancement of Large Language Models (LLMs) within the financial context has long been a focal point of industrial application. While previous models such as BloombergGPT and Baichuan-Finance primarily focused on knowledge enhancement, the deepening complexity of financial services has driven a growing demand for models that possess not only domain knowledge but also robust financial reasoning and agentic capabilities. In this paper, we present QianfanHuijin, a financial domain LLM, and propose a generalizable multi-stage training paradigm for industrial model enhancement.
<br>Our approach begins with Continual Pre-training (CPT) on financial corpora to consolidate the knowledge base. This is followed by a fine-grained Post-training pipeline designed with increasing specificity: starting with Financial SFT, progressing to Finance Reasoning RL and Finance Agentic RL, and culminating in General RL aligned with real-world business scenarios. Empirical results demonstrate that QianfanHuijin achieves superior performance across various authoritative financial benchmarks. Furthermore, ablation studies confirm that the targeted Reasoning RL and Agentic RL stages yield significant gains in their respective capabilities. These findings validate our motivation and suggest that this fine-grained, progressive post-training methodology is poised to become a mainstream paradigm for various industrial-enhanced LLMs.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item38'>[38]</a>
      <a href ="/abs/2512.24329" title="Abstract" id="2512.24329">
        arXiv:2512.24329
      </a>
      
        [<a href="/pdf/2512.24329" title="Download PDF" id="pdf-2512.24329" aria-labelledby="pdf-2512.24329">pdf</a>, <a href="https://arxiv.org/html/2512.24329v1" title="View HTML" id="html-2512.24329" aria-labelledby="html-2512.24329" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24329" title="Other formats" id="oth-2512.24329" aria-labelledby="oth-2512.24329">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          World model inspired sarcasm reasoning with large language model agents
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Inoshita,+K">Keito Inoshita</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mizuno,+S">Shinnosuke Mizuno</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Sarcasm understanding is a challenging problem in natural language processing, as it requires capturing the discrepancy between the surface meaning of an utterance and the speaker&#39;s intentions as well as the surrounding social context. Although recent advances in deep learning and Large Language Models (LLMs) have substantially improved performance, most existing approaches still rely on black-box predictions of a single model, making it difficult to structurally explain the cognitive factors underlying sarcasm. Moreover, while sarcasm often emerges as a mismatch between semantic evaluation and normative expectations or intentions, frameworks that explicitly decompose and model these components remain limited. In this work, we reformulate sarcasm understanding as a world model inspired reasoning process and propose World Model inspired SArcasm Reasoning (WM-SAR), which decomposes literal meaning, context, normative expectation, and intention into specialized LLM-based agents. The discrepancy between literal evaluation and normative expectation is explicitly quantified as a deterministic inconsistency score, and together with an intention score, these signals are integrated by a lightweight Logistic Regression model to infer the final sarcasm probability. This design leverages the reasoning capability of LLMs while maintaining an interpretable numerical decision structure. Experiments on representative sarcasm detection benchmarks show that WM-SAR consistently outperforms existing deep learning and LLM-based methods. Ablation studies and case analyses further demonstrate that integrating semantic inconsistency and intention reasoning is essential for effective sarcasm detection, achieving both strong performance and high interpretability.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item39'>[39]</a>
      <a href ="/abs/2512.24373" title="Abstract" id="2512.24373">
        arXiv:2512.24373
      </a>
      
        [<a href="/pdf/2512.24373" title="Download PDF" id="pdf-2512.24373" aria-labelledby="pdf-2512.24373">pdf</a>, <a href="https://arxiv.org/html/2512.24373v1" title="View HTML" id="html-2512.24373" aria-labelledby="html-2512.24373" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24373" title="Other formats" id="oth-2512.24373" aria-labelledby="oth-2512.24373">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Skim-Aware Contrastive Learning for Efficient Document Representation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abro,+W+A">Waheed Ahmed Abro</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bouraoui,+Z">Zied Bouraoui</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Although transformer-based models have shown strong performance in word- and sentence-level tasks, effectively representing long documents, especially in fields like law and medicine, remains difficult. Sparse attention mechanisms can handle longer inputs, but are resource-intensive and often fail to capture full-document context. Hierarchical transformer models offer better efficiency but do not clearly explain how they relate different sections of a document. In contrast, humans often skim texts, focusing on important sections to understand the overall message. Drawing from this human strategy, we introduce a new self-supervised contrastive learning framework that enhances long document representation. Our method randomly masks a section of the document and uses a natural language inference (NLI)-based contrastive objective to align it with relevant parts while distancing it from unrelated ones. This mimics how humans synthesize information, resulting in representations that are both richer and more computationally efficient. Experiments on legal and biomedical texts confirm significant gains in both accuracy and efficiency.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item40'>[40]</a>
      <a href ="/abs/2512.24410" title="Abstract" id="2512.24410">
        arXiv:2512.24410
      </a>
      
        [<a href="/pdf/2512.24410" title="Download PDF" id="pdf-2512.24410" aria-labelledby="pdf-2512.24410">pdf</a>, <a href="https://arxiv.org/html/2512.24410v1" title="View HTML" id="html-2512.24410" aria-labelledby="html-2512.24410" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24410" title="Other formats" id="oth-2512.24410" aria-labelledby="oth-2512.24410">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Comparing Approaches to Automatic Summarization in Less-Resourced Languages
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Palen-Michel,+C">Chester Palen-Michel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lignos,+C">Constantine Lignos</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Under review
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Automatic text summarization has achieved high performance in high-resourced languages like English, but comparatively less attention has been given to summarization in less-resourced languages. This work compares a variety of different approaches to summarization from zero-shot prompting of LLMs large and small to fine-tuning smaller models like mT5 with and without three data augmentation approaches and multilingual transfer. We also explore an LLM translation pipeline approach, translating from the source language to English, summarizing and translating back. Evaluating with five different metrics, we find that there is variation across LLMs in their performance across similar parameter sizes, that our multilingual fine-tuned mT5 baseline outperforms most other approaches including zero-shot LLM performance for most metrics, and that LLM as judge may be less reliable on less-resourced languages.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item41'>[41]</a>
      <a href ="/abs/2512.24459" title="Abstract" id="2512.24459">
        arXiv:2512.24459
      </a>
      
        [<a href="/pdf/2512.24459" title="Download PDF" id="pdf-2512.24459" aria-labelledby="pdf-2512.24459">pdf</a>, <a href="https://arxiv.org/html/2512.24459v1" title="View HTML" id="html-2512.24459" aria-labelledby="html-2512.24459" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24459" title="Other formats" id="oth-2512.24459" aria-labelledby="oth-2512.24459">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Cleaning English Abstracts of Scientific Publications
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rose,+M+E">Michael E. Rose</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Herrmann,+N+A">Nils A. Herrmann</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Erhardt,+S">Sebastian Erhardt</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          2 tables, 2 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Scientific abstracts are often used as proxies for the content and thematic focus of research publications. However, a significant share of published abstracts contains extraneous information-such as publisher copyright statements, section headings, author notes, registrations, and bibliometric or bibliographic metadata-that can distort downstream analyses, particularly those involving document similarity or textual embeddings. We introduce an open-source, easy-to-integrate language model designed to clean English-language scientific abstracts by automatically identifying and removing such clutter. We demonstrate that our model is both conservative and precise, alters similarity rankings of cleaned abstracts and improves information content of standard-length embeddings.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item42'>[42]</a>
      <a href ="/abs/2512.24460" title="Abstract" id="2512.24460">
        arXiv:2512.24460
      </a>
      
        [<a href="/pdf/2512.24460" title="Download PDF" id="pdf-2512.24460" aria-labelledby="pdf-2512.24460">pdf</a>, <a href="https://arxiv.org/html/2512.24460v1" title="View HTML" id="html-2512.24460" aria-labelledby="html-2512.24460" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24460" title="Other formats" id="oth-2512.24460" aria-labelledby="oth-2512.24460">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          IELTS Writing Revision Platform with Automated Essay Scoring and Adaptive Feedback
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramancauskas,+T">Titas Ramancauskas</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramancauske,+K">Kotryna Ramancauske</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Human-Computer Interaction (cs.HC)
        </div>

        <p class='mathjax'>
          This paper presents the design, development, and evaluation of a proposed revision platform assisting candidates for the International English Language Testing System (IELTS) writing exam. Traditional IELTS preparation methods lack personalised feedback, catered to the IELTS writing rubric. To address these shortcomings, the platform features an attractive user interface (UI), an Automated Essay Scoring system (AES), and targeted feedback tailored to candidates and the IELTS writing rubric. The platform architecture separates conversational guidance from a dedicated writing interface to reduce cognitive load and simulate exam conditions. Through iterative, Design-Based Research (DBR) cycles, the study progressed from rule-based to transformer-based with a regression head scoring, mounted with adaptive feedback.
<br>Early cycles (2-3) revealed fundamental limitations of rule-based approaches: mid-band compression, low accuracy, and negative $R^2$ values. DBR Cycle 4 implemented a DistilBERT transformer model with a regression head, yielding substantial improvements with MAE of 0.66 and positive $R^2$. This enabled Cycle 5&#39;s adaptive feedback implementation, which demonstrated statistically significant score improvements (mean +0.060 bands, p = 0.011, Cohen&#39;s d = 0.504), though effectiveness varied by revision strategy. Findings suggest automated feedback functions are most suited as a supplement to human instruction, with conservative surface-level corrections proving more reliable than aggressive structural interventions for IELTS preparation contexts. Challenges remain in assessing higher-band essays, and future work should incorporate longitudinal studies with real IELTS candidates and validation from official examiners.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item43'>[43]</a>
      <a href ="/abs/2512.24517" title="Abstract" id="2512.24517">
        arXiv:2512.24517
      </a>
      
        [<a href="/pdf/2512.24517" title="Download PDF" id="pdf-2512.24517" aria-labelledby="pdf-2512.24517">pdf</a>, <a href="https://arxiv.org/html/2512.24517v1" title="View HTML" id="html-2512.24517" aria-labelledby="html-2512.24517" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24517" title="Other formats" id="oth-2512.24517" aria-labelledby="oth-2512.24517">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Paragraph Segmentation Revisited: Towards a Standard Task for Structuring Speech
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Retkowski,+F">Fabian Retkowski</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Waibel,+A">Alexander Waibel</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Automatic speech transcripts are often delivered as unstructured word streams that impede readability and repurposing. We recast paragraph segmentation as the missing structuring step and fill three gaps at the intersection of speech processing and text segmentation. First, we establish TEDPara (human-annotated TED talks) and YTSegPara (YouTube videos with synthetic labels) as the first benchmarks for the paragraph segmentation task. The benchmarks focus on the underexplored speech domain, where paragraph segmentation has traditionally not been part of post-processing, while also contributing to the wider text segmentation field, which still lacks robust and naturalistic benchmarks. Second, we propose a constrained-decoding formulation that lets large language models insert paragraph breaks while preserving the original transcript, enabling faithful, sentence-aligned evaluation. Third, we show that a compact model (MiniSeg) attains state-of-the-art accuracy and, when extended hierarchically, jointly predicts chapters and paragraphs with minimal computational cost. Together, our resources and methods establish paragraph segmentation as a standardized, practical task in speech processing.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item44'>[44]</a>
      <a href ="/abs/2512.24556" title="Abstract" id="2512.24556">
        arXiv:2512.24556
      </a>
      
        [<a href="/pdf/2512.24556" title="Download PDF" id="pdf-2512.24556" aria-labelledby="pdf-2512.24556">pdf</a>, <a href="https://arxiv.org/html/2512.24556v1" title="View HTML" id="html-2512.24556" aria-labelledby="html-2512.24556" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24556" title="Other formats" id="oth-2512.24556" aria-labelledby="oth-2512.24556">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Safe in the Future, Dangerous in the Past: Dissecting Temporal and Linguistic Vulnerabilities in LLMs
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Said,+M+A">Muhammad Abdullahi Said</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sani,+M+S">Muhammad Sammani Sani</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          As Large Language Models (LLMs) integrate into critical global infrastructure, the assumption that safety alignment transfers zero-shot from English to other languages remains a dangerous blind spot. This study presents a systematic audit of three state of the art models (GPT-5.1, Gemini 3 Pro, and Claude 4.5 Opus) using HausaSafety, a novel adversarial dataset grounded in West African threat scenarios (e.g., Yahoo-Yahoo fraud, Dane gun manufacturing). Employing a 2 x 4 factorial design across 1,440 evaluations, we tested the non-linear interaction between language (English vs. Hausa) and temporal framing. Our results challenge the prevailing multilingual safety gap narrative. Instead of a simple degradation in low-resource settings, we identified a mechanism of Complex Interference where safety is determined by the intersection of variables. While models exhibited a Reverse Linguistic with Claude 4.5 Opus proving significantly safer in Hausa (45.0%) than in English (36.7%) due to uncertainty-driven refusal they suffered catastrophic failures in temporal reasoning. We report a profound Temporal Asymmetry, where past-tense framing bypassed defenses (15.6% safe) while future-tense scenarios triggered hyper-conservative refusals (57.2% safe). The magnitude of this volatility is illustrated by a 9.2x disparity between the safest and most vulnerable configurations, proving that safety is not a fixed property but a context-dependent state. We conclude that current models rely on superficial heuristics rather than robust semantic understanding, creating Safety Pockets that leave Global South users exposed to localized harms. We propose Invariant Alignment as a necessary paradigm shift to ensure safety stability across linguistic and temporal shifts.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item45'>[45]</a>
      <a href ="/abs/2512.24562" title="Abstract" id="2512.24562">
        arXiv:2512.24562
      </a>
      
        [<a href="/pdf/2512.24562" title="Download PDF" id="pdf-2512.24562" aria-labelledby="pdf-2512.24562">pdf</a>, <a href="https://arxiv.org/html/2512.24562v1" title="View HTML" id="html-2512.24562" aria-labelledby="html-2512.24562" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24562" title="Other formats" id="oth-2512.24562" aria-labelledby="oth-2512.24562">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          HaluNet: Multi-Granular Uncertainty Modeling for Efficient Hallucination Detection in LLM Question Answering
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tong,+C">Chaodong Tong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Q">Qi Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+J">Jiayang Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+L">Lei Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">Yanbing Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+N">Nannan Sun</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          13 pages, 5 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Large Language Models (LLMs) excel at question answering (QA) but often generate hallucinations, including factual errors or fabricated content. Detecting hallucinations from internal uncertainty signals is attractive due to its scalability and independence from external resources. Existing methods often aim to accurately capture a single type of uncertainty while overlooking the complementarity among different sources, particularly between token-level probability uncertainty and the uncertainty conveyed by internal semantic representations, which provide complementary views on model reliability. We present \textbf{HaluNet}, a lightweight and trainable neural framework that integrates multi granular token level uncertainties by combining semantic embeddings with probabilistic confidence and distributional uncertainty. Its multi branch architecture adaptively fuses what the model knows with the uncertainty expressed in its outputs, enabling efficient one pass hallucination detection. Experiments on SQuAD, TriviaQA, and Natural Questions show that HaluNet delivers strong detection performance and favorable computational efficiency, with or without access to context, highlighting its potential for real time hallucination detection in LLM based QA systems.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item46'>[46]</a>
      <a href ="/abs/2512.24572" title="Abstract" id="2512.24572">
        arXiv:2512.24572
      </a>
      
        [<a href="/pdf/2512.24572" title="Download PDF" id="pdf-2512.24572" aria-labelledby="pdf-2512.24572">pdf</a>, <a href="https://arxiv.org/html/2512.24572v1" title="View HTML" id="html-2512.24572" aria-labelledby="html-2512.24572" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24572" title="Other formats" id="oth-2512.24572" aria-labelledby="oth-2512.24572">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Korean Canonical Legal Benchmark: Toward Knowledge-Independent Evaluation of LLMs&#39; Legal Reasoning Capabilities
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Oh,+H">Hongseok Oh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hwang,+W">Wonseok Hwang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=On,+K">Kyoung-Woon On</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          We introduce the Korean Canonical Legal Benchmark (KCL), a benchmark designed to assess language models&#39; legal reasoning capabilities independently of domain-specific knowledge. KCL provides question-level supporting precedents, enabling a more faithful disentanglement of reasoning ability from parameterized knowledge. KCL consists of two components: (1) KCL-MCQA, multiple-choice problems of 283 questions with 1,103 aligned precedents, and (2) KCL-Essay, open-ended generation problems of 169 questions with 550 aligned precedents and 2,739 instance-level rubrics for automated evaluation. Our systematic evaluation of 30+ models shows large remaining gaps, particularly in KCL-Essay, and that reasoning-specialized models consistently outperform their general-purpose counterparts. We release all resources, including the benchmark dataset and evaluation code, at <a href="https://github.com/lbox-kr/kcl" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item47'>[47]</a>
      <a href ="/abs/2512.24574" title="Abstract" id="2512.24574">
        arXiv:2512.24574
      </a>
      
        [<a href="/pdf/2512.24574" title="Download PDF" id="pdf-2512.24574" aria-labelledby="pdf-2512.24574">pdf</a>, <a href="https://arxiv.org/html/2512.24574v1" title="View HTML" id="html-2512.24574" aria-labelledby="html-2512.24574" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24574" title="Other formats" id="oth-2512.24574" aria-labelledby="oth-2512.24574">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Understanding and Steering the Cognitive Behaviors of Reasoning Models at Test-Time
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">Zhenyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+X">Xiaoxia Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Z">Zhongzhu Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Q">Qingyang Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yineng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ponnusamy,+P">Pragaash Ponnusamy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Subbaraj,+H">Harikaran Subbaraj</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">Jue Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+S+L">Shuaiwen Leon Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Athiwaratkun,+B">Ben Athiwaratkun</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Large Language Models (LLMs) often rely on long chain-of-thought (CoT) reasoning to solve complex tasks. While effective, these trajectories are frequently inefficient, leading to high latency from excessive token generation, or unstable reasoning that alternates between underthinking (shallow, inconsistent steps) and overthinking (repetitive, verbose reasoning). In this work, we study the structure of reasoning trajectories and uncover specialized attention heads that correlate with distinct cognitive behaviors such as verification and backtracking. By lightly intervening on these heads at inference time, we can steer the model away from inefficient modes. Building on this insight, we propose CREST, a training-free method for Cognitive REasoning Steering at Test-time. CREST has two components: (1) an offline calibration step that identifies cognitive heads and derives head-specific steering vectors, and (2) an inference-time procedure that rotates hidden representations to suppress components along those vectors. CREST adaptively suppresses unproductive reasoning behaviors, yielding both higher accuracy and lower computational cost. Across diverse reasoning benchmarks and models, CREST improves accuracy by up to 17.5% while reducing token usage by 37.6%, offering a simple and effective pathway to faster, more reliable LLM reasoning.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item48'>[48]</a>
      <a href ="/abs/2512.24618" title="Abstract" id="2512.24618">
        arXiv:2512.24618
      </a>
      
        [<a href="/pdf/2512.24618" title="Download PDF" id="pdf-2512.24618" aria-labelledby="pdf-2512.24618">pdf</a>, <a href="https://arxiv.org/html/2512.24618v1" title="View HTML" id="html-2512.24618" aria-labelledby="html-2512.24618" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24618" title="Other formats" id="oth-2512.24618" aria-labelledby="oth-2512.24618">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Youtu-LLM: Unlocking the Native Agentic Potential for Lightweight Large Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+J">Junru Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin,+J">Jiarui Qin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiao,+L">Lingfeng Qiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yinghui Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+X">Xinyi Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ke,+B">Bo Ke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+J">Jianfeng He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiao,+R">Ruizhi Qiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+D">Di Yin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+X">Xing Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Y">Yunsheng Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">Yinsong Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S">Shuangyin Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+M">Mingkong Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+H">Haodong Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kuang,+J">Jiayi Kuang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Meng,+F">Fanxu Meng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+X">Xiaojuan Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xi,+Y">Yunjia Xi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+J">Junjie Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+H">Haotong Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+Z">Zhenyi Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yangning Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Q">Qianwen Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+Y">Yifei Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=An,+S">Siyu An</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+J">Junnan Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Q">Qiufeng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">Jie Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+K">Keyu Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wen,+W">Wei Wen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+T">Taian Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+Z">Zhifeng Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+D">Daohai Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jiahao Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+K">Ke Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z">Zongyi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+X">Xiaoyu Tan</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          57 pages, 26 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          We introduce Youtu-LLM, a lightweight yet powerful language model that harmonizes high computational efficiency with native agentic intelligence. Unlike typical small models that rely on distillation, Youtu-LLM (1.96B) is pre-trained from scratch to systematically cultivate reasoning and planning capabilities. The key technical advancements are as follows: (1) Compact Architecture with Long-Context Support: Built on a dense Multi-Latent Attention (MLA) architecture with a novel STEM-oriented vocabulary, Youtu-LLM supports a 128k context window. This design enables robust long-context reasoning and state tracking within a minimal memory footprint, making it ideal for long-horizon agent and reasoning tasks. (2) Principled &#34;Commonsense-STEM-Agent&#34; Curriculum: We curated a massive corpus of approximately 11T tokens and implemented a multi-stage training strategy. By progressively shifting the pre-training data distribution from general commonsense to complex STEM and agentic tasks, we ensure the model acquires deep cognitive abilities rather than superficial alignment. (3) Scalable Agentic Mid-training: Specifically for the agentic mid-training, we employ diverse data construction schemes to synthesize rich and varied trajectories across math, coding, and tool-use domains. This high-quality data enables the model to internalize planning and reflection behaviors effectively. Extensive evaluations show that Youtu-LLM sets a new state-of-the-art for sub-2B LLMs. On general benchmarks, it achieves competitive performance against larger models, while on agent-specific tasks, it significantly surpasses existing SOTA baselines, demonstrating that lightweight models can possess strong intrinsic agentic capabilities.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item49'>[49]</a>
      <a href ="/abs/2512.24661" title="Abstract" id="2512.24661">
        arXiv:2512.24661
      </a>
      
        [<a href="/pdf/2512.24661" title="Download PDF" id="pdf-2512.24661" aria-labelledby="pdf-2512.24661">pdf</a>, <a href="https://arxiv.org/html/2512.24661v1" title="View HTML" id="html-2512.24661" aria-labelledby="html-2512.24661" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24661" title="Other formats" id="oth-2512.24661" aria-labelledby="oth-2512.24661">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Do Large Language Models Know What They Are Capable Of?
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barkan,+C+O">Casey O. Barkan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Black,+S">Sid Black</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sourbut,+O">Oliver Sourbut</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          23 pages, 8 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          We investigate whether large language models (LLMs) can predict whether they will succeed on a given task and whether their predictions improve as they progress through multi-step tasks. We also investigate whether LLMs can learn from in-context experiences to make better decisions about whether to pursue a task in scenarios where failure is costly. All LLMs we tested are overconfident, but most predict their success with better-than-random discriminatory power. We find that newer and larger LLMs generally do not have greater discriminatory power, though Claude models do show such a trend. On multi-step agentic tasks, the overconfidence of several frontier LLMs worsens as they progress through the tasks, and reasoning LLMs perform comparably to or worse than non-reasoning LLMs. With in-context experiences of failure, some but not all LLMs reduce their overconfidence leading to significantly improved decision making, while others do not. Interestingly, all LLMs&#39; decisions are approximately rational given their estimated probabilities of success, yet their overly-optimistic estimates result in poor decision making. These results suggest that current LLM agents are hindered by their lack of awareness of their own capabilities. We discuss the implications of LLMs&#39; awareness of their capabilities for AI misuse and misalignment risks.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item50'>[50]</a>
      <a href ="/abs/2512.24684" title="Abstract" id="2512.24684">
        arXiv:2512.24684
      </a>
      
        [<a href="/pdf/2512.24684" title="Download PDF" id="pdf-2512.24684" aria-labelledby="pdf-2512.24684">pdf</a>, <a href="https://arxiv.org/html/2512.24684v1" title="View HTML" id="html-2512.24684" aria-labelledby="html-2512.24684" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24684" title="Other formats" id="oth-2512.24684" aria-labelledby="oth-2512.24684">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          R-Debater: Retrieval-Augmented Debate Generation through Argumentative Memory
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+M">Maoyuan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zhongsheng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H">Haoyuan Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">Jiamou Liu</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Accepteed by AAMAS 2026 full paper
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          We present R-Debater, an agentic framework for generating multi-turn debates built on argumentative memory. Grounded in rhetoric and memory studies, the system views debate as a process of recalling and adapting prior arguments to maintain stance consistency, respond to opponents, and support claims with evidence. Specifically, R-Debater integrates a debate knowledge base for retrieving case-like evidence and prior debate moves with a role-based agent that composes coherent utterances across turns. We evaluate on standardized ORCHID debates, constructing a 1,000-item retrieval corpus and a held-out set of 32 debates across seven domains. Two tasks are evaluated: next-utterance generation, assessed by InspireScore (subjective, logical, and factual), and adversarial multi-turn simulations, judged by Debatrix (argument, source, language, and overall). Compared with strong LLM baselines, R-Debater achieves higher single-turn and multi-turn scores. Human evaluation with 20 experienced debaters further confirms its consistency and evidence use, showing that combining retrieval grounding with structured planning yields more faithful, stance-aligned, and coherent debates across turns.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item51'>[51]</a>
      <a href ="/abs/2512.24693" title="Abstract" id="2512.24693">
        arXiv:2512.24693
      </a>
      
        [<a href="/pdf/2512.24693" title="Download PDF" id="pdf-2512.24693" aria-labelledby="pdf-2512.24693">pdf</a>, <a href="https://arxiv.org/html/2512.24693v1" title="View HTML" id="html-2512.24693" aria-labelledby="html-2512.24693" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24693" title="Other formats" id="oth-2512.24693" aria-labelledby="oth-2512.24693">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MUSIC: MUlti-Step Instruction Contrast for Multi-Turn Reward Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+W">Wenzhe Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Shujian Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+W">Wenxuan Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lambert,+J">John Lambert</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+C">Chi Jin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hard,+A">Andrew Hard</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mathews,+R">Rajiv Mathews</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L">Lun Wang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Evaluating the quality of multi-turn conversations is crucial for developing capable Large Language Models (LLMs), yet remains a significant challenge, often requiring costly human evaluation. Multi-turn reward models (RMs) offer a scalable alternative and can provide valuable signals for guiding LLM training. While recent work has advanced multi-turn \textit{training} techniques, effective automated \textit{evaluation} specifically for multi-turn interactions lags behind. We observe that standard preference datasets, typically contrasting responses based only on the final conversational turn, provide insufficient signal to capture the nuances of multi-turn interactions. Instead, we find that incorporating contrasts spanning \textit{multiple} turns is critical for building robust multi-turn RMs. Motivated by this finding, we propose \textbf{MU}lti-\textbf{S}tep \textbf{I}nstruction \textbf{C}ontrast (MUSIC), an unsupervised data augmentation strategy that synthesizes contrastive conversation pairs exhibiting differences across multiple turns. Leveraging MUSIC on the Skywork preference dataset, we train a multi-turn RM based on the Gemma-2-9B-Instruct model. Empirical results demonstrate that our MUSIC-augmented RM outperforms baseline methods, achieving higher alignment with judgments from advanced proprietary LLM judges on multi-turn conversations, crucially, without compromising performance on standard single-turn RM benchmarks.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item52'>[52]</a>
      <a href ="/abs/2512.24733" title="Abstract" id="2512.24733">
        arXiv:2512.24733
      </a>
      
        [<a href="/pdf/2512.24733" title="Download PDF" id="pdf-2512.24733" aria-labelledby="pdf-2512.24733">pdf</a>, <a href="https://arxiv.org/html/2512.24733v1" title="View HTML" id="html-2512.24733" aria-labelledby="html-2512.24733" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24733" title="Other formats" id="oth-2512.24733" aria-labelledby="oth-2512.24733">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          BIOME-Bench: A Benchmark for Biomolecular Interaction Inference and Multi-Omics Pathway Mechanism Elucidation from Scientific Literature
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+S">Sibo Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+P">Peng Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+L">Lifeng Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+Y">Yin Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L">Lei Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+P">Peng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+W">Wenpeng Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+J">Jianbin Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+H">Hongjun Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng,+D">Dajun Zeng</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Multi-omics studies often rely on pathway enrichment to interpret heterogeneous molecular changes, but pathway enrichment (PE)-based workflows inherit structural limitations of pathway resources, including curation lag, functional redundancy, and limited sensitivity to molecular states and interventions. Although recent work has explored using large language models (LLMs) to improve PE-based interpretation, the lack of a standardized benchmark for end-to-end multi-omics pathway mechanism elucidation has largely confined evaluation to small, manually curated datasets or ad hoc case studies, hindering reproducible progress. To address this issue, we introduce BIOME-Bench, constructed via a rigorous four-stage workflow, to evaluate two core capabilities of LLMs in multi-omics analysis: Biomolecular Interaction Inference and end-to-end Multi-Omics Pathway Mechanism Elucidation. We develop evaluation protocols for both tasks and conduct comprehensive experiments across multiple strong contemporary models. Experimental results demonstrate that existing models still exhibit substantial deficiencies in multi-omics analysis, struggling to reliably distinguish fine-grained biomolecular relation types and to generate faithful, robust pathway-level mechanistic explanations.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item53'>[53]</a>
      <a href ="/abs/2512.24772" title="Abstract" id="2512.24772">
        arXiv:2512.24772
      </a>
      
        [<a href="/pdf/2512.24772" title="Download PDF" id="pdf-2512.24772" aria-labelledby="pdf-2512.24772">pdf</a>, <a href="https://arxiv.org/html/2512.24772v1" title="View HTML" id="html-2512.24772" aria-labelledby="html-2512.24772" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24772" title="Other formats" id="oth-2512.24772" aria-labelledby="oth-2512.24772">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Uncertainty-aware Semi-supervised Ensemble Teacher Framework for Multilingual Depression Detection
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rehman,+M+Z+U">Mohammad Zia Ur Rehman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Navya,+V">Velpuru Navya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sanskar">Sanskar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qureshi,+S+U">Shuja Uddin Qureshi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+N">Nagendra Kumar</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Detecting depression from social media text is still a challenging task. This is due to different language styles, informal expression, and the lack of annotated data in many languages. To tackle these issues, we propose, Semi-SMDNet, a strong Semi-Supervised Multilingual Depression detection Network. It combines teacher-student pseudo-labelling, ensemble learning, and augmentation of data. Our framework uses a group of teacher models. Their predictions come together through soft voting. An uncertainty-based threshold filters out low-confidence pseudo-labels to reduce noise and improve learning stability. We also use a confidence-weighted training method that focuses on reliable pseudo-labelled samples. This greatly boosts robustness across languages. Tests on Arabic, Bangla, English, and Spanish datasets show that our approach consistently beats strong baselines. It significantly reduces the performance gap between settings that have plenty of resources and those that do not. Detailed experiments and studies confirm that our framework is effective and can be used in various situations. This shows that it is suitable for scalable, cross-language mental health monitoring where labelled resources are limited.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item54'>[54]</a>
      <a href ="/abs/2512.24776" title="Abstract" id="2512.24776">
        arXiv:2512.24776
      </a>
      
        [<a href="/pdf/2512.24776" title="Download PDF" id="pdf-2512.24776" aria-labelledby="pdf-2512.24776">pdf</a>, <a href="https://arxiv.org/html/2512.24776v1" title="View HTML" id="html-2512.24776" aria-labelledby="html-2512.24776" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24776" title="Other formats" id="oth-2512.24776" aria-labelledby="oth-2512.24776">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Compute-Accuracy Pareto Frontiers for Open-Source Reasoning Large Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prucs,+%C3%81">kos Prucs</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Csutora,+M">Mrton Csutora</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Antal,+M">Mtys Antal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Marosi,+M">Mrk Marosi</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Large Language Models (LLMs) are demonstrating rapid improvements on complex reasoning benchmarks, particularly when allowed to utilize intermediate reasoning steps before converging on a final solution. However, current literature often overlooks the significant computational burden associated with generating long reasoning sequences. For industrial applications, model selection depends not only on raw accuracy but also on resource constraints and inference costs. In this work, we conduct a test-time-compute aware evaluation of both contemporary and older open-source LLMs, mapping their Pareto frontiers across math- and reasoning-intensive benchmarks. Our findings identify the Mixture of Experts (MoE) architecture as a strong candidate to balance performance and efficiency in our evaluation setting. Furthermore, we trace the trajectory of Pareto efficiency over time to derive an emergent trend regarding accuracy gain per unit of compute. Finally, we demonstrate that there is a saturation point for inference-time compute. Beyond a certain threshold, accuracy gains diminish, indicating that while extended reasoning capabilities are beneficial, they cannot overcome intrinsic model limitations regarding specific complexities.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item55'>[55]</a>
      <a href ="/abs/2512.24825" title="Abstract" id="2512.24825">
        arXiv:2512.24825
      </a>
      
        [<a href="/pdf/2512.24825" title="Download PDF" id="pdf-2512.24825" aria-labelledby="pdf-2512.24825">pdf</a>, <a href="https://arxiv.org/html/2512.24825v1" title="View HTML" id="html-2512.24825" aria-labelledby="html-2512.24825" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24825" title="Other formats" id="oth-2512.24825" aria-labelledby="oth-2512.24825">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Practising responsibility: Ethics in NLP as a hands-on course
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nissim,+M">Malvina Nissim</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Patti,+V">Viviana Patti</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Savoldi,+B">Beatrice Savoldi</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          As Natural Language Processing (NLP) systems become more pervasive, integrating ethical considerations into NLP education has become essential. However, this presents inherent challenges in curriculum development: the field&#39;s rapid evolution from both academia and industry, and the need to foster critical thinking beyond traditional technical training. We introduce our course on Ethical Aspects in NLP and our pedagogical approach, grounded in active learning through interactive sessions, hands-on activities, and &#34;learning by teaching&#34; methods. Over four years, the course has been refined and adapted across different institutions, educational levels, and interdisciplinary backgrounds; it has also yielded many reusable products, both in the form of teaching materials and in the form of actual educational products aimed at diverse audiences, made by the students themselves. By sharing our approach and experience, we hope to provide inspiration for educators seeking to incorporate social impact considerations into their curricula.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item56'>[56]</a>
      <a href ="/abs/2512.24842" title="Abstract" id="2512.24842">
        arXiv:2512.24842
      </a>
      
        [<a href="/pdf/2512.24842" title="Download PDF" id="pdf-2512.24842" aria-labelledby="pdf-2512.24842">pdf</a>, <a href="https://arxiv.org/html/2512.24842v1" title="View HTML" id="html-2512.24842" aria-labelledby="html-2512.24842" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24842" title="Other formats" id="oth-2512.24842" aria-labelledby="oth-2512.24842">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Triangulation as an Acceptance Rule for Multilingual Mechanistic Interpretability
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Long,+Y">Yanan Long</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          NeurIPS 2025 Workshop Evaluating the Evolving LLM Lifecycle: Benchmarks, Emergent Abilities, and Scaling
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (stat.ML)
        </div>

        <p class='mathjax'>
          Multilingual language models achieve strong aggregate performance yet often behave unpredictably across languages, scripts, and cultures. We argue that mechanistic explanations for such models should satisfy a \emph{causal} standard: claims must survive causal interventions and must \emph{cross-reference} across environments that perturb surface form while preserving meaning. We formalize \emph{reference families} as predicate-preserving variants and introduce \emph{triangulation}, an acceptance rule requiring necessity (ablating the circuit degrades the target behavior), sufficiency (patching activations transfers the behavior), and invariance (both effects remain directionally stable and of sufficient magnitude across the reference family). To supply candidate subgraphs, we adopt automatic circuit discovery and \emph{accept or reject} those candidates by triangulation. We ground triangulation in causal abstraction by casting it as an approximate transformation score over a distribution of interchange interventions, connect it to the pragmatic interpretability agenda, and present a comparative experimental protocol across multiple model families, language pairs, and tasks. Triangulation provides a falsifiable standard for mechanistic claims that filters spurious circuits passing single-environment tests but failing cross-lingual invariance.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item57'>[57]</a>
      <a href ="/abs/2512.24848" title="Abstract" id="2512.24848">
        arXiv:2512.24848
      </a>
      
        [<a href="/pdf/2512.24848" title="Download PDF" id="pdf-2512.24848" aria-labelledby="pdf-2512.24848">pdf</a>, <a href="https://arxiv.org/html/2512.24848v1" title="View HTML" id="html-2512.24848" aria-labelledby="html-2512.24848" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24848" title="Other formats" id="oth-2512.24848" aria-labelledby="oth-2512.24848">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          PrivacyBench: A Conversational Benchmark for Evaluating Privacy in Personalized AI
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mukhopadhyay,+S">Srija Mukhopadhyay</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Reddy,+S">Sathwik Reddy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Muthukumar,+S">Shruthi Muthukumar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=An,+J">Jisun An</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumaraguru,+P">Ponnurangam Kumaraguru</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          11 pages, 2 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Personalized AI agents rely on access to a user&#39;s digital footprint, which often includes sensitive data from private emails, chats and purchase histories. Yet this access creates a fundamental societal and privacy risk: systems lacking social-context awareness can unintentionally expose user secrets, threatening digital well-being. We introduce PrivacyBench, a benchmark with socially grounded datasets containing embedded secrets and a multi-turn conversational evaluation to measure secret preservation. Testing Retrieval-Augmented Generation (RAG) assistants reveals that they leak secrets in up to 26.56% of interactions. A privacy-aware prompt lowers leakage to 5.12%, yet this measure offers only partial mitigation. The retrieval mechanism continues to access sensitive data indiscriminately, which shifts the entire burden of privacy preservation onto the generator. This creates a single point of failure, rendering current architectures unsafe for wide-scale deployment. Our findings underscore the urgent need for structural, privacy-by-design safeguards to ensure an ethical and inclusive web for everyone.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item58'>[58]</a>
      <a href ="/abs/2512.24863" title="Abstract" id="2512.24863">
        arXiv:2512.24863
      </a>
      
        [<a href="/pdf/2512.24863" title="Download PDF" id="pdf-2512.24863" aria-labelledby="pdf-2512.24863">pdf</a>, <a href="https://arxiv.org/html/2512.24863v1" title="View HTML" id="html-2512.24863" aria-labelledby="html-2512.24863" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24863" title="Other formats" id="oth-2512.24863" aria-labelledby="oth-2512.24863">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Big AI is accelerating the metacrisis: What can we do?
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bird,+S">Steven Bird</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          9 pages, 1 figure
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY)
        </div>

        <p class='mathjax'>
          The world is in the grip of ecological, meaning, and language crises which are converging into a metacrisis. Big AI is accelerating them all. Language engineers are playing a central role, persisting with a scalability story that is failing humanity, supplying critical talent to plutocrats and kleptocrats, and creating new technologies as if the whole endeavour was value-free. We urgently need to explore alternatives, applying our collective intelligence to design a life-affirming future for NLP that is centered on human flourishing on a living planet.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item59'>[59]</a>
      <a href ="/abs/2512.24867" title="Abstract" id="2512.24867">
        arXiv:2512.24867
      </a>
      
        [<a href="/pdf/2512.24867" title="Download PDF" id="pdf-2512.24867" aria-labelledby="pdf-2512.24867">pdf</a>, <a href="https://arxiv.org/html/2512.24867v1" title="View HTML" id="html-2512.24867" aria-labelledby="html-2512.24867" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24867" title="Other formats" id="oth-2512.24867" aria-labelledby="oth-2512.24867">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Encyclo-K: Evaluating LLMs with Dynamically Composed Knowledge Statements
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+Y">Yiming Liang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yizhi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+Y">Yantao Du</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+G">Ge Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+J">Jiayi Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Y">Yuchen Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Piao,+Y">Yinzhu Piao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+D">Denghui Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+T">Tong Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z">Ziniu Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+L">Li Du</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lei,+B">Bo Lei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">Jiaheng Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+C">Chenghua Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">Zhaoxiang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+W">Wenhao Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">Jiajun Zhang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Benchmarks play a crucial role in tracking the rapid advancement of large language models (LLMs) and identifying their capability boundaries. However, existing benchmarks predominantly curate questions at the question level, suffering from three fundamental limitations: vulnerability to data contamination, restriction to single-knowledge-point assessment, and reliance on costly domain expert annotation. We propose Encyclo-K, a statement-based benchmark that rethinks benchmark construction from the ground up. Our key insight is that knowledge statements, not questions, can serve as the unit of curation, and questions can then be constructed from them. We extract standalone knowledge statements from authoritative textbooks and dynamically compose them into evaluation questions through random sampling at test time. This design directly addresses all three limitations: the combinatorial space is too vast to memorize, and model rankings remain stable across dynamically generated question sets, enabling reliable periodic dataset refresh; each question aggregates 8-10 statements for comprehensive multi-knowledge assessment; annotators only verify formatting compliance without requiring domain expertise, substantially reducing annotation costs. Experiments on over 50 LLMs demonstrate that Encyclo-K poses substantial challenges with strong discriminative power. Even the top-performing OpenAI-GPT-5.1 achieves only 62.07% accuracy, and model performance displays a clear gradient distribution--reasoning models span from 16.04% to 62.07%, while chat models range from 9.71% to 50.40%. These results validate the challenges introduced by dynamic evaluation and multi-statement comprehensive understanding. These findings establish Encyclo-K as a scalable framework for dynamic evaluation of LLMs&#39; comprehensive understanding over multiple fine-grained disciplinary knowledge statements.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item60'>[60]</a>
      <a href ="/abs/2512.24880" title="Abstract" id="2512.24880">
        arXiv:2512.24880
      </a>
      
        [<a href="/pdf/2512.24880" title="Download PDF" id="pdf-2512.24880" aria-labelledby="pdf-2512.24880">pdf</a>, <a href="/format/2512.24880" title="Other formats" id="oth-2512.24880" aria-labelledby="oth-2512.24880">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          mHC: Manifold-Constrained Hyper-Connections
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xie,+Z">Zhenda Xie</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+Y">Yixuan Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+H">Huanqi Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+C">Chenggang Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+C">Chengqi Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jiashi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+D">Damai Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+H">Huazuo Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chang,+J">Jiang Chang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+L">Liang Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+S">Shangyan Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Z">Zhean Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">Zhengyan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng,+W">Wangding Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+S">Shengding Hu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yuqing Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+J">Jingyang Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L">Lean Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+W">Wenfeng Liang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Recently, studies exemplified by Hyper-Connections (HC) have extended the ubiquitous residual connection paradigm established over the past decade by expanding the residual stream width and diversifying connectivity patterns. While yielding substantial performance gains, this diversification fundamentally compromises the identity mapping property intrinsic to the residual connection, which causes severe training instability and restricted scalability, and additionally incurs notable memory access overhead. To address these challenges, we propose Manifold-Constrained Hyper-Connections (mHC), a general framework that projects the residual connection space of HC onto a specific manifold to restore the identity mapping property, while incorporating rigorous infrastructure optimization to ensure efficiency. Empirical experiments demonstrate that mHC is effective for training at scale, offering tangible performance improvements and superior scalability. We anticipate that mHC, as a flexible and practical extension of HC, will contribute to a deeper understanding of topological architecture design and suggest promising directions for the evolution of foundational models.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item61'>[61]</a>
      <a href ="/abs/2512.24885" title="Abstract" id="2512.24885">
        arXiv:2512.24885
      </a>
      
        [<a href="/pdf/2512.24885" title="Download PDF" id="pdf-2512.24885" aria-labelledby="pdf-2512.24885">pdf</a>, <a href="/format/2512.24885" title="Other formats" id="oth-2512.24885" aria-labelledby="oth-2512.24885">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          BEDA: Belief Estimation as Probabilistic Constraints for Performing Strategic Dialogue Acts
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+H">Hengli Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+Z">Zhaoxin Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+Q">Qi Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C">Chenxi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+M">Mengmeng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+T">Tinglang Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kang,+Y">Yipeng Kang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yuxuan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+S">Song-Chun Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jia,+Z">Zixia Jia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Z">Zilong Zheng</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Accepted by AAMAS 2026
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Computer Science and Game Theory (cs.GT); Multiagent Systems (cs.MA)
        </div>

        <p class='mathjax'>
          Strategic dialogue requires agents to execute distinct dialogue acts, for which belief estimation is essential. While prior work often estimates beliefs accurately, it lacks a principled mechanism to use those beliefs during generation. We bridge this gap by first formalizing two core acts Adversarial and Alignment, and by operationalizing them via probabilistic constraints on what an agent may generate. We instantiate this idea in BEDA, a framework that consists of the world set, the belief estimator for belief estimation, and the conditional generator that selects acts and realizes utterances consistent with the inferred beliefs. Across three settings, Conditional Keeper Burglar (CKBG, adversarial), Mutual Friends (MF, cooperative), and CaSiNo (negotiation), BEDA consistently outperforms strong baselines: on CKBG it improves success rate by at least 5.0 points across backbones and by 20.6 points with GPT-4.1-nano; on Mutual Friends it achieves an average improvement of 9.3 points; and on CaSiNo it achieves the optimal deal relative to all baselines. These results indicate that casting belief estimation as constraints provides a simple, general mechanism for reliable strategic dialogue.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item62'>[62]</a>
      <a href ="/abs/2512.24933" title="Abstract" id="2512.24933">
        arXiv:2512.24933
      </a>
      
        [<a href="/pdf/2512.24933" title="Download PDF" id="pdf-2512.24933" aria-labelledby="pdf-2512.24933">pdf</a>, <a href="/format/2512.24933" title="Other formats" id="oth-2512.24933" aria-labelledby="oth-2512.24933">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Adaptive Dependency-aware Prompt Optimization Framework for Multi-Step LLM Pipeline
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+M">Minjun Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xinyu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Shuai Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+D">Deyang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+R">Ruifeng Shi</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Multi-step LLM pipelines invoke large language models multiple times in a structured sequence and can effectively solve complex tasks, but their performance heavily depends on the prompts used at each step. Jointly optimizing these prompts is difficult due to missing step-level supervision and inter-step dependencies. Existing end-to-end prompt optimization methods struggle under these conditions and often yield suboptimal or unstable updates. We propose ADOPT, an Adaptive Dependency-aware Prompt Optimization framework for multi-step LLM pipelines. ADOPT explicitly models the dependency between each LLM step and the final task outcome, enabling precise text-gradient estimation analogous to computing analytical derivatives. It decouples textual gradient estimation from gradient updates, reducing multi-prompt optimization to flexible single-prompt optimization steps, and employs a Shapley-based mechanism to adaptively allocate optimization resources. Experiments on real-world datasets and diverse pipeline structures show that ADOPT is effective and robust, consistently outperforming state-of-the-art prompt optimization baselines.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item63'>[63]</a>
      <a href ="/abs/2512.24997" title="Abstract" id="2512.24997">
        arXiv:2512.24997
      </a>
      
        [<a href="/pdf/2512.24997" title="Download PDF" id="pdf-2512.24997" aria-labelledby="pdf-2512.24997">pdf</a>, <a href="https://arxiv.org/html/2512.24997v1" title="View HTML" id="html-2512.24997" aria-labelledby="html-2512.24997" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24997" title="Other formats" id="oth-2512.24997" aria-labelledby="oth-2512.24997">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Classifying long legal documents using short random chunks
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cabrera-Diego,+L+A">Luis Adrin Cabrera-Diego</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Classifying legal documents is a challenge, besides their specialized vocabulary, sometimes they can be very long. This means that feeding full documents to a Transformers-based models for classification might be impossible, expensive or slow. Thus, we present a legal document classifier based on DeBERTa V3 and a LSTM, that uses as input a collection of 48 randomly-selected short chunks (max 128 tokens). Besides, we present its deployment pipeline using Temporal, a durable execution solution, which allow us to have a reliable and robust processing workflow. The best model had a weighted F-score of 0.898, while the pipeline running on CPU had a processing median time of 498 seconds per 100 files.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item64'>[64]</a>
      <a href ="/abs/2512.25015" title="Abstract" id="2512.25015">
        arXiv:2512.25015
      </a>
      
        [<a href="/pdf/2512.25015" title="Download PDF" id="pdf-2512.25015" aria-labelledby="pdf-2512.25015">pdf</a>, <a href="/format/2512.25015" title="Other formats" id="oth-2512.25015" aria-labelledby="oth-2512.25015">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MAMA-Memeia! Multi-Aspect Multi-Agent Collaboration for Depressive Symptoms Identification in Memes
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+S">Siddhant Agarwal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dhuler,+A">Adya Dhuler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ruhnke,+P">Polly Ruhnke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Speisman,+M">Melvin Speisman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Akhtar,+M+S">Md Shad Akhtar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yadav,+S">Shweta Yadav</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Accepted by AAAI 2026
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Over the past years, memes have evolved from being exclusively a medium of humorous exchanges to one that allows users to express a range of emotions freely and easily. With the ever-growing utilization of memes in expressing depressive sentiments, we conduct a study on identifying depressive symptoms exhibited by memes shared by users of online social media platforms. We introduce RESTOREx as a vital resource for detecting depressive symptoms in memes on social media through the Large Language Model (LLM) generated and human-annotated explanations. We introduce MAMAMemeia, a collaborative multi-agent multi-aspect discussion framework grounded in the clinical psychology method of Cognitive Analytic Therapy (CAT) Competencies. MAMAMemeia improves upon the current state-of-the-art by 7.55% in macro-F1 and is established as the new benchmark compared to over 30 methods.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item65'>[65]</a>
      <a href ="/abs/2512.25026" title="Abstract" id="2512.25026">
        arXiv:2512.25026
      </a>
      
        [<a href="/pdf/2512.25026" title="Download PDF" id="pdf-2512.25026" aria-labelledby="pdf-2512.25026">pdf</a>, <a href="https://arxiv.org/html/2512.25026v1" title="View HTML" id="html-2512.25026" aria-labelledby="html-2512.25026" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.25026" title="Other formats" id="oth-2512.25026" aria-labelledby="oth-2512.25026">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Modeling Language as a Sequence of Thoughts
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Borazjanizadeh,+N">Nasim Borazjanizadeh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=McClelland,+J">James McClelland</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level &#34;thought&#34; states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG&#39;s loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item66'>[66]</a>
      <a href ="/abs/2512.25052" title="Abstract" id="2512.25052">
        arXiv:2512.25052
      </a>
      
        [<a href="/pdf/2512.25052" title="Download PDF" id="pdf-2512.25052" aria-labelledby="pdf-2512.25052">pdf</a>, <a href="https://arxiv.org/html/2512.25052v1" title="View HTML" id="html-2512.25052" aria-labelledby="html-2512.25052" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.25052" title="Other formats" id="oth-2512.25052" aria-labelledby="oth-2512.25052">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Peng,+C">Chao Peng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+B">Bin Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Long,+Z">Zhilei Long</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sheng,+J">Jinfang Sheng</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Preprint. Under review
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)
        </div>

        <p class='mathjax'>
          Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.
        </p>
      </div>
    </dd>
</dl>

      <dl id='articles'>
    <h3>Cross submissions (showing 21 of 21 entries)</h3>


    <dt>
      <a name='item67'>[67]</a>
      <a href ="/abs/2512.23747" title="Abstract" id="2512.23747">
        arXiv:2512.23747
      </a>
          (cross-list from cs.SE)

        [<a href="/pdf/2512.23747" title="Download PDF" id="pdf-2512.23747" aria-labelledby="pdf-2512.23747">pdf</a>, <a href="https://arxiv.org/html/2512.23747v1" title="View HTML" id="html-2512.23747" aria-labelledby="html-2512.23747" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23747" title="Other formats" id="oth-2512.23747" aria-labelledby="oth-2512.23747">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          State-of-the-art Small Language Coder Model: Mify-Coder
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Parmar,+A">Abhinav Parmar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Panigrahi,+A">Abhisek Panigrahi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dwivedi,+A+K">Abhishek Kumar Dwivedi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bhattacharya,+A">Abhishek Bhattacharya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramachandra,+A">Adarsh Ramachandra</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Choudhary,+A">Aditya Choudhary</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Garg,+A">Aditya Garg</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Raj,+A">Aditya Raj</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bhatt,+A">Alankrit Bhatt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yadav,+A">Alpesh Yadav</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vishnu,+A">Anant Vishnu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pillai,+A">Ananthu Pillai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+A">Ankush Kumar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Patnaik,+A">Aryan Patnaik</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=S,+A+N">Aswatha Narayanan S</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+A+R">Avanish Raj Singh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gadda,+B+S">Bhavya Shree Gadda</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kachhadiya,+B+P">Brijesh Pankajbhai Kachhadiya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jahnavi,+B">Buggala Jahnavi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Krishna,+C+N">Chidurala Nithin Krishna</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shah,+C">Chintan Shah</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Akshaya,+C">Chunduru Akshaya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Banerjee,+D">Debarshi Banerjee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dey,+D">Debrup Dey</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=R.,+D">Deepa R.</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=G,+D+B">Deepika B G</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rahman,+F+u">Faiz ur Rahman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gayari,+G">Gagan Gayari</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Naidu,+G+J+K">Gudhi Jagadeesh Kumar Naidu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Singh,+G">Gursimar Singh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tyagi,+H">Harshal Tyagi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=K,+H">Harshini K</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vathalloor,+J+M">James Mani Vathalloor</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nettar,+J">Jayarama Nettar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gajjam,+J">Jayashree Gajjam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=George,+J+W+S">Joe Walter Sugil George</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tadepalli,+K+S+K">Kamalakara Sri Krishna Tadepalli</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rathinasamy,+K">Kamalkumar Rathinasamy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chaurasia,+K">Karan Chaurasia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=S,+K">Karthikeyan S</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Arora,+K">Kashish Arora</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Desai,+K">Kaushal Desai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Buwade,+K">Khushboo Buwade</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Manjrekar,+K">Kiran Manjrekar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Likhitha,+M+V+S">Malikireddy Venkata Sai Likhitha</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=A,+M">Manjunath A</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bedmutha,+M+M">Mitali Mahavir Bedmutha</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tarafdar,+M+R">Mohammed Rafee Tarafdar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tiwari,+N">Nikhil Tiwari</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gigi,+N+K">Nikitha K Gigi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ravikumar,+P">Pavan Ravikumar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Swarnanjali,+P">Pendyala Swarnanjali</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anand,+P">Piyush Anand</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chandrasekar,+P">Prakash Chandrasekar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gawade,+P+B">Prasanna Bhalchandra Gawade</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sivan,+P">Prasanth Sivan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khurana,+P">Preeti Khurana</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Babbar,+P">Priyanshi Babbar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mondal,+R+A">Rajab Ali Mondal</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vissapragada,+R+K">Rajesh Kumar Vissapragada</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ganesan,+R">Rajeshwari Ganesan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koppisetti,+R">Rajeswari Koppisetti</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=R.,+R">Ramjee R.</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Thiruppathisamy,+R">Ramkumar Thiruppathisamy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=S.,+R+G">Rani G. S.</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Reka,+S">S Reka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gupta,+S">Samarth Gupta</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kothakota,+S+R">Sandeep Reddy Kothakota</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=K,+S">Sarathy K</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+S+S">Sathyanarayana Sampath Kumar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+S">Saurabh Kumar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khasare,+S">Shashank Khasare</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+S+D+V">Shenbaga Devi Venkatesh Kumar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Parvatham,+S+R+K">Shiva Rama Krishna Parvatham</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shaikh,+S">Shoeb Shaikh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=A,+S">Shrishanmathi A</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pathak,+S">Shubham Pathak</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Koppaka,+S+S">Sree Samhita Koppaka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=S,+S+R+K">Sreenivasa Raghavan K S</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Venkatasubramanian,+S">Sreeram Venkatasubramanian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bojja,+S+D">Suprabha Desai Bojja</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=R,+S">Swetha R</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ahmed,+S">Syed Ahmed</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Thota,+C+H">Chinmai Harshitha Thota</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yadav,+T">Tushar Yadav</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kusumitha,+V">Veeravelly Kusumitha</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Patnaik,+V+V+S+S+P">V V S S Prasanth Patnaik</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sesetti,+V+S">Vidya Sri Sesetti</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=K,+V">Vijayakeerthi K</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bakshi,+V+R">Vikram Raj Bakshi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=K,+V+K">Vinay K K</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Loganathan,+V+K">Vinoth Kumar Loganathan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tiwari,+V">Vipin Tiwari</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shrivastav,+V+K">Vivek Kumar Shrivastav</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Charan,+V+V+S+D">V Venkata Sri Datta Charan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khan,+W+A">Wasim Akhtar Khan</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Software Engineering (cs.SE)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          We present Mify-Coder, a 2.5B-parameter code model trained on 4.2T tokens using a compute-optimal strategy built on the Mify-2.5B foundation model. Mify-Coder achieves comparable accuracy and safety while significantly outperforming much larger baseline models on standard coding and function-calling benchmarks, demonstrating that compact models can match frontier-grade models in code generation and agent-driven workflows. Our training pipeline combines high-quality curated sources with synthetic data generated through agentically designed prompts, refined iteratively using enterprise-grade evaluation datasets. LLM-based quality filtering further enhances data density, enabling frugal yet effective training. Through disciplined exploration of CPT-SFT objectives, data mixtures, and sampling dynamics, we deliver frontier-grade code intelligence within a single continuous training trajectory. Empirical evidence shows that principled data and compute discipline allow smaller models to achieve competitive accuracy, efficiency, and safety compliance. Quantized variants of Mify-Coder enable deployment on standard desktop environments without requiring specialized hardware.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item68'>[68]</a>
      <a href ="/abs/2512.23850" title="Abstract" id="2512.23850">
        arXiv:2512.23850
      </a>
          (cross-list from cs.AI)

        [<a href="/pdf/2512.23850" title="Download PDF" id="pdf-2512.23850" aria-labelledby="pdf-2512.23850">pdf</a>, <a href="https://arxiv.org/html/2512.23850v1" title="View HTML" id="html-2512.23850" aria-labelledby="html-2512.23850" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23850" title="Other formats" id="oth-2512.23850" aria-labelledby="oth-2512.23850">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          The Drill-Down and Fabricate Test (DDFT): A Protocol for Measuring Epistemic Robustness in Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Baxi,+R">Rahul Baxi</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Currently under review at TMLR
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Current language model evaluations measure what models know under ideal conditions but not how robustly they know it under realistic stress. Static benchmarks like MMLU and TruthfulQA cannot distinguish a model that lacks knowledge from one whose verification mechanisms collapse when information degrades or adversaries probe for weaknesses. We introduce the Drill-Down and Fabricate Test (DDFT), a protocol that measures epistemic robustness: a model&#39;s ability to maintain factual accuracy under progressive semantic compression and adversarial fabrication. We propose a two-system cognitive model comprising a Semantic System that generates fluent text and an Epistemic Verifier that validates factual accuracy. Our findings, based on evaluating 9 frontier models across 8 knowledge domains at 5 compression levels (1,800 turn-level evaluations), reveal that epistemic robustness is orthogonal to conventional design paradigms. Neither parameter count (r=0.083, p=0.832) nor architectural type (r=0.153, p=0.695) significantly predicts robustness, suggesting it emerges from training methodology and verification mechanisms distinct from current approaches. Error detection capability strongly predicts overall robustness (rho=-0.817, p=0.007), indicating this is the critical bottleneck. We find that flagship models exhibit brittleness despite their scale, while smaller models can achieve robust performance, challenging assumptions about the relationship between model size and reliability. The DDFT framework provides both theoretical foundation and practical tools for assessing epistemic robustness before deployment in critical applications.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item69'>[69]</a>
      <a href ="/abs/2512.23852" title="Abstract" id="2512.23852">
        arXiv:2512.23852
      </a>
          (cross-list from cs.LG)

        [<a href="/pdf/2512.23852" title="Download PDF" id="pdf-2512.23852" aria-labelledby="pdf-2512.23852">pdf</a>, <a href="https://arxiv.org/html/2512.23852v1" title="View HTML" id="html-2512.23852" aria-labelledby="html-2512.23852" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23852" title="Other formats" id="oth-2512.23852" aria-labelledby="oth-2512.23852">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Trellis: Learning to Compress Key-Value Memory in Attention Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Karami,+M">Mahdi Karami</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Behrouz,+A">Ali Behrouz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kacham,+P">Praneeth Kacham</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mirrokni,+V">Vahab Mirrokni</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          In Second Conference on Language Modeling (COLM) (2025)
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Transformers, while powerful, suffer from quadratic computational complexity and the ever-growing Key-Value (KV) cache of the attention mechanism. This paper introduces Trellis, a novel Transformer architecture with bounded memory that learns how to compress its key-value memory dynamically at test time. Trellis replaces the standard KV cache with a fixed-size memory and train a two-pass recurrent compression mechanism to store new keys and values into memory. To achieve this, it leverages an online gradient descent procedure with a forget gate, enabling the compressed memory to be updated recursively while learning to retain important contextual information from incoming tokens at test time. Extensive experiments on language modeling, common-sense reasoning, recall-intensive tasks, and time series show that the proposed architecture outperforms strong baselines. Notably, its performance gains increase as the sequence length grows, highlighting its potential for long-context applications.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item70'>[70]</a>
      <a href ="/abs/2512.23862" title="Abstract" id="2512.23862">
        arXiv:2512.23862
      </a>
          (cross-list from cs.LG)

        [<a href="/pdf/2512.23862" title="Download PDF" id="pdf-2512.23862" aria-labelledby="pdf-2512.23862">pdf</a>, <a href="https://arxiv.org/html/2512.23862v1" title="View HTML" id="html-2512.23862" aria-labelledby="html-2512.23862" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23862" title="Other formats" id="oth-2512.23862" aria-labelledby="oth-2512.23862">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Probing the Limits of Compressive Memory: A Study of Infini-Attention in Small-Scale Pretraining
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+R">Ruizhe Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+K">Kexuan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+Y">Yihao Fang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+B">Baifeng Yu</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          This study investigates small-scale pretraining for Small Language Models (SLMs) to enable efficient use of limited data and compute, improve accessibility in low-resource settings and reduce costs. To enhance long-context extrapolation in compact models, we focus on Infini-attention, which builds a compressed memory from past segments while preserving local attention. In our work, we conduct an empirical study using 300M-parameter LLaMA models pretrained with Infini-attention. The model demonstrates training stability and outperforms the baseline in long-context retrieval. We identify the balance factor as a key part of the model performance, and we found that retrieval accuracy drops with repeated memory compressions over long sequences. Even so, Infini-attention still effectively compensates for the SLM&#39;s limited parameters. Particularly, despite performance degradation at a 16,384-token context, the Infini-attention model achieves up to 31% higher accuracy than the baseline. Our findings suggest that achieving robust long-context capability in SLMs benefits from architectural memory like Infini-attention.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item71'>[71]</a>
      <a href ="/abs/2512.24044" title="Abstract" id="2512.24044">
        arXiv:2512.24044
      </a>
          (cross-list from cs.CR)

        [<a href="/pdf/2512.24044" title="Download PDF" id="pdf-2512.24044" aria-labelledby="pdf-2512.24044">pdf</a>, <a href="https://arxiv.org/html/2512.24044v1" title="View HTML" id="html-2512.24044" aria-labelledby="html-2512.24044" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24044" title="Other formats" id="oth-2512.24044" aria-labelledby="oth-2512.24044">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xin,+Y">Yuan Xin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D">Dingfan Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+L">Linyi Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Backes,+M">Michael Backes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xiao Zhang</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          26 pages,11 tables, 7 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          As large language models (LLMs) are increasingly deployed, ensuring their safe use is paramount. Jailbreaking, adversarial prompts that bypass model alignment to trigger harmful outputs, present significant risks, with existing studies reporting high success rates in evading common LLMs. However, previous evaluations have focused solely on the models, neglecting the full deployment pipeline, which typically incorporates additional safety mechanisms like content moderation filters. To address this gap, we present the first systematic evaluation of jailbreak attacks targeting LLM safety alignment, assessing their success across the full inference pipeline, including both input and output filtering stages. Our findings yield two key insights: first, nearly all evaluated jailbreak techniques can be detected by at least one safety filter, suggesting that prior assessments may have overestimated the practical success of these attacks; second, while safety filters are effective in detection, there remains room to better balance recall and precision to further optimize protection and user experience. We highlight critical gaps and call for further refinement of detection accuracy and usability in LLM safety systems.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item72'>[72]</a>
      <a href ="/abs/2512.24052" title="Abstract" id="2512.24052">
        arXiv:2512.24052
      </a>
          (cross-list from cs.SD)

        [<a href="/pdf/2512.24052" title="Download PDF" id="pdf-2512.24052" aria-labelledby="pdf-2512.24052">pdf</a>, <a href="https://arxiv.org/html/2512.24052v1" title="View HTML" id="html-2512.24052" aria-labelledby="html-2512.24052" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24052" title="Other formats" id="oth-2512.24052" aria-labelledby="oth-2512.24052">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          AHA: Aligning Large Audio-Language Models for Reasoning Hallucinations via Counterfactual Hard Negatives
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Y">Yanxi Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+W">Wenhui Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X">Xiwen Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zhipeng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xin Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+P">Peijie Qiu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H">Hao Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dong,+X">Xuanzhao Dong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+Y">Yujian Xiong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schneider,+A">Anderson Schneider</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nevmyvaka,+Y">Yuriy Nevmyvaka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yalin Wang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Sound (cs.SD)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM)
        </div>

        <p class='mathjax'>
          Although Large Audio-Language Models (LALMs) deliver state-of-the-art (SOTA) performance, they frequently suffer from hallucinations, e.g. generating text not grounded in the audio input. We analyze these grounding failures and identify a distinct taxonomy: Event Omission, False Event Identity, Temporal Relation Error, and Quantitative Temporal Error. To address this, we introduce the AHA (Audio Hallucination Alignment) framework. By leveraging counterfactual hard negative mining, our pipeline constructs a high-quality preference dataset that forces models to distinguish strict acoustic evidence from linguistically plausible fabrications. Additionally, we establish AHA-Eval, a diagnostic benchmark designed to rigorously test these fine-grained temporal reasoning capabilities. We apply this data to align Qwen2.5-Omni. The resulting model, Qwen-Audio-AHA, achieves a 13.7% improvement on AHA-Eval. Crucially, this benefit generalizes beyond our diagnostic set. Our model shows substantial gains on public benchmarks, including 1.3% on MMAU-Test and 1.6% on MMAR, outperforming latest SOTA methods.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item73'>[73]</a>
      <a href ="/abs/2512.24097" title="Abstract" id="2512.24097">
        arXiv:2512.24097
      </a>
          (cross-list from cs.CV)

        [<a href="/pdf/2512.24097" title="Download PDF" id="pdf-2512.24097" aria-labelledby="pdf-2512.24097">pdf</a>, <a href="https://arxiv.org/html/2512.24097v1" title="View HTML" id="html-2512.24097" aria-labelledby="html-2512.24097" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24097" title="Other formats" id="oth-2512.24097" aria-labelledby="oth-2512.24097">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Factorized Learning for Temporally Grounded Video-Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zeng,+W">Wenzheng Zeng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+D">Difei Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shou,+M+Z">Mike Zheng Shou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ng,+H+T">Hwee Tou Ng</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          ICCV 2025 paper. This arXiv version updates Figure 1 to include the concurrent work Qwen2.5-VL to ensure consistency with Table 1
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Multimedia (cs.MM)
        </div>

        <p class='mathjax'>
          Recent video-language models have shown great potential for video understanding, but still struggle with accurate temporal grounding for event-level perception. We observe that two main factors in video understanding (i.e., temporal grounding and textual response) form a logical hierarchy: accurate temporal evidence grounding lays the foundation for reliable textual response. However, existing works typically handle these two tasks in a coupled manner without a clear logical structure, leading to sub-optimal objectives. We address this from a factorized learning perspective. We first propose D$^2$VLM, a framework that decouples the learning of these two tasks while also emphasizing their inherent dependency. We adopt a &#34;grounding then answering with evidence referencing&#34; paradigm and introduce evidence tokens for evidence grounding, which emphasize event-level visual semantic capture beyond the focus on timestamp representation in existing works. To further facilitate the learning of these two tasks, we introduce a novel factorized preference optimization (FPO) algorithm. Unlike standard preference optimization, FPO explicitly incorporates probabilistic temporal grounding modeling into the optimization objective, enabling preference learning for both temporal grounding and textual response. We also construct a synthetic dataset to address the lack of suitable datasets for factorized preference learning with explicit temporal grounding. Experiments on various tasks demonstrate the clear advantage of our approach. Our source code is available at <a href="https://github.com/nusnlp/d2vlm" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item74'>[74]</a>
      <a href ="/abs/2512.24124" title="Abstract" id="2512.24124">
        arXiv:2512.24124
      </a>
          (cross-list from cs.LG)

        [<a href="/pdf/2512.24124" title="Download PDF" id="pdf-2512.24124" aria-labelledby="pdf-2512.24124">pdf</a>, <a href="https://arxiv.org/html/2512.24124v1" title="View HTML" id="html-2512.24124" aria-labelledby="html-2512.24124" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24124" title="Other formats" id="oth-2512.24124" aria-labelledby="oth-2512.24124">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          OptRot: Mitigating Weight Outliers via Data-Free Rotations for Post-Training Quantization
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gadhikar,+A">Advait Gadhikar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Grazzi,+R">Riccardo Grazzi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hensman,+J">James Hensman</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          25 pages, 10 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          The presence of outliers in Large Language Models (LLMs) weights and activations makes them difficult to quantize. Recent work has leveraged rotations to mitigate these outliers. In this work, we propose methods that learn fusible rotations by minimizing principled and cheap proxy objectives to the weight quantization error. We primarily focus on GPTQ as the quantization method. Our main method is OptRot, which reduces weight outliers simply by minimizing the element-wise fourth power of the rotated weights. We show that OptRot outperforms both Hadamard rotations and more expensive, data-dependent methods like SpinQuant and OSTQuant for weight quantization. It also improves activation quantization in the W4A8 setting. We also propose a data-dependent method, OptRot$^{+}$, that further improves performance by incorporating information on the activation covariance. In the W4A4 setting, we see that both OptRot and OptRot$^{+}$ perform worse, highlighting a trade-off between weight and activation quantization.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item75'>[75]</a>
      <a href ="/abs/2512.24340" title="Abstract" id="2512.24340">
        arXiv:2512.24340
      </a>
          (cross-list from cs.CV)

        [<a href="/pdf/2512.24340" title="Download PDF" id="pdf-2512.24340" aria-labelledby="pdf-2512.24340">pdf</a>, <a href="https://arxiv.org/html/2512.24340v1" title="View HTML" id="html-2512.24340" aria-labelledby="html-2512.24340" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24340" title="Other formats" id="oth-2512.24340" aria-labelledby="oth-2512.24340">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          DermaVQA-DAS: Dermatology Assessment Schema (DAS) &amp; Datasets for Closed-Ended Question Answering &amp; Segmentation in Patient-Generated Dermatology Images
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yim,+W">Wen-wai Yim</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+Y">Yujuan Fu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abacha,+A+B">Asma Ben Abacha</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yetisgen,+M">Meliha Yetisgen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Codella,+N">Noel Codella</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Novoa,+R+A">Roberto Andres Novoa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Malvehy,+J">Josep Malvehy</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Recent advances in dermatological image analysis have been driven by large-scale annotated datasets; however, most existing benchmarks focus on dermatoscopic images and lack patient-authored queries and clinical context, limiting their applicability to patient-centered care. To address this gap, we introduce DermaVQA-DAS, an extension of the DermaVQA dataset that supports two complementary tasks: closed-ended question answering (QA) and dermatological lesion segmentation. Central to this work is the Dermatology Assessment Schema (DAS), a novel expert-developed framework that systematically captures clinically meaningful dermatological features in a structured and standardized form. DAS comprises 36 high-level and 27 fine-grained assessment questions, with multiple-choice options in English and Chinese. Leveraging DAS, we provide expert-annotated datasets for both closed QA and segmentation and benchmark state-of-the-art multimodal models. For segmentation, we evaluate multiple prompting strategies and show that prompt design impacts performance: the default prompt achieves the best results under Mean-of-Max and Mean-of-Mean evaluation aggregation schemes, while an augmented prompt incorporating both patient query title and content yields the highest performance under majority-vote-based microscore evaluation, achieving a Jaccard index of 0.395 and a Dice score of 0.566 with BiomedParse. For closed-ended QA, overall performance is strong across models, with average accuracies ranging from 0.729 to 0.798; o3 achieves the best overall accuracy (0.798), closely followed by GPT-4.1 (0.796), while Gemini-1.5-Pro shows competitive performance within the Gemini family (0.783). We publicly release DermaVQA-DAS, the DAS schema, and evaluation protocols to support and accelerate future research in patient-centered dermatological vision-language modeling (<a href="https://osf.io/72rp3" rel="external noopener nofollow" class="link-external link-https">this https URL</a>).
        </p>
      </div>
    </dd>
    <dt>
      <a name='item76'>[76]</a>
      <a href ="/abs/2512.24532" title="Abstract" id="2512.24532">
        arXiv:2512.24532
      </a>
          (cross-list from cs.AI)

        [<a href="/pdf/2512.24532" title="Download PDF" id="pdf-2512.24532" aria-labelledby="pdf-2512.24532">pdf</a>, <a href="https://arxiv.org/html/2512.24532v1" title="View HTML" id="html-2512.24532" aria-labelledby="html-2512.24532" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24532" title="Other formats" id="oth-2512.24532" aria-labelledby="oth-2512.24532">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          From Building Blocks to Planning: Multi-Step Spatial Reasoning in LLMs with Reinforcement Learning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tahmasbi,+A">Amir Tahmasbi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Majidi,+S">Sadegh Majidi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Taram,+K">Kazem Taram</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bera,+A">Aniket Bera</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Spatial reasoning in large language models (LLMs) has gained increasing attention due to applications in navigation and planning. Despite strong general language capabilities, LLMs still struggle with spatial transformations and multi-step planning in structured environments. We propose a two-stage approach that decomposes spatial reasoning into atomic building blocks and their composition. First, we apply supervised fine-tuning on elementary spatial transformations, such as rotation, translation, and scaling, to equip the model with basic spatial physics. We then freeze this physics-aware model and train lightweight LoRA adapters within the GRPO framework to learn policies that compose these building blocks for multi-step planning in puzzle-based environments, in a closed-loop manner. To support this pipeline, we synthesize an ASCII-art dataset and construct a corresponding ASCII-based reinforcement learning environment. Our method consistently outperforms baselines, including the generic backbone, physics-aware model, and end-to-end RL models, under both Dynamic environments with explicit state updates and Static environments where the model must rely on its internal state across steps. In addition, the proposed approach converges faster and exhibits more stable training compared to end-to-end reinforcement learning from scratch. Finally, we analyze attention patterns to assess whether fine-tuning induces meaningful improvements in spatial understanding.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item77'>[77]</a>
      <a href ="/abs/2512.24545" title="Abstract" id="2512.24545">
        arXiv:2512.24545
      </a>
          (cross-list from cs.LG)

        [<a href="/pdf/2512.24545" title="Download PDF" id="pdf-2512.24545" aria-labelledby="pdf-2512.24545">pdf</a>, <a href="https://arxiv.org/html/2512.24545v1" title="View HTML" id="html-2512.24545" aria-labelledby="html-2512.24545" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24545" title="Other formats" id="oth-2512.24545" aria-labelledby="oth-2512.24545">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          More Than Bits: Multi-Envelope Double Binary Factorization for Extreme Quantization
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ichikawa,+Y">Yuma Ichikawa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fujisawa,+Y">Yoshihiko Fujisawa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fujimoto,+Y">Yudai Fujimoto</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sakai,+A">Akira Sakai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fujisawa,+K">Katsuki Fujisawa</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          14 pages, 2 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)
        </div>

        <p class='mathjax'>
          For extreme low-bit quantization of large language models (LLMs), Double Binary Factorization (DBF) is attractive as it enables efficient inference without sacrificing accuracy. However, the scaling parameters of DBF are too restrictive; after factoring out signs, all rank components share the same magnitude profile, resulting in performance saturation. We propose Multi-envelope DBF (MDBF), which retains a shared pair of 1-bit sign bases but replaces the single envelope with a rank-$l$ envelope. By sharing sign matrices among envelope components, MDBF effectively maintains a binary carrier and utilizes the limited memory budget for magnitude expressiveness. We also introduce a closed-form initialization and an alternating refinement method to optimize MDBF. Across the LLaMA and Qwen families, MDBF enhances perplexity and zero-shot accuracy over previous binary formats at matched bits per weight while preserving the same deployment-friendly inference primitive.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item78'>[78]</a>
      <a href ="/abs/2512.24601" title="Abstract" id="2512.24601">
        arXiv:2512.24601
      </a>
          (cross-list from cs.AI)

        [<a href="/pdf/2512.24601" title="Download PDF" id="pdf-2512.24601" aria-labelledby="pdf-2512.24601">pdf</a>, <a href="https://arxiv.org/html/2512.24601v1" title="View HTML" id="html-2512.24601" aria-labelledby="html-2512.24601" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24601" title="Other formats" id="oth-2512.24601" aria-labelledby="oth-2512.24601">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Recursive Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+A+L">Alex L. Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kraska,+T">Tim Kraska</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khattab,+O">Omar Khattab</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          9 pages, 33 with Appendix
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference strategy that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs successfully handle inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of base LLMs and common long-context scaffolds across four diverse long-context tasks, while having comparable (or cheaper) cost per query.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item79'>[79]</a>
      <a href ="/abs/2512.24687" title="Abstract" id="2512.24687">
        arXiv:2512.24687
      </a>
          (cross-list from quant-ph)

        [<a href="/pdf/2512.24687" title="Download PDF" id="pdf-2512.24687" aria-labelledby="pdf-2512.24687">pdf</a>, <a href="https://arxiv.org/html/2512.24687v1" title="View HTML" id="html-2512.24687" aria-labelledby="html-2512.24687" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24687" title="Other formats" id="oth-2512.24687" aria-labelledby="oth-2512.24687">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Quantum Visual Word Sense Disambiguation: Unraveling Ambiguities Through Quantum Inference Model
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/quant-ph?searchtype=author&amp;query=Qiao,+W">Wenbo Qiao</a>, <a href="https://arxiv.org/search/quant-ph?searchtype=author&amp;query=Zhang,+P">Peng Zhang</a>, <a href="https://arxiv.org/search/quant-ph?searchtype=author&amp;query=Hu,+Q">Qinghua Hu</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Quantum Physics (quant-ph)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Visual word sense disambiguation focuses on polysemous words, where candidate images can be easily confused. Traditional methods use classical probability to calculate the likelihood of an image matching each gloss of the target word, summing these to form a posterior probability. However, due to the challenge of semantic uncertainty, glosses from different sources inevitably carry semantic biases, which can lead to biased disambiguation results. Inspired by quantum superposition in modeling uncertainty, this paper proposes a Quantum Inference Model for Unsupervised Visual Word Sense Disambiguation (Q-VWSD). It encodes multiple glosses of the target word into a superposition state to mitigate semantic biases. Then, the quantum circuit is executed, and the results are observed. By formalizing our method, we find that Q-VWSD is a quantum generalization of the method based on classical probability. Building on this, we further designed a heuristic version of Q-VWSD that can run more efficiently on classical computing. The experiments demonstrate that our method outperforms state-of-the-art classical methods, particularly by effectively leveraging non-specialized glosses from large language models, which further enhances performance. Our approach showcases the potential of quantum machine learning in practical applications and provides a case for leveraging quantum modeling advantages on classical computers while quantum hardware remains immature.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item80'>[80]</a>
      <a href ="/abs/2512.24873" title="Abstract" id="2512.24873">
        arXiv:2512.24873
      </a>
          (cross-list from cs.AI)

        [<a href="/pdf/2512.24873" title="Download PDF" id="pdf-2512.24873" aria-labelledby="pdf-2512.24873">pdf</a>, <a href="/format/2512.24873" title="Other formats" id="oth-2512.24873" aria-labelledby="oth-2512.24873">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Let It Flow: Agentic Crafting on Rock and Roll, Building the ROME Model within an Open Agentic Learning Ecosystem
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W">Weixun Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+X">XiaoXiao Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=An,+W">Wanhe An</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+F">Fangwen Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+W">Wei Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+Y">Yancheng He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+J">Ju Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ji,+Q">Qiang Ji</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+H">Hanqi Jin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xiaoyang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z">Zhongwen Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+S">Shirong Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">Jiashun Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Z">Zenan Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+T">Tao Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Muhtar,+D">Dilxat Muhtar</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+Y">Yuanbin Qu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+J">Jiaqiang Shi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+Q">Qinghui Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+Y">Yingshui Tan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+H">Hao Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+R">Runze Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zhaoguo Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+Y">Yanan Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+S">Shaopan Xiong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+B">Binchen Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+X">Xander Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y">Yuchi Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Q">Qipeng Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xixia Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+H">Haizhou Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+J">Jie Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+S">Shuaibing Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+B">Baihui Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+J">Jianhui Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+S">Suhang Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhu,+Y">Yanni Zhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+M">Mengze Cai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+K">Kerui Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X">Xitong Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+Y">Yue Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Du,+L">Lifan Du</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+T">Tao Feng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+T">Tao He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+J">Jin Hu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+Y">Yijie Hu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+Z">Ziyu Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C">Cheng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xiang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+J">Jing Liang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C">Chonghuan Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Z">ZhenDong Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mi,+H">Haodong Mi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mo,+Y">Yanhu Mo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ni,+J">Junjia Ni</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pei,+S">Shixin Pei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+J">Jingyu Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+X">XiaoShuai Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">Cecilia Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">Chaofan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+K">Kangyu Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+P">Pei Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+T">Tao Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W">Wei Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+K">Ke Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+M">Mingyu Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+T">Tiange Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ya,+N">Nan Ya</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+S">Siran Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+J">Jianan Ye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zang,+Y">Yaxing Zang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+D">Duo Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">Junbo Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+B">Boren Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+W">Wanxi Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+L">Ling Pan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+L">Lin Qu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+W">Wenbo Su</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+J">Jiamang Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W">Wei Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+H">Hu Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+M">Minggang Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+C">Cheng Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+B">Bing Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Z">Zhicheng Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+B">Bo Zheng</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          36 pages, 15 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Agentic crafting requires LLMs to operate in real-world environments over multiple turns by taking actions, observing outcomes, and iteratively refining artifacts. Despite its importance, the open-source community lacks a principled, end-to-end ecosystem to streamline agent development. We introduce the Agentic Learning Ecosystem (ALE), a foundational infrastructure that optimizes the production pipeline for agent LLMs. ALE consists of three components: ROLL, a post-training framework for weight optimization; ROCK, a sandbox environment manager for trajectory generation; and iFlow CLI, an agent framework for efficient context engineering. We release ROME (ROME is Obviously an Agentic Model), an open-source agent grounded by ALE and trained on over one million trajectories. Our approach includes data composition protocols for synthesizing complex behaviors and a novel policy optimization algorithm, Interaction-based Policy Alignment (IPA), which assigns credit over semantic interaction chunks rather than individual tokens to improve long-horizon training stability. Empirically, we evaluate ROME within a structured setting and introduce Terminal Bench Pro, a benchmark with improved scale and contamination control. ROME demonstrates strong performance across benchmarks like SWE-bench Verified and Terminal Bench, proving the effectiveness of the ALE infrastructure.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item81'>[81]</a>
      <a href ="/abs/2512.24939" title="Abstract" id="2512.24939">
        arXiv:2512.24939
      </a>
          (cross-list from cs.HC)

        [<a href="/pdf/2512.24939" title="Download PDF" id="pdf-2512.24939" aria-labelledby="pdf-2512.24939">pdf</a>, <a href="/format/2512.24939" title="Other formats" id="oth-2512.24939" aria-labelledby="oth-2512.24939">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Vibe Coding, Interface Flattening
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+H">Hongrui Jin</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          16 pages, 1 figure
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Human-Computer Interaction (cs.HC)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Large language models are reshaping programming by enabling &#39;vibe coding&#39;: the development of softwares through natural-language interaction with model-driven toolchains. This article argues that vibe coding is best understood as interface flattening, a reconfiguration in which previously distinct modalities (GUI, CLI, and API) appear to converge into a single conversational surface, even as the underlying chain of translation from intention to machinic effect lengthens and thickens. Drawing on Friedrich Kittler&#39;s materialist media theory and Alexander Galloway&#39;s account of interfaces as sites of protocol control, the paper situates programming as a historically localised interface arrangement rather than an essential relation to computation. Through a materialist reconstruction of the contemporary vibe-coding stack, it shows how remote compute infrastructures, latency and connectivity, structured outputs, function/tool calling, and interoperability standards such as the Model Context Protocol relocate control and meaning-making power to model and protocol providers. The apparent democratisation of technical capability therefore depends on new dependencies and new literacies. By foregrounding the tension between experiential flattening and infrastructural thickening, I demonstrate how LLM-mediated development redistributes symbolic labour/power, obscures responsibility, and privatises competencies previously dispersed across programming communities, contributing a critical lens on the political economy of AI-mediated human-computer interaction.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item82'>[82]</a>
      <a href ="/abs/2512.24940" title="Abstract" id="2512.24940">
        arXiv:2512.24940
      </a>
          (cross-list from cs.AI)

        [<a href="/pdf/2512.24940" title="Download PDF" id="pdf-2512.24940" aria-labelledby="pdf-2512.24940">pdf</a>, <a href="https://arxiv.org/html/2512.24940v1" title="View HTML" id="html-2512.24940" aria-labelledby="html-2512.24940" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24940" title="Other formats" id="oth-2512.24940" aria-labelledby="oth-2512.24940">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Iterative Deployment Improves Planning Skills in LLMs
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Corr%C3%AAa,+A+B">Augusto B. Corra</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gelberg,+Y">Yoav Gelberg</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Melo,+L+C">Luckeciano C. Melo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shumailov,+I">Ilia Shumailov</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pereira,+A+G">Andr G. Pereira</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gal,+Y">Yarin Gal</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          We show that iterative deployment of large language models (LLMs), each fine-tuned on data carefully curated by users from the previous models&#39; deployment, can significantly change the properties of the resultant models. By testing this mechanism on various planning domains, we observe substantial improvements in planning skills, with later models displaying emergent generalization by discovering much longer plans than the initial models. We then provide theoretical analysis showing that iterative deployment effectively implements reinforcement learning (RL) training in the outer-loop (i.e. not as part of intentional model training), with an implicit reward function. The connection to RL has two important implications: first, for the field of AI safety, as the reward function entailed by repeated deployment is not defined explicitly, and could have unexpected implications to the properties of future model deployments. Second, the mechanism highlighted here can be viewed as an alternative training regime to explicit RL, relying on data curation rather than explicit rewards.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item83'>[83]</a>
      <a href ="/abs/2512.24943" title="Abstract" id="2512.24943">
        arXiv:2512.24943
      </a>
          (cross-list from cs.IR)

        [<a href="/pdf/2512.24943" title="Download PDF" id="pdf-2512.24943" aria-labelledby="pdf-2512.24943">pdf</a>, <a href="https://arxiv.org/html/2512.24943v1" title="View HTML" id="html-2512.24943" aria-labelledby="html-2512.24943" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24943" title="Other formats" id="oth-2512.24943" aria-labelledby="oth-2512.24943">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          RAIR: A Rule-Aware Benchmark Uniting Challenging Long-Tail and Visual Salience Subset for E-commerce Relevance Assessment
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+C">Chenji Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z">Zhuo Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+H">Hui Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zhenyi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+P">Pengjie Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+J">Jian Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+B">Bo Zheng</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Search relevance plays a central role in web e-commerce. While large language models (LLMs) have shown significant results on relevance task, existing benchmarks lack sufficient complexity for comprehensive model assessment, resulting in an absence of standardized relevance evaluation metrics across the industry. To address this limitation, we propose Rule-Aware benchmark with Image for Relevance assessment(RAIR), a Chinese dataset derived from real-world scenarios. RAIR established a standardized framework for relevance assessment and provides a set of universal rules, which forms the foundation for standardized evaluation. Additionally, RAIR analyzes essential capabilities required for current relevance models and introduces a comprehensive dataset consists of three subset: (1) a general subset with industry-balanced sampling to evaluate fundamental model competencies; (2) a long-tail hard subset focus on challenging cases to assess performance limits; (3) a visual salience subset for evaluating multimodal understanding capabilities. We conducted experiments on RAIR using 14 open and closed-source models. The results demonstrate that RAIR presents sufficient challenges even for GPT-5, which achieved the best performance. RAIR data are now available, serving as an industry benchmark for relevance assessment while providing new insights into general LLM and Visual Language Model(VLM) evaluation.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item84'>[84]</a>
      <a href ="/abs/2512.24947" title="Abstract" id="2512.24947">
        arXiv:2512.24947
      </a>
          (cross-list from cs.CV)

        [<a href="/pdf/2512.24947" title="Download PDF" id="pdf-2512.24947" aria-labelledby="pdf-2512.24947">pdf</a>, <a href="https://arxiv.org/html/2512.24947v1" title="View HTML" id="html-2512.24947" aria-labelledby="html-2512.24947" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24947" title="Other formats" id="oth-2512.24947" aria-labelledby="oth-2512.24947">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          CPJ: Explainable Agricultural Pest Diagnosis via Caption-Prompt-Judge with LLM-Judged Refinement
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+W">Wentao Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+T">Tao Fang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+L">Lina Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+L">Lifei Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhong,+W">Weihe Zhong</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          This paper is 6 pages in length and contains 2 figures. Tao Fang (Corresponding Author), Lina Lu (Co-corresponding Author)
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Accurate and interpretable crop disease diagnosis is essential for agricultural decision-making, yet existing methods often rely on costly supervised fine-tuning and perform poorly under domain shifts. We propose Caption--Prompt--Judge (CPJ), a training-free few-shot framework that enhances Agri-Pest VQA through structured, interpretable image captions. CPJ employs large vision-language models to generate multi-angle captions, refined iteratively via an LLM-as-Judge module, which then inform a dual-answer VQA process for both recognition and management responses. Evaluated on CDDMBench, CPJ significantly improves performance: using GPT-5-mini captions, GPT-5-Nano achieves \textbf{+22.7} pp in disease classification and \textbf{+19.5} points in QA score over no-caption baselines. The framework provides transparent, evidence-based reasoning, advancing robust and explainable agricultural diagnosis without fine-tuning. Our code and data are publicly available at: <a href="https://github.com/CPJ-Agricultural/CPJ-Agricultural-Diagnosis" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item85'>[85]</a>
      <a href ="/abs/2512.24969" title="Abstract" id="2512.24969">
        arXiv:2512.24969
      </a>
          (cross-list from cond-mat.stat-mech)

        [<a href="/pdf/2512.24969" title="Download PDF" id="pdf-2512.24969" aria-labelledby="pdf-2512.24969">pdf</a>, <a href="https://arxiv.org/html/2512.24969v1" title="View HTML" id="html-2512.24969" aria-labelledby="html-2512.24969" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.24969" title="Other formats" id="oth-2512.24969" aria-labelledby="oth-2512.24969">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Large language models and the entropy of English
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cond-mat?searchtype=author&amp;query=Scheibner,+C">Colin Scheibner</a>, <a href="https://arxiv.org/search/cond-mat?searchtype=author&amp;query=Smith,+L+M">Lindsay M. Smith</a>, <a href="https://arxiv.org/search/cond-mat?searchtype=author&amp;query=Bialek,+W">William Bialek</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          8 pages, 6 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Statistical Mechanics (cond-mat.stat-mech)</span>; Computation and Language (cs.CL); Biological Physics (physics.bio-ph); Neurons and Cognition (q-bio.NC)
        </div>

        <p class='mathjax'>
          We use large language models (LLMs) to uncover long-ranged structure in English texts from a variety of sources. The conditional entropy or code length in many cases continues to decrease with context length at least to $N\sim 10^4$ characters, implying that there are direct dependencies or interactions across these distances. A corollary is that there are small but significant correlations between characters at these separations, as we show from the data independent of models. The distribution of code lengths reveals an emergent certainty about an increasing fraction of characters at large $N$. Over the course of model training, we observe different dynamics at long and short context lengths, suggesting that long-ranged structure is learned only gradually. Our results constrain efforts to build statistical physics models of LLMs or language itself.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item86'>[86]</a>
      <a href ="/abs/2512.25063" title="Abstract" id="2512.25063">
        arXiv:2512.25063
      </a>
          (cross-list from cs.LG)

        [<a href="/pdf/2512.25063" title="Download PDF" id="pdf-2512.25063" aria-labelledby="pdf-2512.25063">pdf</a>, <a href="https://arxiv.org/html/2512.25063v1" title="View HTML" id="html-2512.25063" aria-labelledby="html-2512.25063" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.25063" title="Other formats" id="oth-2512.25063" aria-labelledby="oth-2512.25063">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Many Minds from One Model: Bayesian Transformers for Population Intelligence
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+D">Diji Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yi Zhang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.
<br>B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item87'>[87]</a>
      <a href ="/abs/2512.25070" title="Abstract" id="2512.25070">
        arXiv:2512.25070
      </a>
          (cross-list from cs.LG)

        [<a href="/pdf/2512.25070" title="Download PDF" id="pdf-2512.25070" aria-labelledby="pdf-2512.25070">pdf</a>, <a href="https://arxiv.org/html/2512.25070v1" title="View HTML" id="html-2512.25070" aria-labelledby="html-2512.25070" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.25070" title="Other formats" id="oth-2512.25070" aria-labelledby="oth-2512.25070">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Scaling Open-Ended Reasoning to Predict the Future
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chandak,+N">Nikhil Chandak</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goel,+S">Shashwat Goel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Prabhu,+A">Ameya Prabhu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hardt,+M">Moritz Hardt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Geiping,+J">Jonas Geiping</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          45 pages
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.
        </p>
      </div>
    </dd>
</dl>

      <dl id='articles'>
    <h3>Replacement submissions (showing 58 of 58 entries)</h3>


    <dt>
      <a name='item88'>[88]</a>
      <a href ="/abs/2304.01424" title="Abstract" id="2304.01424">
        arXiv:2304.01424
      </a>
          (replaced)

        [<a href="/pdf/2304.01424" title="Download PDF" id="pdf-2304.01424" aria-labelledby="pdf-2304.01424">pdf</a>, <a href="https://arxiv.org/html/2304.01424v2" title="View HTML" id="html-2304.01424" aria-labelledby="html-2304.01424" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2304.01424" title="Other formats" id="oth-2304.01424" aria-labelledby="oth-2304.01424">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          CascadeNS: Confidence-Cascaded Neurosymbolic Model for Sarcasm Detection
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mane,+S">Swapnil Mane</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khatavkar,+V">Vaibhav Khatavkar</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          10 pages
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Sarcasm detection in product reviews requires balancing domain-specific symbolic pattern recognition with deep semantic understanding. Symbolic representations capture explicit linguistic phenomena that are often decisive for sarcasm detection. Existing work either favors interpretable symbolic representation or semantic neural modeling, but rarely achieves both effectively. Prior hybrid methods typically combine these paradigms through feature fusion or ensembling, which can degrade performance. We propose CascadeNS, a confidence-calibrated neurosymbolic architecture that integrates symbolic and neural reasoning through selective activation rather than fusion. A symbolic semigraph handles pattern-rich instances with high confidence, while semantically ambiguous cases are delegated to a neural module based on pre-trained LLM embeddings. At the core of CascadeNS is a calibrated confidence measure derived from polarity-weighted semigraph scores. This measure reliably determines when symbolic reasoning is sufficient and when neural analysis is needed. Experiments on product reviews show that CascadeNS outperforms the strong baselines by 7.44%.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item89'>[89]</a>
      <a href ="/abs/2407.05434" title="Abstract" id="2407.05434">
        arXiv:2407.05434
      </a>
          (replaced)

        [<a href="/pdf/2407.05434" title="Download PDF" id="pdf-2407.05434" aria-labelledby="pdf-2407.05434">pdf</a>, <a href="https://arxiv.org/html/2407.05434v2" title="View HTML" id="html-2407.05434" aria-labelledby="html-2407.05434" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2407.05434" title="Other formats" id="oth-2407.05434" aria-labelledby="oth-2407.05434">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          LTLBench: Towards Benchmarks for Evaluating Temporal Logic Reasoning in Large Language Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+W">Weizhi Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nuamah,+K">Kwabena Nuamah</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Belle,+V">Vaishak Belle</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Temporal Reasoning (TR) is a critical ability for LLMs to understand and reason over temporal information and relationships between events. To study the TR ability in LLMs, prior works provide different ways for evaluating various aspects of TR ability. In this work, we propose an alternative perspective for evaluating TR ability by leveraging Linear Temporal Logic (LTL), and develop a pipeline to automatically synthesize challenges for assessing the TR ability of LLMs. Based on this pipeline, we construct a dataset, namely \LTL, consisting of $2000$ TR challenges, and benchmark 12 LLMs across 5 different methods. Furthermore, we conduct additional experiments to investigate the impact of increasing the number of formula operators and events on both LLM performance and the complexity of TR problems. We also perform qualitative analyses of their reasoning processes and the effects of varying the number of events and formula operators, which reveal 3 main issues in their temporal reasoning processes and the unexpected performance changes observed as problem complexity increases. We expect this work to provide valuable insights into the TR ability of LLMs.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item90'>[90]</a>
      <a href ="/abs/2410.00414" title="Abstract" id="2410.00414">
        arXiv:2410.00414
      </a>
          (replaced)

        [<a href="/pdf/2410.00414" title="Download PDF" id="pdf-2410.00414" aria-labelledby="pdf-2410.00414">pdf</a>, <a href="/format/2410.00414" title="Other formats" id="oth-2410.00414" aria-labelledby="oth-2410.00414">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Semantic Parsing with Candidate Expressions for Knowledge Base Question Answering
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nam,+D">Daehwan Nam</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+G+G">Gary Geunbae Lee</a></div>


        <div class='list-journal-ref'><span class='descriptor'>Journal-ref:</span>
          Expert Syst. Appl. 306 (2026) 130564
        </div>

        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Semantic parsers convert natural language to logical forms, which can be evaluated on knowledge bases (KBs) to produce denotations. Recent semantic parsers have been developed with sequence-to-sequence (seq2seq) pre-trained language models (PLMs) or large language models, where the models treat logical forms as sequences of tokens. For syntactic and semantic validity, the semantic parsers use grammars that enable constrained decoding. However, the grammars lack the ability to utilize large information of KBs, although logical forms contain representations of KB elements, such as entities or relations. In this work, we propose a grammar augmented with candidate expressions for semantic parsing on a large KB with a seq2seq PLM. The grammar defines actions as production rules, and our semantic parser predicts actions during inference under the constraints by types and candidate expressions. We apply the grammar to knowledge base question answering, where the constraints by candidate expressions assist a semantic parser to generate valid KB elements. We also introduce two special rules, sub-type inference and union types, and a mask caching algorithm. In particular, sub-type inference and the mask caching algorithm greatly increase the decoding speed of our semantic parser. We experimented on two benchmarks, KQA Pro and Overnight, where the constraints by candidate expressions increased the accuracy of our semantic parser, whether it was trained with strong supervision or weak supervision. In addition, our semantic parser had a fast decoding speed in the experiments. Our source code is publicly available at <a href="https://github.com/daehwannam/candexpr-sp.git" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item91'>[91]</a>
      <a href ="/abs/2410.15051" title="Abstract" id="2410.15051">
        arXiv:2410.15051
      </a>
          (replaced)

        [<a href="/pdf/2410.15051" title="Download PDF" id="pdf-2410.15051" aria-labelledby="pdf-2410.15051">pdf</a>, <a href="https://arxiv.org/html/2410.15051v2" title="View HTML" id="html-2410.15051" aria-labelledby="html-2410.15051" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2410.15051" title="Other formats" id="oth-2410.15051" aria-labelledby="oth-2410.15051">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Automatic identification of diagnosis from hospital discharge letters via weakly-supervised Natural Language Processing
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Torri,+V">Vittorio Torri</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barbieri,+E">Elisa Barbieri</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cantarutti,+A">Anna Cantarutti</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Giaquinto,+C">Carlo Giaquinto</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ieva,+F">Francesca Ieva</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          49 pages, 7 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Identifying patient diagnoses from discharge letters is essential to enable large-scale cohort selection and epidemiological research, but traditional supervised approaches rely on extensive manual annotation, which is often impractical for large textual datasets. In this study, we present a novel weakly-supervised Natural Language Processing pipeline designed to classify Italian discharge letters without requiring manual labelling. After extracting diagnosis-related sentences, the method leverages a transformer-based model with an additional pre-training on Italian medical documents to generate semantic embeddings. A two-level clustering procedure is applied to these embeddings, and the resulting clusters are mapped to the diseases of interest to derive weak labels for a subset of data, eventually used to train a transformer-based classifier. We evaluate the approach on a real-world case study on bronchiolitis in a corpus of 33,176 Italian discharge letters of children admitted to 44 emergency rooms or hospitals in the Veneto Region between 2017 and 2020. The pipeline achieves an area under the curve (AUC) of 77.68% ($\pm 4.30\%)$ and an F1-score of 78.14% ($\pm 4.89\%$) against manual annotations. Its performance surpasses other unsupervised methods and approaches fully supervised models, maintaining robustness to cluster selection and promising generalizability across different disease types. It allows saving approximately 3 minutes of expert time per discharge letter, resulting in more than 1,500 hours for a dataset like ours. This study demonstrates the feasibility of a weakly-supervised strategy for identifying diagnoses from Italian discharge letters. The pipeline achieves strong performance, is adaptable to various diseases, and offers a scalable solution for clinical text classification, reducing the need for manual annotation while maintaining reliable accuracy.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item92'>[92]</a>
      <a href ="/abs/2410.18565" title="Abstract" id="2410.18565">
        arXiv:2410.18565
      </a>
          (replaced)

        [<a href="/pdf/2410.18565" title="Download PDF" id="pdf-2410.18565" aria-labelledby="pdf-2410.18565">pdf</a>, <a href="/format/2410.18565" title="Other formats" id="oth-2410.18565" aria-labelledby="oth-2410.18565">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Bielik 7B v0.1: A Polish Language Model -- Development, Insights, and Evaluation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ociepa,+K">Krzysztof Ociepa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Flis,+%C5%81">ukasz Flis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wr%C3%B3bel,+K">Krzysztof Wrbel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gwo%C5%BAdziej,+A">Adrian Gwodziej</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kinas,+R">Remigiusz Kinas</a></div>


        <div class='list-journal-ref'><span class='descriptor'>Journal-ref:</span>
          Computer Science 26(4) (2025) 131-161
        </div>

        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          We introduce Bielik 7B v0.1, a 7-billion-parameter generative text model for Polish language processing. Trained on curated Polish corpora, this model addresses key challenges in language model development through innovative techniques. These include Weighted Instruction Cross-Entropy Loss, which balances the learning of different instruction types, and Adaptive Learning Rate, which dynamically adjusts the learning rate based on training progress. To evaluate performance, we created the Open PL LLM Leaderboard and Polish MT-Bench, novel frameworks assessing various NLP tasks and conversational abilities. Bielik 7B v0.1 demonstrates significant improvements, achieving a 9 percentage point increase in average score compared to Mistral-7B-v0.1 on the RAG Reader task. It also excels in the Polish MT-Bench, particularly in Reasoning (6.15/10) and Role-playing (7.83/10) categories. This model represents a substantial advancement in Polish language AI, offering a powerful tool for diverse linguistic applications and setting new benchmarks in the field.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item93'>[93]</a>
      <a href ="/abs/2412.04235" title="Abstract" id="2412.04235">
        arXiv:2412.04235
      </a>
          (replaced)

        [<a href="/pdf/2412.04235" title="Download PDF" id="pdf-2412.04235" aria-labelledby="pdf-2412.04235">pdf</a>, <a href="https://arxiv.org/html/2412.04235v3" title="View HTML" id="html-2412.04235" aria-labelledby="html-2412.04235" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2412.04235" title="Other formats" id="oth-2412.04235" aria-labelledby="oth-2412.04235">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Addressing Hallucinations with RAG and NMISS in Italian Healthcare LLM Chatbots
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Priola,+M+P">Maria Paola Priola</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          I combine detection and mitigation techniques to addresses hallucinations in Large Language Models (LLMs). Mitigation is achieved in a question-answering Retrieval-Augmented Generation (RAG) framework while detection is obtained by introducing the Negative Missing Information Scoring System (NMISS), which accounts for contextual relevance in responses. While RAG mitigates hallucinations by grounding answers in external data, NMISS refines the evaluation by identifying cases where traditional metrics incorrectly flag contextually accurate responses as hallucinations. I use Italian health news articles as context to evaluate LLM performance. Results show that Gemma2 and GPT-4 outperform the other models, with GPT-4 producing answers closely aligned with reference responses. Mid-tier models, such as Llama2, Llama3, and Mistral benefit significantly from NMISS, highlighting their ability to provide richer contextual information. This combined approach offers new insights into the reduction and more accurate assessment of hallucinations in LLMs, with applications in real-world healthcare tasks and other domains.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item94'>[94]</a>
      <a href ="/abs/2412.15241" title="Abstract" id="2412.15241">
        arXiv:2412.15241
      </a>
          (replaced)

        [<a href="/pdf/2412.15241" title="Download PDF" id="pdf-2412.15241" aria-labelledby="pdf-2412.15241">pdf</a>, <a href="https://arxiv.org/html/2412.15241v4" title="View HTML" id="html-2412.15241" aria-labelledby="html-2412.15241" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2412.15241" title="Other formats" id="oth-2412.15241" aria-labelledby="oth-2412.15241">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Quantifying Positional Biases in Text Embedding Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+R+J">Reagan J. Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Goel,+S">Samarth Goel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ramchandran,+K">Kannan Ramchandran</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          13 pages, 11 figures, NeurIPS
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)
        </div>

        <p class='mathjax'>
          Embedding models are crucial for tasks in Information Retrieval (IR) and semantic similarity measurement, yet their handling of longer texts and associated positional biases remains underexplored. In this study, we investigate the impact of content position and input size on text embeddings. Our experiments reveal that embedding models, irrespective of their positional encoding mechanisms, disproportionately prioritize the beginning of an input. Ablation studies demonstrate that insertion of irrelevant text or removal at the start of a document reduces cosine similarity between altered and original embeddings by up to 12.3% more than ablations at the end. Regression analysis further confirms this bias, with sentence importance declining as position moves further from the start, even with with content-agnosticity. We hypothesize that this effect arises from pre-processing strategies and chosen positional encoding techniques. These findings quantify the sensitivity of retrieval systems and suggest a new lens towards embedding model robustness.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item95'>[95]</a>
      <a href ="/abs/2502.05568" title="Abstract" id="2502.05568">
        arXiv:2502.05568
      </a>
          (replaced)

        [<a href="/pdf/2502.05568" title="Download PDF" id="pdf-2502.05568" aria-labelledby="pdf-2502.05568">pdf</a>, <a href="https://arxiv.org/html/2502.05568v2" title="View HTML" id="html-2502.05568" aria-labelledby="html-2502.05568" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2502.05568" title="Other formats" id="oth-2502.05568" aria-labelledby="oth-2502.05568">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Large Multimodal Models for Low-Resource Languages: A Survey
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lupascu,+M">Marian Lupascu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rogoz,+A">Ana-Cristina Rogoz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Stupariu,+M+S">Mihai Sorin Stupariu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ionescu,+R+T">Radu Tudor Ionescu</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          In this survey, we systematically analyze techniques used to adapt large multimodal models (LMMs) for low-resource (LR) languages, examining approaches ranging from visual enhancement and data creation to cross-modal transfer and fusion strategies. Through a comprehensive analysis of 117 studies across 96 LR languages, we identify key patterns in how researchers tackle the challenges of limited data and computational resources. We categorize works into resource-oriented and method-oriented contributions, further dividing contributions into relevant sub-categories. We compare method-oriented contributions in terms of performance and efficiency, discussing benefits and limitations of representative studies. We find that visual information often serves as a crucial bridge for improving model performance in LR settings, though significant challenges remain in areas such as hallucination mitigation and computational efficiency. In summary, we provide researchers with a clear understanding of current approaches and remaining challenges in making LMMs more accessible to speakers of LR (understudied) languages. We complement our survey with an open-source repository available at: <a href="https://github.com/marianlupascu/LMM4LRL-Survey" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item96'>[96]</a>
      <a href ="/abs/2502.14780" title="Abstract" id="2502.14780">
        arXiv:2502.14780
      </a>
          (replaced)

        [<a href="/pdf/2502.14780" title="Download PDF" id="pdf-2502.14780" aria-labelledby="pdf-2502.14780">pdf</a>, <a href="https://arxiv.org/html/2502.14780v2" title="View HTML" id="html-2502.14780" aria-labelledby="html-2502.14780" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2502.14780" title="Other formats" id="oth-2502.14780" aria-labelledby="oth-2502.14780">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          ReVision: A Dataset and Baseline VLM for Privacy-Preserving Task-Oriented Visual Instruction Rewriting
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mishra,+A">Abhijit Mishra</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+M">Mingda Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fu,+H">Hsiang Fu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Noh,+R">Richard Noh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+M">Minji Kim</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Accepted and to appear in IJCNLP-AACL 2025
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
        </div>

        <p class='mathjax'>
          Efficient and privacy-preserving multimodal interaction is essential as AR, VR, and modern smartphones with powerful cameras become primary interfaces for human-computer communication. Existing powerful large vision-language models (VLMs) enabling multimodal interaction often rely on cloud-based processing, raising significant concerns about (1) visual privacy by transmitting sensitive vision data to servers, and (2) their limited real-time, on-device usability. This paper explores Visual Instruction Rewriting, a novel approach that transforms multimodal instructions into text-only commands, allowing seamless integration of lightweight on-device instruction rewriter VLMs (250M parameters) with existing conversational AI systems, enhancing vision data privacy. To achieve this, we present a dataset of over 39,000 examples across 14 domains and develop a compact VLM, pretrained on image captioning datasets and fine-tuned for instruction rewriting. Experimental results, evaluated through NLG metrics such as BLEU, METEOR, and ROUGE, along with semantic parsing analysis, demonstrate that even a quantized version of the model (&lt;500MB storage footprint) can achieve effective instruction rewriting, thus enabling privacy-focused, multimodal AI applications.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item97'>[97]</a>
      <a href ="/abs/2503.21614" title="Abstract" id="2503.21614">
        arXiv:2503.21614
      </a>
          (replaced)

        [<a href="/pdf/2503.21614" title="Download PDF" id="pdf-2503.21614" aria-labelledby="pdf-2503.21614">pdf</a>, <a href="https://arxiv.org/html/2503.21614v2" title="View HTML" id="html-2503.21614" aria-labelledby="html-2503.21614" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2503.21614" title="Other formats" id="oth-2503.21614" aria-labelledby="oth-2503.21614">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          A Survey of Efficient Reasoning for Large Reasoning Models: Language, Multimodality, and Beyond
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qu,+X">Xiaoye Qu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Y">Yafu Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+Z">Zhao-Chen Su</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+W">Weigao Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+J">Jianhao Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+D">Dongrui Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cui,+G">Ganqu Cui</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+D">Daizong Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liang,+S">Shuxian Liang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+J">Junxian He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+P">Peng Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+W">Wei Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shao,+J">Jing Shao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+C">Chaochao Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yue Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hua,+X">Xian-Sheng Hua</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+B">Bowen Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+Y">Yu Cheng</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Update recent RL papers. Project page: <a href="https://github.com/XiaoYee/Awesome_Efficient_LRM_Reasoning" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Recent Large Reasoning Models (LRMs), such as DeepSeek-R1 and OpenAI o1, have demonstrated strong performance gains by scaling up the length of Chain-of-Thought (CoT) reasoning during inference. However, a growing concern lies in their tendency to produce excessively long reasoning traces, which are often filled with redundant content (e.g., repeated definitions), over-analysis of simple problems, and superficial exploration of multiple reasoning paths for harder tasks. This inefficiency introduces significant challenges for training, inference, and real-world deployment (e.g., in agent-based systems), where token economy is critical. In this survey, we provide a comprehensive overview of recent efforts aimed at improving reasoning efficiency in LRMs, with a particular focus on the unique challenges that arise in this new paradigm. We identify common patterns of inefficiency, examine methods proposed across the LRM lifecycle, i.e., from pretraining to inference, and discuss promising future directions for research. To support ongoing development, we also maintain a real-time GitHub repository tracking recent progress in the field. We hope this survey serves as a foundation for further exploration and inspires innovation in this rapidly evolving area.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item98'>[98]</a>
      <a href ="/abs/2504.10481" title="Abstract" id="2504.10481">
        arXiv:2504.10481
      </a>
          (replaced)

        [<a href="/pdf/2504.10481" title="Download PDF" id="pdf-2504.10481" aria-labelledby="pdf-2504.10481">pdf</a>, <a href="https://arxiv.org/html/2504.10481v2" title="View HTML" id="html-2504.10481" aria-labelledby="html-2504.10481" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2504.10481" title="Other formats" id="oth-2504.10481" aria-labelledby="oth-2504.10481">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          xVerify: Efficient Answer Verifier for Reasoning Model Evaluations
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+D">Ding Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+Q">Qingchen Yu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+P">Pengyuan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+M">Mengting Hu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+W">Wentao Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zhengren Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+B">Bo Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiong,+F">Feiyu Xiong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xinchi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+C">Chao Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+M">Minchuan Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+Z">Zhiyu Li</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          35 pages
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          With the release of OpenAI&#39;s o1 model, reasoning models that adopt slow-thinking strategies have become increasingly common. Their outputs often contain complex reasoning, intermediate steps, and self-reflection, making existing evaluation methods and reward models inadequate. In particular, they struggle to judge answer equivalence and to reliably extract final answers from long, complex responses. To address this challenge, we propose xVerify, an efficient answer verifier for evaluating reasoning models. xVerify shows strong equivalence judgment capabilities, enabling accurate comparison between model outputs and reference answers across diverse question types. To train and evaluate xVerify, we construct the VAR dataset, which consists of question-answer pairs generated by multiple LLMs across various datasets. The dataset incorporates multiple reasoning models and challenging evaluation sets specifically designed for reasoning assessment, with a multi-round annotation process to ensure label quality. Based on VAR, we train xVerify models at different scales. Experimental results on both test and generalization sets show that all xVerify variants achieve over 95% F1 score and accuracy. Notably, the smallest model, xVerify-0.5B-I, outperforms all evaluation methods except GPT-4o, while xVerify-3B-Ib surpasses GPT-4o in overall performance. In addition, reinforcement learning experiments using xVerify as the reward model yield an 18.4% improvement for Qwen2.5-7B compared with direct generation, exceeding the gains achieved with Math Verify as the reward. These results demonstrate the effectiveness and generalizability of xVerify. All xVerify resources are available on \href{<a href="https://github.com/IAAR-Shanghai/xVerify" rel="external noopener nofollow" class="link-external link-https">this https URL</a>}{GitHub}.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item99'>[99]</a>
      <a href ="/abs/2504.15843" title="Abstract" id="2504.15843">
        arXiv:2504.15843
      </a>
          (replaced)

        [<a href="/pdf/2504.15843" title="Download PDF" id="pdf-2504.15843" aria-labelledby="pdf-2504.15843">pdf</a>, <a href="https://arxiv.org/html/2504.15843v3" title="View HTML" id="html-2504.15843" aria-labelledby="html-2504.15843" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2504.15843" title="Other formats" id="oth-2504.15843" aria-labelledby="oth-2504.15843">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+J">Junshu Pan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+W">Wei Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+S">Shulin Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Q">Qiji Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yue Zhang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item100'>[100]</a>
      <a href ="/abs/2505.03452" title="Abstract" id="2505.03452">
        arXiv:2505.03452
      </a>
          (replaced)

        [<a href="/pdf/2505.03452" title="Download PDF" id="pdf-2505.03452" aria-labelledby="pdf-2505.03452">pdf</a>, <a href="https://arxiv.org/html/2505.03452v3" title="View HTML" id="html-2505.03452" aria-labelledby="html-2505.03452" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2505.03452" title="Other formats" id="oth-2505.03452" aria-labelledby="oth-2505.03452">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          An Analysis of Hyper-Parameter Optimization Methods for Retrieval Augmented Generation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Orbach,+M">Matan Orbach</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Eytan,+O">Ohad Eytan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sznajder,+B">Benjamin Sznajder</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gera,+A">Ariel Gera</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Boni,+O">Odellia Boni</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kantor,+Y">Yoav Kantor</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bloch,+G">Gal Bloch</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Levy,+O">Omri Levy</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abraham,+H">Hadas Abraham</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barzilay,+N">Nitzan Barzilay</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shnarch,+E">Eyal Shnarch</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Factor,+M+E">Michael E. Factor</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ofek-Koifman,+S">Shila Ofek-Koifman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ta-Shma,+P">Paula Ta-Shma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Toledo,+A">Assaf Toledo</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          AAAI 2026 Workshop on New Frontiers in Information Retrieval. For associated results, see <a href="https://github.com/IBM/rag-hpo-bench" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Optimizing Retrieval-Augmented Generation (RAG) configurations for specific tasks is a complex and resource-intensive challenge. Motivated by this challenge, frameworks for RAG hyper-parameter optimization (HPO) have recently emerged, yet their effectiveness has not been rigorously benchmarked. To fill this gap, we present a comprehensive study involving five HPO algorithms over five datasets from diverse domains, including a newly curated real-world product documentation dataset. Our study explores the largest RAG HPO search space to date that includes full grid-search evaluations, and uses three evaluation metrics as optimization targets. Analysis of the results shows that RAG HPO can be done efficiently, either greedily or with random search, and that it significantly boosts RAG performance for all datasets. For greedy HPO approaches, we show that optimizing model selection first is preferable to the common practice of following the RAG pipeline order during optimization.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item101'>[101]</a>
      <a href ="/abs/2505.20298" title="Abstract" id="2505.20298">
        arXiv:2505.20298
      </a>
          (replaced)

        [<a href="/pdf/2505.20298" title="Download PDF" id="pdf-2505.20298" aria-labelledby="pdf-2505.20298">pdf</a>, <a href="https://arxiv.org/html/2505.20298v2" title="View HTML" id="html-2505.20298" aria-labelledby="html-2505.20298" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2505.20298" title="Other formats" id="oth-2505.20298" aria-labelledby="oth-2505.20298">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal Manga Understanding
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Baek,+J">Jeonghun Baek</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Egashira,+K">Kazuki Egashira</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Onohara,+S">Shota Onohara</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Miyai,+A">Atsuyuki Miyai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Imajuku,+Y">Yuki Imajuku</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ikuta,+H">Hikaru Ikuta</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Aizawa,+K">Kiyoharu Aizawa</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          21 pages, 13 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computer Vision and Pattern Recognition (cs.CV)
        </div>

        <p class='mathjax'>
          Manga, or Japanese comics, is a richly multimodal narrative form that blends images and text in complex ways. Teaching large multimodal models (LMMs) to understand such narratives at a human-like level could help manga creators reflect on and refine their stories. To this end, we introduce two benchmarks for multimodal manga understanding: MangaOCR, which targets in-page text recognition, and MangaVQA, a novel benchmark designed to evaluate contextual understanding through visual question answering. MangaVQA consists of 526 high-quality, manually constructed question-answer pairs, enabling reliable evaluation across diverse narrative and visual scenarios. Building on these benchmarks, we develop MangaLMM, a manga-specialized model finetuned from the open-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive experiments, including comparisons with proprietary models such as GPT-4o and Gemini 2.5, we assess how well LMMs understand manga. Our benchmark and model provide a comprehensive foundation for evaluating and advancing LMMs in the richly narrative domain of manga.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item102'>[102]</a>
      <a href ="/abs/2505.24830" title="Abstract" id="2505.24830">
        arXiv:2505.24830
      </a>
          (replaced)

        [<a href="/pdf/2505.24830" title="Download PDF" id="pdf-2505.24830" aria-labelledby="pdf-2505.24830">pdf</a>, <a href="/format/2505.24830" title="Other formats" id="oth-2505.24830" aria-labelledby="oth-2505.24830">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Improving Reliability and Explainability of Medical Question Answering through Atomic Fact Checking in Retrieval-Augmented LLMs
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vladika,+J">Juraj Vladika</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Domres,+A">Annika Domres</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nguyen,+M">Mai Nguyen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Moser,+R">Rebecca Moser</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nano,+J">Jana Nano</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Busch,+F">Felix Busch</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Adams,+L+C">Lisa C. Adams</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bressem,+K+K">Keno K. Bressem</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bernhardt,+D">Denise Bernhardt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Combs,+S+E">Stephanie E. Combs</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Borm,+K+J">Kai J. Borm</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Matthes,+F">Florian Matthes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Peeken,+J+C">Jan C. Peeken</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          18 pages, 7 figures and tables
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Large language models (LLMs) exhibit extensive medical knowledge but are prone to hallucinations and inaccurate citations, which pose a challenge to their clinical adoption and regulatory compliance. Current methods, such as Retrieval Augmented Generation, partially address these issues by grounding answers in source documents, but hallucinations and low fact-level explainability persist. In this work, we introduce a novel atomic fact-checking framework designed to enhance the reliability and explainability of LLMs used in medical long-form question answering. This method decomposes LLM-generated responses into discrete, verifiable units called atomic facts, each of which is independently verified against an authoritative knowledge base of medical guidelines. This approach enables targeted correction of errors and direct tracing to source literature, thereby improving the factual accuracy and explainability of medical Q&amp;A. Extensive evaluation using multi-reader assessments by medical experts and an automated open Q&amp;A benchmark demonstrated significant improvements in factual accuracy and explainability. Our framework achieved up to a 40% overall answer improvement and a 50% hallucination detection rate. The ability to trace each atomic fact back to the most relevant chunks from the database provides a granular, transparent explanation of the generated responses, addressing a major gap in current medical AI applications. This work represents a crucial step towards more trustworthy and reliable clinical applications of LLMs, addressing key prerequisites for clinical application and fostering greater confidence in AI-assisted healthcare.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item103'>[103]</a>
      <a href ="/abs/2506.08899" title="Abstract" id="2506.08899">
        arXiv:2506.08899
      </a>
          (replaced)

        [<a href="/pdf/2506.08899" title="Download PDF" id="pdf-2506.08899" aria-labelledby="pdf-2506.08899">pdf</a>, <a href="https://arxiv.org/html/2506.08899v3" title="View HTML" id="html-2506.08899" aria-labelledby="html-2506.08899" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2506.08899" title="Other formats" id="oth-2506.08899" aria-labelledby="oth-2506.08899">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Toward Robust Legal Text Formalization into Defeasible Deontic Logic using LLMs
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Horner,+E">Elias Horner</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mateis,+C">Cristinel Mateis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Governatori,+G">Guido Governatori</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ciabattoni,+A">Agata Ciabattoni</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          This version is an extended version with additional results and discussion
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Computers and Society (cs.CY); Logic in Computer Science (cs.LO)
        </div>

        <p class='mathjax'>
          We present a comprehensive approach to the automated formalization of legal texts using large language models (LLMs), targeting their transformation into Defeasible Deontic Logic (DDL). Our method employs a structured pipeline that segments complex normative language into atomic snippets, extracts deontic rules, and evaluates them for syntactic and semantic coherence. We introduce a refined success metric that more precisely captures the completeness of formalizations, and a novel two-stage pipeline with a dedicated refinement step to improve logical consistency and coverage. The evaluation procedure has been strengthened with stricter error assessment, and we provide comparative results across multiple LLM configurations, including newly released models and various prompting and fine-tuning strategies. Experiments on legal norms from the Australian Telecommunications Consumer Protections Code demonstrate that, when guided effectively, LLMs can produce formalizations that align closely with expert-crafted representations, underscoring their potential for scalable legal informatics.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item104'>[104]</a>
      <a href ="/abs/2506.15301" title="Abstract" id="2506.15301">
        arXiv:2506.15301
      </a>
          (replaced)

        [<a href="/pdf/2506.15301" title="Download PDF" id="pdf-2506.15301" aria-labelledby="pdf-2506.15301">pdf</a>, <a href="https://arxiv.org/html/2506.15301v3" title="View HTML" id="html-2506.15301" aria-labelledby="html-2506.15301" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2506.15301" title="Other formats" id="oth-2506.15301" aria-labelledby="oth-2506.15301">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          A Survey on LLM-Assisted Clinical Trial Recruitment
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ghosh,+S">Shrestha Ghosh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Schneider,+M">Moritz Schneider</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Reinicke,+C">Carina Reinicke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Eickhoff,+C">Carsten Eickhoff</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Accepted to IJCNLP-AACl 2025
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Recent advances in LLMs have greatly improved general-domain NLP tasks. Yet, their adoption in critical domains, such as clinical trial recruitment, remains limited. As trials are designed in natural language and patient data is represented as both structured and unstructured text, the task of matching trials and patients benefits from knowledge aggregation and reasoning abilities of LLMs. Classical approaches are trial-specific and LLMs with their ability to consolidate distributed knowledge hold the potential to build a more general solution. Yet recent applications of LLM-assisted methods rely on proprietary models and weak evaluation benchmarks. In this survey, we are the first to analyze the task of trial-patient matching and contextualize emerging LLM-based approaches in clinical trial recruitment. We critically examine existing benchmarks, approaches and evaluation frameworks, the challenges to adopting LLM technologies in clinical research and exciting future directions.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item105'>[105]</a>
      <a href ="/abs/2507.01785" title="Abstract" id="2507.01785">
        arXiv:2507.01785
      </a>
          (replaced)

        [<a href="/pdf/2507.01785" title="Download PDF" id="pdf-2507.01785" aria-labelledby="pdf-2507.01785">pdf</a>, <a href="https://arxiv.org/html/2507.01785v2" title="View HTML" id="html-2507.01785" aria-labelledby="html-2507.01785" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2507.01785" title="Other formats" id="oth-2507.01785" aria-labelledby="oth-2507.01785">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MuRating: A High Quality Data Selecting Approach to Multilingual Large Language Model Pretraining
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z">Zhixun Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+P">Ping Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+W">Wenhan Han</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yifan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+B">Binbin Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+H">Haobin Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+F">Fengze Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Y">Yan Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+B">Bingni Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+T">Taifeng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+Y">Yin Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fang,+M">Meng Fang</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          NeurIPS 2025 poster
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Data quality is a critical driver of large language model performance, yet existing model-based selection methods focus almost exclusively on English. We introduce MuRating, a scalable framework that transfers high-quality English data-quality signals into a single rater for 17 target languages. MuRating aggregates multiple English &#34;raters&#34; via pairwise comparisons to learn unified document-quality scores,then projects these judgments through translation to train a multilingual evaluator on monolingual, cross-lingual, and parallel text pairs. Applied to web data, MuRating selects balanced subsets of English and multilingual content to pretrain a 1.2 B-parameter LLaMA model. Compared to strong baselines, including QuRater, AskLLM, DCLM and so on, our approach boosts average accuracy on both English benchmarks and multilingual evaluations, with especially large gains on knowledge-intensive tasks. We further analyze translation fidelity, selection biases, and underrepresentation of narrative material, outlining directions for future work.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item106'>[106]</a>
      <a href ="/abs/2507.06415" title="Abstract" id="2507.06415">
        arXiv:2507.06415
      </a>
          (replaced)

        [<a href="/pdf/2507.06415" title="Download PDF" id="pdf-2507.06415" aria-labelledby="pdf-2507.06415">pdf</a>, <a href="https://arxiv.org/html/2507.06415v2" title="View HTML" id="html-2507.06415" aria-labelledby="html-2507.06415" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2507.06415" title="Other formats" id="oth-2507.06415" aria-labelledby="oth-2507.06415">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z">Zeming Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Romanou,+A">Angelika Romanou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Weiss,+G">Gail Weiss</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bosselut,+A">Antoine Bosselut</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          10 pages, 7 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Long-context reasoning requires accurately identifying relevant information in extensive, noisy input contexts. Previous research shows that using test-time learning to encode context directly into model parameters can effectively enable reasoning over noisy information. However, meta-learning methods for enabling test-time learning are prohibitively memory-intensive, preventing their application to long context settings. In this work, we propose PERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for learning to encode long input contexts using gradient updates to a lightweight model adapter at test time. Specifically, PERK employs two nested optimization loops in a meta-training phase. The inner loop rapidly encodes contexts into a low-rank adapter (LoRA) that serves as a parameter-efficient memory module for the base model. Concurrently, the outer loop learns to use the updated adapter to accurately recall and reason over relevant information from the encoded long context. Our evaluations on several long-context reasoning tasks show that PERK significantly outperforms the standard prompt-based long-context baseline, achieving average absolute performance gains of up to 90% for smaller models (GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In general, PERK is more robust to reasoning complexity, length extrapolation, and the locations of relevant information in contexts. Finally, we show that while PERK is memory-intensive during training, it scales more efficiently at inference time than prompt-based long-context inference.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item107'>[107]</a>
      <a href ="/abs/2507.17974" title="Abstract" id="2507.17974">
        arXiv:2507.17974
      </a>
          (replaced)

        [<a href="/pdf/2507.17974" title="Download PDF" id="pdf-2507.17974" aria-labelledby="pdf-2507.17974">pdf</a>, <a href="/format/2507.17974" title="Other formats" id="oth-2507.17974" aria-labelledby="oth-2507.17974">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Natural Language Processing for Tigrinya: Current State and Future Directions
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gaim,+F">Fitsum Gaim</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Park,+J+C">Jong C. Park</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Despite being spoken by millions of people, Tigrinya remains severely underrepresented in Natural Language Processing (NLP) research. This work presents a comprehensive survey of NLP research for Tigrinya, analyzing over 50 studies from 2011 to 2025. We systematically review the current state of computational resources, models, and applications across fifteen downstream tasks, including morphological processing, part-of-speech tagging, named entity recognition, machine translation, question-answering, speech recognition, and synthesis. Our analysis reveals a clear trajectory from foundational, rule-based systems to modern neural architectures, with progress consistently driven by milestones in resource creation. We identify key challenges rooted in Tigrinya&#39;s morphological properties and resource scarcity, and highlight promising research directions, including morphology-aware modeling, cross-lingual transfer, and community-centered resource development. This work serves both as a reference for researchers and as a roadmap for advancing Tigrinya NLP. An anthology of surveyed studies and resources is publicly available.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item108'>[108]</a>
      <a href ="/abs/2508.00743" title="Abstract" id="2508.00743">
        arXiv:2508.00743
      </a>
          (replaced)

        [<a href="/pdf/2508.00743" title="Download PDF" id="pdf-2508.00743" aria-labelledby="pdf-2508.00743">pdf</a>, <a href="/format/2508.00743" title="Other formats" id="oth-2508.00743" aria-labelledby="oth-2508.00743">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Multi-step retrieval and reasoning improves radiology question answering with large language models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wind,+S">Sebastian Wind</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sopa,+J">Jeta Sopa</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Truhn,+D">Daniel Truhn</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lotfinia,+M">Mahshad Lotfinia</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nguyen,+T">Tri-Thien Nguyen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bressem,+K">Keno Bressem</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Adams,+L">Lisa Adams</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rusu,+M">Mirabela Rusu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=K%C3%B6stler,+H">Harald Kstler</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wellein,+G">Gerhard Wellein</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Maier,+A">Andreas Maier</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Arasteh,+S+T">Soroosh Tayebi Arasteh</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Published in npj Digital Medicine
        </div>

        <div class='list-journal-ref'><span class='descriptor'>Journal-ref:</span>
          npj Digit. Med. 8, 790 (2025)
        </div>

        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Clinical decision-making in radiology increasingly benefits from artificial intelligence (AI), particularly through large language models (LLMs). However, traditional retrieval-augmented generation (RAG) systems for radiology question answering (QA) typically rely on single-step retrieval, limiting their ability to handle complex clinical reasoning tasks. Here we propose radiology Retrieval and Reasoning (RaR), a multi-step retrieval and reasoning framework designed to improve diagnostic accuracy, factual consistency, and clinical reliability of LLMs in radiology question answering. We evaluated 25 LLMs spanning diverse architectures, parameter scales (0.5B to &gt;670B), and training paradigms (general-purpose, reasoning-optimized, clinically fine-tuned), using 104 expert-curated radiology questions from previously established RSNA-RadioQA and ExtendedQA datasets. To assess generalizability, we additionally tested on an unseen internal dataset of 65 real-world radiology board examination questions. RaR significantly improved mean diagnostic accuracy over zero-shot prompting and conventional online RAG. The greatest gains occurred in small-scale models, while very large models (&gt;200B parameters) demonstrated minimal changes (&lt;2% improvement). Additionally, RaR retrieval reduced hallucinations (mean 9.4%) and retrieved clinically relevant context in 46% of cases, substantially aiding factual grounding. Even clinically fine-tuned models showed gains from RaR (e.g., MedGemma-27B), indicating that retrieval remains beneficial despite embedded domain knowledge. These results highlight the potential of RaR to enhance factuality and diagnostic accuracy in radiology QA, warranting future studies to validate their clinical utility. All datasets, code, and the full RaR framework are publicly available to support open research and clinical translation.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item109'>[109]</a>
      <a href ="/abs/2508.16390" title="Abstract" id="2508.16390">
        arXiv:2508.16390
      </a>
          (replaced)

        [<a href="/pdf/2508.16390" title="Download PDF" id="pdf-2508.16390" aria-labelledby="pdf-2508.16390">pdf</a>, <a href="https://arxiv.org/html/2508.16390v3" title="View HTML" id="html-2508.16390" aria-labelledby="html-2508.16390" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2508.16390" title="Other formats" id="oth-2508.16390" aria-labelledby="oth-2508.16390">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MedQARo: A Large-Scale Benchmark for Evaluating Large Language Models on Medical Question Answering in Romanian
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rogoz,+A">Ana-Cristina Rogoz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ionescu,+R+T">Radu Tudor Ionescu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Anghel,+A">Alexandra-Valentina Anghel</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Antone-Iordache,+I">Ionut-Lucian Antone-Iordache</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Coniac,+S">Simona Coniac</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ionescu,+A+I">Andreea Iuliana Ionescu</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Question answering (QA) is an actively studied topic, being a core natural language processing (NLP) task that needs to be addressed before achieving Artificial General Intelligence (AGI). However, the lack of QA datasets in specific domains and languages hinders the development of robust AI models able to generalize across various domains and languages. To this end, we introduce MedQARo, the first large-scale medical QA benchmark in Romanian, alongside a comprehensive evaluation of state-of-the-art (SOTA) large language models (LLMs). We construct a high-quality and large-scale dataset comprising 105,880 QA pairs related to cancer patients from two medical centers. The questions regard medical case summaries of 1,242 patients, requiring either keyword extraction or reasoning to be answered correctly. MedQARo is the result of a time-consuming manual annotation process carried out by seven physicians specialized in oncology or radiotherapy, who spent a total of about 3,000 work hours to generate the QA pairs. Our benchmark contains both in-domain and cross-domain (cross-center and cross-cancer) test collections, enabling a precise assessment of generalization capabilities. We experiment with four open-source LLMs from distinct families of models on MedQARo. Each model is employed in two scenarios, namely one based on zero-shot prompting and one based on supervised fine-tuning. We also evaluate two state-of-the-art LLMs exposed only through APIs, namely GPT-5.2 and Gemini 3 Flash. Our results show that fine-tuned models significantly outperform zero-shot models, clearly indicating that pretrained models fail to generalize on MedQARo. Our findings demonstrate the importance of both domain-specific and language-specific fine-tuning for reliable clinical QA in Romanian. We publicly release our dataset and code at <a href="https://github.com/ana-rogoz/MedQARo" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item110'>[110]</a>
      <a href ="/abs/2509.01560" title="Abstract" id="2509.01560">
        arXiv:2509.01560
      </a>
          (replaced)

        [<a href="/pdf/2509.01560" title="Download PDF" id="pdf-2509.01560" aria-labelledby="pdf-2509.01560">pdf</a>, <a href="https://arxiv.org/html/2509.01560v3" title="View HTML" id="html-2509.01560" aria-labelledby="html-2509.01560" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2509.01560" title="Other formats" id="oth-2509.01560" aria-labelledby="oth-2509.01560">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          In-N-Out: A Parameter-Level API Graph Dataset for Tool Agents
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+S">Seungkyu Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kim,+N">Nalim Kim</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jo,+Y">Yohan Jo</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Tool agents--LLM-based systems that interact with external APIs--offer a way to execute real-world tasks. However, as tasks become increasingly complex, these agents struggle to identify and call the correct APIs in the proper order. To tackle this problem, we investigate converting API documentation into a structured API graph that captures API dependencies and leveraging it for multi-tool queries that require compositional API calls. To support this, we introduce In-N-Out, the first expert-annotated dataset of API graphs built from two real-world API benchmarks and their documentation. Using In-N-Out significantly improves performance on both tool retrieval and multi-tool query generation, nearly doubling that of LLMs using documentation alone. Moreover, graphs generated by models fine-tuned on In-N-Out close 90% of this gap, showing that our dataset helps models learn to comprehend API documentation and parameter relationships. Our findings highlight the promise of using explicit API graphs for tool agents and the utility of In-N-Out as a valuable resource. We release our dataset and code at <a href="https://github.com/holi-lab/In-N-Out-API-Graph" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item111'>[111]</a>
      <a href ="/abs/2509.04903" title="Abstract" id="2509.04903">
        arXiv:2509.04903
      </a>
          (replaced)

        [<a href="/pdf/2509.04903" title="Download PDF" id="pdf-2509.04903" aria-labelledby="pdf-2509.04903">pdf</a>, <a href="https://arxiv.org/html/2509.04903v3" title="View HTML" id="html-2509.04903" aria-labelledby="html-2509.04903" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2509.04903" title="Other formats" id="oth-2509.04903" aria-labelledby="oth-2509.04903">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          ACE-RL: Adaptive Constraint-Enhanced Reward for Long-form Generation Reinforcement Learning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+J">Jianghao Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+W">Wei Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+Q">Qixiang Yin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+Z">Zhixing Tan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">Jiajun Zhang</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Under review
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Long-form generation has become a critical and challenging application for Large Language Models (LLMs). Existing studies are limited by their reliance on scarce, high-quality long-form response data and their focus on coarse-grained, general-purpose metrics (e.g., coherence and helpfulness), overlooking the nuanced, scenario-specific requirements of real-world tasks. To address these limitations, we propose a framework utilizing Adaptive Constraint-Enhanced reward for long-form generation Reinforcement Learning (ACE-RL). ACE-RL first decomposes each instruction into a set of fine-grained, adaptive constraint criteria spanning key dimensions of long-form generation tasks. Subsequently, we design a reward mechanism to quantify the response quality based on their satisfaction over corresponding constraints, converting subjective quality evaluation into constraint verification. Finally, we leverage reinforcement learning to optimize LLMs using these fine-grained signals. Experimental results show that ACE-RL significantly outperforms existing SFT and RL baselines by 18.63% and 7.61% on WritingBench, and our top-performing model even surpasses proprietary systems like GPT-4o by 8.76%, providing a more effective training paradigm in long-form generation scenarios.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item112'>[112]</a>
      <a href ="/abs/2509.15579" title="Abstract" id="2509.15579">
        arXiv:2509.15579
      </a>
          (replaced)

        [<a href="/pdf/2509.15579" title="Download PDF" id="pdf-2509.15579" aria-labelledby="pdf-2509.15579">pdf</a>, <a href="https://arxiv.org/html/2509.15579v2" title="View HTML" id="html-2509.15579" aria-labelledby="html-2509.15579" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2509.15579" title="Other formats" id="oth-2509.15579" aria-labelledby="oth-2509.15579">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Chunk Based Speech Pre-training with High Resolution Finite Scalar Quantization
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+Y">Yun Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tseng,+C">Cindy Tseng</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Sound (cs.SD); Audio and Speech Processing (eess.AS)
        </div>

        <p class='mathjax'>
          Low latency speech human-machine communication is becoming increasingly necessary as speech technology advances quickly in the last decade. One of the primary factors behind the advancement of speech technology is self-supervised learning. Most self-supervised learning algorithms are designed with full utterance assumption and compromises have to made if partial utterances are presented, which are common in the streaming applications. In this work, we propose a chunk based self-supervised learning (Chunk SSL) algorithm as an unified solution for both streaming and offline speech pre-training. Chunk SSL is optimized with the masked prediction loss and an acoustic encoder is encouraged to restore indices of those masked speech frames with help from unmasked frames in the same chunk and preceding chunks. A copy and append data augmentation approach is proposed to conduct efficient chunk based pre-training. Chunk SSL utilizes a finite scalar quantization (FSQ) module to discretize input speech features and our study shows a high resolution FSQ codebook, i.e., a codebook with vocabulary size up to a few millions, is beneficial to transfer knowledge from the pre-training task to the downstream tasks. A group masked prediction loss is employed during pre-training to alleviate the high memory and computation cost introduced by the large codebook. The proposed approach is examined in two speech to text tasks, i.e., speech recognition and speech translation. Experimental results on the \textsc{Librispeech} and \textsc{Must-C} datasets show that the proposed method could achieve very competitive results for speech to text tasks at both streaming and offline modes.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item113'>[113]</a>
      <a href ="/abs/2509.17912" title="Abstract" id="2509.17912">
        arXiv:2509.17912
      </a>
          (replaced)

        [<a href="/pdf/2509.17912" title="Download PDF" id="pdf-2509.17912" aria-labelledby="pdf-2509.17912">pdf</a>, <a href="https://arxiv.org/html/2509.17912v2" title="View HTML" id="html-2509.17912" aria-labelledby="html-2509.17912" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2509.17912" title="Other formats" id="oth-2509.17912" aria-labelledby="oth-2509.17912">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          SiDiaC: Sinhala Diachronic Corpus
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jayatilleke,+N">Nevidu Jayatilleke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=de+Silva,+N">Nisansa de Silva</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          17 pages, 7 figures, 9 tables, Accepted paper at the 39th Pacific Asia Conference on Language, Information and Computation (PACLIC 39)
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          SiDiaC, the first comprehensive Sinhala Diachronic Corpus, covers a historical span from the 5th to the 20th century CE. SiDiaC comprises 58k words across 46 literary works, annotated carefully based on the written date, after filtering based on availability, authorship, copyright compliance, and data attribution. Texts from the National Library of Sri Lanka were digitised using Google Document AI OCR, followed by post-processing to correct formatting and modernise the orthography. The construction of SiDiaC was informed by practices from other corpora, such as FarPaHC, particularly in syntactic annotation and text normalisation strategies, due to the shared characteristics of low-resourced language status. This corpus is categorised based on genres into two layers: primary and secondary. Primary categorisation is binary, classifying each book into Non-Fiction or Fiction, while the secondary categorisation is more specific, grouping texts under Religious, History, Poetry, Language, and Medical genres. Despite challenges including limited access to rare texts and reliance on secondary date sources, SiDiaC serves as a foundational resource for Sinhala NLP, significantly extending the resources available for Sinhala, enabling diachronic studies in lexical change, neologism tracking, historical syntax, and corpus-based lexicography.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item114'>[114]</a>
      <a href ="/abs/2510.05138" title="Abstract" id="2510.05138">
        arXiv:2510.05138
      </a>
          (replaced)

        [<a href="/pdf/2510.05138" title="Download PDF" id="pdf-2510.05138" aria-labelledby="pdf-2510.05138">pdf</a>, <a href="https://arxiv.org/html/2510.05138v3" title="View HTML" id="html-2510.05138" aria-labelledby="html-2510.05138" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2510.05138" title="Other formats" id="oth-2510.05138" aria-labelledby="oth-2510.05138">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Go,+G+H+T">Gregory Hok Tjoan Go</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ly,+K">Khang Ly</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=S%C3%B8gaard,+A">Anders Sgaard</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tabatabaei,+A">Amin Tabatabaei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=de+Rijke,+M">Maarten de Rijke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+X">Xinyi Chen</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Camera-ready version
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          The rapid growth of scientific publications has made it increasingly difficult to keep literature reviews comprehensive and up-to-date. Though prior work has focused on automating retrieval and screening, the writing phase of systematic reviews remains largely under-explored, especially with regard to readability and factual accuracy. To address this, we present LiRA (Literature Review Agents), a multi-agent collaborative workflow which emulates the human literature review process. LiRA utilizes specialized agents for content outlining, subsection writing, editing, and reviewing, producing cohesive and comprehensive review articles. Evaluated on SciReviewGen and a proprietary ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey and MASS-Survey in writing and citation quality, while maintaining competitive similarity to human-written reviews. We further evaluate LiRA in real-world scenarios using document retrieval and assess its robustness to reviewer model variation. Our findings highlight the potential of agentic LLM workflows, even without domain-specific tuning, to improve the reliability and usability of automated scientific writing.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item115'>[115]</a>
      <a href ="/abs/2510.10161" title="Abstract" id="2510.10161">
        arXiv:2510.10161
      </a>
          (replaced)

        [<a href="/pdf/2510.10161" title="Download PDF" id="pdf-2510.10161" aria-labelledby="pdf-2510.10161">pdf</a>, <a href="/format/2510.10161" title="Other formats" id="oth-2510.10161" aria-labelledby="oth-2510.10161">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Large Language Model Sourcing: A Survey
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pang,+L">Liang Pang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+J">Jia Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dai,+S">Sunhao Dai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+Z">Zihao Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Duan,+Z">Zenghao Duan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wu,+K">Kangxi Wu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+Z">Zhiyi Yin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+J">Jun Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+H">Huawei Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+X">Xueqi Cheng</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          31 pages
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Due to the black-box nature of large language models (LLMs) and the realism of their generated content, issues such as hallucinations, bias, unfairness, and copyright infringement have become significant. In this context, sourcing information from multiple perspectives is essential. This survey presents a systematic investigation organized around four interrelated dimensions: Model Sourcing, Model Structure Sourcing, Training Data Sourcing, and External Data Sourcing. Moreover, a unified dual-paradigm taxonomy is proposed that classifies existing sourcing methods into prior-based (proactive traceability embedding) and posterior-based (retrospective inference) approaches. Traceability across these dimensions enhances the transparency, accountability, and trustworthiness of LLMs deployment in real-world applications.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item116'>[116]</a>
      <a href ="/abs/2510.11557" title="Abstract" id="2510.11557">
        arXiv:2510.11557
      </a>
          (replaced)

        [<a href="/pdf/2510.11557" title="Download PDF" id="pdf-2510.11557" aria-labelledby="pdf-2510.11557">pdf</a>, <a href="https://arxiv.org/html/2510.11557v2" title="View HTML" id="html-2510.11557" aria-labelledby="html-2510.11557" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2510.11557" title="Other formats" id="oth-2510.11557" aria-labelledby="oth-2510.11557">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Invisible Languages of the LLM Universe
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Khanna,+S">Saurabh Khanna</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xinxu Li</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Large Language Models are trained on massive multilingual corpora, yet this abundance masks a profound crisis: of the world&#39;s 7,613 living languages, approximately 2,000 languages with millions of speakers remain effectively invisible in digital ecosystems. We propose a critical framework connecting empirical measurements of language vitality (real world demographic strength) and digitality (online presence) with postcolonial theory and epistemic injustice to explain why linguistic inequality in AI systems is not incidental but structural. Analyzing data across all documented human languages, we identify four categories: Strongholds (33%, high vitality and digitality), Digital Echoes (6%, high digitality despite declining vitality), Fading Voices (36%, low on both dimensions), and critically, Invisible Giants (27%, high vitality but near-zero digitality) - languages spoken by millions yet absent from the LLM universe. We demonstrate that these patterns reflect continuities from colonial-era linguistic hierarchies to contemporary AI development, constituting digital epistemic injustice. Our analysis reveals that English dominance in AI is not a technical necessity but an artifact of power structures that systematically exclude marginalized linguistic knowledge. We conclude with implications for decolonizing language technology and democratizing access to AI benefits.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item117'>[117]</a>
      <a href ="/abs/2510.13940" title="Abstract" id="2510.13940">
        arXiv:2510.13940
      </a>
          (replaced)

        [<a href="/pdf/2510.13940" title="Download PDF" id="pdf-2510.13940" aria-labelledby="pdf-2510.13940">pdf</a>, <a href="https://arxiv.org/html/2510.13940v2" title="View HTML" id="html-2510.13940" aria-labelledby="html-2510.13940" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2510.13940" title="Other formats" id="oth-2510.13940" aria-labelledby="oth-2510.13940">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Z">Zhen Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M">Mingyang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+F">Feng Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ding,+G">Ganggui Ding</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hou,+L">Liang Hou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tao,+X">Xin Tao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wan,+P">Pengfei Wan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Y">Ying-Cong Chen</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Code: <a href="https://github.com/EnVision-Research/MTI" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model&#39;s KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +9.28% average improvement on six benchmarks for DeepSeek-R1-7B and +11.25% on AIME2024 using Ling-mini-2.0-while remaining highly efficient.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item118'>[118]</a>
      <a href ="/abs/2511.00602" title="Abstract" id="2511.00602">
        arXiv:2511.00602
      </a>
          (replaced)

        [<a href="/pdf/2511.00602" title="Download PDF" id="pdf-2511.00602" aria-labelledby="pdf-2511.00602">pdf</a>, <a href="https://arxiv.org/html/2511.00602v2" title="View HTML" id="html-2511.00602" aria-labelledby="html-2511.00602" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2511.00602" title="Other formats" id="oth-2511.00602" aria-labelledby="oth-2511.00602">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          OpenSIR: Open-Ended Self-Improving Reasoner
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kwan,+W">Wai-Chung Kwan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Leang,+J+O+J">Joshua Ong Jun Leang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Vougiouklis,+P">Pavlos Vougiouklis</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pan,+J+Z">Jeff Z. Pan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Valentino,+M">Marco Valentino</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Minervini,+P">Pasquale Minervini</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Recent advances in large language model (LLM) reasoning through reinforcement learning rely on annotated datasets for verifiable rewards, which may limit models&#39; ability to surpass human-level performance. While self-play offers a promising alternative, existing approaches depend on external verifiers or cannot learn open-endedly. We present Open-Ended Self-Improving Reasoner (OpenSIR), a self-play framework where an LLM learns to generate and solve novel problems by alternating teacher and student roles without external supervision. To generate novel problems, OpenSIR optimises for both difficulty and diversity, rewarding problems that challenge appropriately while exploring distinct concepts, enabling open-ended mathematical discovery. Starting from a single trivial seed problem, OpenSIR substantially improves instruction models: Llama-3.2-3B-Instruct advances from 73.9 to 78.3 on GSM8K, and from 28.8 to 34.4 on College Math, while Gemma-2-2B-Instruct rises from 38.5 to 58.7 on GSM8K. Our analyses reveal that OpenSIR achieves open-ended learning through co-evolving teacher-student roles that adaptively calibrate difficulty and drive diverse exploration, progressing autonomously from basic to advanced mathematics.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item119'>[119]</a>
      <a href ="/abs/2511.07055" title="Abstract" id="2511.07055">
        arXiv:2511.07055
      </a>
          (replaced)

        [<a href="/pdf/2511.07055" title="Download PDF" id="pdf-2511.07055" aria-labelledby="pdf-2511.07055">pdf</a>, <a href="https://arxiv.org/html/2511.07055v2" title="View HTML" id="html-2511.07055" aria-labelledby="html-2511.07055" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2511.07055" title="Other formats" id="oth-2511.07055" aria-labelledby="oth-2511.07055">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Can ensembles improve evidence recall? A case study
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Beckh,+K">Katharina Beckh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Heuser,+S">Sven Heuser</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=R%C3%BCping,+S">Stefan Rping</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Submitted to ESANN 2026
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Information Retrieval (cs.IR); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Feature attribution methods typically provide minimal sufficient evidence justifying a model decision. However, in many applications, such as compliance and cataloging, the full set of contributing features must be identified: complete evidence. We present a case study using existing language models and a medical dataset which contains human-annotated complete evidence. Our findings show that an ensemble approach, aggregating evidence from several models, improves evidence recall over individual models. We examine different ensemble sizes, the effect of evidence-guided training, and provide qualitative insights.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item120'>[120]</a>
      <a href ="/abs/2511.08579" title="Abstract" id="2511.08579">
        arXiv:2511.08579
      </a>
          (replaced)

        [<a href="/pdf/2511.08579" title="Download PDF" id="pdf-2511.08579" aria-labelledby="pdf-2511.08579">pdf</a>, <a href="https://arxiv.org/html/2511.08579v2" title="View HTML" id="html-2511.08579" aria-labelledby="html-2511.08579" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2511.08579" title="Other formats" id="oth-2511.08579" aria-labelledby="oth-2511.08579">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Training Language Models to Explain Their Own Computations
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+B+Z">Belinda Z. Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+Z+C">Zifan Carl Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Huang,+V">Vincent Huang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Steinhardt,+J">Jacob Steinhardt</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Andreas,+J">Jacob Andreas</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          33 pages, 7 tables, 8 figures. Code and data at <a href="https://github.com/TransluceAI/introspective-interp" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Can language models (LMs) learn to faithfully describe their internal computations? Are they better able to describe themselves than other models? We study the extent to which LMs&#39; privileged access to their own internals can be leveraged to produce new techniques for explaining their behavior. Using existing interpretability techniques as a source of ground truth, we fine-tune LMs to generate natural language descriptions of (1) the information encoded by LM features, (2) the causal structure of LMs&#39; internal activations, and (3) the influence of specific input tokens on LM outputs. When trained with only tens of thousands of example explanations, explainer models exhibit non-trivial generalization to new queries. This generalization appears partly attributable to explainer models&#39; privileged access to their own internals: using a model to explain its own computations generally works better than using a *different* model to explain its computations (even if the other model is significantly more capable). Our results suggest not only that LMs can learn to reliably explain their internal computations, but that such explanations offer a scalable complement to existing interpretability methods. Code and data at <a href="https://github.com/TransluceAI/introspective-interp" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </p>
      </div>
    </dd>
    <dt>
      <a name='item121'>[121]</a>
      <a href ="/abs/2511.10045" title="Abstract" id="2511.10045">
        arXiv:2511.10045
      </a>
          (replaced)

        [<a href="/pdf/2511.10045" title="Download PDF" id="pdf-2511.10045" aria-labelledby="pdf-2511.10045">pdf</a>, <a href="https://arxiv.org/html/2511.10045v4" title="View HTML" id="html-2511.10045" aria-labelledby="html-2511.10045" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2511.10045" title="Other formats" id="oth-2511.10045" aria-labelledby="oth-2511.10045">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Do Language Models Associate Sound with Meaning? A Multimodal Study of Sound Symbolism
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jeong,+J">Jinhong Jeong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+S">Sunghyun Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lee,+J">Jaeyoung Lee</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+S">Seonah Han</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yu,+Y">Youngjae Yu</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          33 pages, 27 tables, 10 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Sound symbolism is a linguistic concept that refers to non-arbitrary associations between phonetic forms and their meanings. We suggest that this can be a compelling probe into how Multimodal Large Language Models (MLLMs) interpret auditory information in human languages. We investigate MLLMs&#39; performance on phonetic iconicity across textual (orthographic and IPA) and auditory forms of inputs with up to 25 semantic dimensions (e.g., sharp vs. round), observing models&#39; layer-wise information processing by measuring phoneme-level attention fraction scores. To this end, we present LEX-ICON, an extensive mimetic word dataset consisting of 8,052 words from four natural languages (English, French, Japanese, and Korean) and 2,930 systematically constructed pseudo-words, annotated with semantic features applied across both text and audio modalities. Our key findings demonstrate (1) MLLMs&#39; phonetic intuitions that align with existing linguistic research across multiple semantic dimensions and (2) phonosemantic attention patterns that highlight models&#39; focus on iconic phonemes. These results bridge domains of artificial intelligence and cognitive linguistics, providing the first large-scale, quantitative analyses of phonetic iconicity in terms of MLLMs&#39; interpretability.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item122'>[122]</a>
      <a href ="/abs/2511.10684" title="Abstract" id="2511.10684">
        arXiv:2511.10684
      </a>
          (replaced)

        [<a href="/pdf/2511.10684" title="Download PDF" id="pdf-2511.10684" aria-labelledby="pdf-2511.10684">pdf</a>, <a href="https://arxiv.org/html/2511.10684v3" title="View HTML" id="html-2511.10684" aria-labelledby="html-2511.10684" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2511.10684" title="Other formats" id="oth-2511.10684" aria-labelledby="oth-2511.10684">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          SpiderGen: Towards Procedure Generation For Carbon Life Cycle Assessments with Generative AI
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sitaraman,+A">Anupama Sitaraman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Balaji,+B">Bharathan Balaji</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Agarwal,+Y">Yuvraj Agarwal</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)
        </div>

        <p class='mathjax'>
          Investigating the effects of climate change and global warming caused by GHG emissions have been a key concern worldwide. These emissions are largely contributed to by the production, use and disposal of consumer products. Thus, it is important to build tools to estimate the environmental impact of consumer goods, an essential part of which is conducting Life Cycle Assessments (LCAs). LCAs specify and account for the appropriate processes involved with the production, use, and disposal of the products. We present SpiderGen, an LLM-based workflow which integrates the taxonomy and methodology of traditional LCA with the reasoning capabilities and world knowledge of LLMs to generate graphical representations of the key procedural information used for LCA, known as Product Category Rules Process Flow Graphs (PCR PFGs). We additionally evaluate the output of SpiderGen by comparing it with 65 real-world LCA documents. We find that SpiderGen provides accurate LCA process information that is either fully correct or has minor errors, achieving an F1-Score of 65% across 10 sample data points, as compared to 53% using a one-shot prompting method. We observe that the remaining errors occur primarily due to differences in detail between LCA documents, as well as differences in the &#34;scope&#34; of which auxiliary processes must also be included. We also demonstrate that SpiderGen performs better than several baselines techniques, such as chain-of-thought prompting and one-shot prompting. Finally, we highlight SpiderGen&#39;s potential to reduce the human effort and costs for estimating carbon impact, as it is able to produce LCA process information for less than \$1 USD in under 10 minutes as compared to the status quo LCA, which can cost over \$25000 USD and take up to 21-person days.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item123'>[123]</a>
      <a href ="/abs/2511.20143" title="Abstract" id="2511.20143">
        arXiv:2511.20143
      </a>
          (replaced)

        [<a href="/pdf/2511.20143" title="Download PDF" id="pdf-2511.20143" aria-labelledby="pdf-2511.20143">pdf</a>, <a href="/format/2511.20143" title="Other formats" id="oth-2511.20143" aria-labelledby="oth-2511.20143">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          SEDA: A Self-Adapted Entity-Centric Data Augmentation for Boosting Gird-based Discontinuous NER Models
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+W">Wen-Fang Su</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chou,+H">Hsiao-Wei Chou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+W">Wen-Yang Lin</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          9 pages, 5 figures. This paper was presented at the CIKM&#39;25 Workshop on Small and Efficient Large Language Models for Knowledge Extraction
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Information Retrieval (cs.IR)
        </div>

        <p class='mathjax'>
          Named Entity Recognition (NER) is a critical task in natural language processing, yet it remains particularly challenging for discontinuous entities. The primary difficulty lies in text segmentation, as traditional methods often missegment or entirely miss cross-sentence discontinuous entities, significantly affecting recognition accuracy. Therefore, we aim to address the segmentation and omission issues associated with such entities. Recent studies have shown that grid-tagging methods are effective for information extraction due to their flexible tagging schemes and robust architectures. Building on this, we integrate image data augmentation techniques, such as cropping, scaling, and padding, into grid-based models to enhance their ability to recognize discontinuous entities and handle segmentation challenges. Experimental results demonstrate that traditional segmentation methods often fail to capture cross-sentence discontinuous entities, leading to decreased performance. In contrast, our augmented grid models achieve notable improvements. Evaluations on the CADEC, ShARe13, and ShARe14 datasets show F1 score gains of 1-2.5% overall and 3.7-8.4% for discontinuous entities, confirming the effectiveness of our approach.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item124'>[124]</a>
      <a href ="/abs/2512.03402" title="Abstract" id="2512.03402">
        arXiv:2512.03402
      </a>
          (replaced)

        [<a href="/pdf/2512.03402" title="Download PDF" id="pdf-2512.03402" aria-labelledby="pdf-2512.03402">pdf</a>, <a href="https://arxiv.org/html/2512.03402v3" title="View HTML" id="html-2512.03402" aria-labelledby="html-2512.03402" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.03402" title="Other formats" id="oth-2512.03402" aria-labelledby="oth-2512.03402">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Dual LoRA: Enhancing LoRA with Magnitude and Direction Updates
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+Y">Yixing Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+C">Chao Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+X">Xuanwu Yin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tiwari,+S">Spandan Tiwari</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+D">Dong Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sirasao,+A">Ashish Sirasao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Barsoum,+E">Emad Barsoum</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Low-rank adaptation (LoRA) is one of the most popular methods among parameter-efficient fine-tuning (PEFT) methods to adapt pre-trained large language models (LLMs) to specific downstream tasks. However, the model trained based on LoRA often has an unsatisfactory performance due to its low-rank assumption. In this paper, we propose a novel method called Dual LoRA to improve the performance by incorporating an inductive bias into the original LoRA. Specifically, we separate low-rank matrices into two groups: the magnitude group to control whether or not and how far we should update a parameter and the direction group to decide whether this parameter should move forward or backward, to better simulate the parameter updating process of the full fine-tuning based on gradient-based optimization algorithms. We show that this can be simply achieved by adding a ReLU function to the magnitude group and a sign function to the direction group. We conduct several experiments over a wide range of NLP tasks, including natural language understanding (NLU) and commonsense reasoning datasets on RoBERTa, DeBERTa, and LLaMA-1/2/3 as baseline models. The results show that we consistently outperform LoRA and its state-of-the-art variants with the same number of trainable parameters.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item125'>[125]</a>
      <a href ="/abs/2512.07540" title="Abstract" id="2512.07540">
        arXiv:2512.07540
      </a>
          (replaced)

        [<a href="/pdf/2512.07540" title="Download PDF" id="pdf-2512.07540" aria-labelledby="pdf-2512.07540">pdf</a>, <a href="https://arxiv.org/html/2512.07540v3" title="View HTML" id="html-2512.07540" aria-labelledby="html-2512.07540" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.07540" title="Other formats" id="oth-2512.07540" aria-labelledby="oth-2512.07540">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Minimum Bayes Risk Decoding for Error Span Detection in Reference-Free Automatic Machine Translation Evaluation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lyu,+B">Boxuan Lyu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Song,+H">Haiyue Song</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kamigaito,+H">Hidetaka Kamigaito</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ding,+C">Chenchen Ding</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tanaka,+H">Hideki Tanaka</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Utiyama,+M">Masao Utiyama</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Funakoshi,+K">Kotaro Funakoshi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Okumura,+M">Manabu Okumura</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Error Span Detection (ESD) extends automatic machine translation (MT) evaluation by localizing translation errors and labeling their severity. Current generative ESD methods typically use Maximum a Posteriori (MAP) decoding, assuming that the model-estimated probabilities are perfectly correlated with similarity to the human annotation, but we often observe higher likelihood assigned to an incorrect annotation than to the human one. We instead apply Minimum Bayes Risk (MBR) decoding to generative ESD. We use a sentence- or span-level similarity function for MBR decoding, which selects candidate hypotheses based on their approximate similarity to the human annotation. Experimental results on the WMT24 Metrics Shared Task show that MBR decoding significantly improves span-level performance and generally matches or outperforms MAP at the system and sentence levels. To reduce the computational cost of MBR decoding, we further distill its decisions into a model decoded via greedy search, removing the inference-time latency bottleneck.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item126'>[126]</a>
      <a href ="/abs/2512.17083" title="Abstract" id="2512.17083">
        arXiv:2512.17083
      </a>
          (replaced)

        [<a href="/pdf/2512.17083" title="Download PDF" id="pdf-2512.17083" aria-labelledby="pdf-2512.17083">pdf</a>, <a href="https://arxiv.org/html/2512.17083v3" title="View HTML" id="html-2512.17083" aria-labelledby="html-2512.17083" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.17083" title="Other formats" id="oth-2512.17083" aria-labelledby="oth-2512.17083">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Coen,+M+H">Michael H. Coen</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          34 pages, 4 figures. Evaluation and methodology study on dialogue topic segmentation
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of work, evaluation practice remains dominated by strict boundary matching and F1-based metrics. Modern large language model (LLM) based conversational systems increasingly rely on segmentation to manage conversation history beyond fixed context windows. In such systems, unstructured context accumulation degrades efficiency and coherence.
<br>This paper introduces an evaluation framework that reports boundary density and segment alignment diagnostics (purity and coverage) alongside window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, we evaluate segmentation quality across density regimes rather than at a single operating point. Cross-dataset evaluation shows that reported performance differences often reflect annotation granularity mismatch rather than boundary placement quality alone.
<br>We evaluate structurally distinct segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Boundary-based metrics are strongly coupled to boundary density: threshold sweeps produce larger W-F1 changes than switching between methods. These findings support viewing topic segmentation as a granularity selection problem rather than prediction of a single correct boundary set. This motivates separating boundary scoring from boundary selection for analyzing and tuning segmentation under varying annotation granularities.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item127'>[127]</a>
      <a href ="/abs/2512.18072" title="Abstract" id="2512.18072">
        arXiv:2512.18072
      </a>
          (replaced)

        [<a href="/pdf/2512.18072" title="Download PDF" id="pdf-2512.18072" aria-labelledby="pdf-2512.18072">pdf</a>, <a href="https://arxiv.org/html/2512.18072v2" title="View HTML" id="html-2512.18072" aria-labelledby="html-2512.18072" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.18072" title="Other formats" id="oth-2512.18072" aria-labelledby="oth-2512.18072">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Statistical laws and linguistics inform meaning in naturalistic and fictional conversation
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fehr,+A+M+A">Ashley M. A. Fehr</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Beauregard,+C+G">Calla G. Beauregard</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zimmerman,+J+W">Julia Witte Zimmerman</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ekstr%C3%B6m,+K">Katie Ekstrm</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rosillo-Rodes,+P">Pablo Rosillo-Rodes</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Danforth,+C+M">Christopher M. Danforth</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Dodds,+P+S">Peter Sheridan Dodds</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Computers and Society (cs.CY)
        </div>

        <p class='mathjax'>
          Conversation is a cornerstone of social connection and is linked to well-being outcomes. Conversations vary widely in type with some portion generating complex, dynamic stories. One approach to studying how conversations unfold in time is through statistical patterns such as Heaps&#39; law, which holds that vocabulary size scales with document length. Little work on Heaps&#39; law has looked at conversation and considered how language features impact scaling. We measure Heaps&#39; law for conversations recorded in two distinct mediums: 1. Strangers brought together on video chat and 2. Fictional characters in movies. We find that scaling of vocabulary size differs by parts of speech. We discuss these findings through behavioral and linguistic frameworks.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item128'>[128]</a>
      <a href ="/abs/2512.19432" title="Abstract" id="2512.19432">
        arXiv:2512.19432
      </a>
          (replaced)

        [<a href="/pdf/2512.19432" title="Download PDF" id="pdf-2512.19432" aria-labelledby="pdf-2512.19432">pdf</a>, <a href="https://arxiv.org/html/2512.19432v3" title="View HTML" id="html-2512.19432" aria-labelledby="html-2512.19432" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.19432" title="Other formats" id="oth-2512.19432" aria-labelledby="oth-2512.19432">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          MobileWorld: Benchmarking Autonomous Mobile Agents in Agent-User Interactive and MCP-Augmented Environments
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kong,+Q">Quyu Kong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+X">Xu Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Z">Zhenyu Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+N">Nolan Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+C">Chen Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tong,+P">Panrong Tong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+C">Chenglin Cai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+H">Hanzhang Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">Jianan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+L">Liangyu Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Z">Zhidan Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hoi,+S">Steven Hoi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yue Wang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Among existing online mobile-use benchmarks, AndroidWorld has emerged as the dominant benchmark due to its reproducible environment and deterministic evaluation; however, recent agents achieving over 90% success rates indicate its saturation and motivate the need for a more challenging benchmark. In addition, its environment lacks key application categories, such as e-commerce and enterprise communication, and does not reflect realistic mobile-use scenarios characterized by vague user instructions and hybrid tool usage. We introduce MobileWorld, a substantially more challenging benchmark designed to reflect real-world usage through 201 tasks across 20 applications. MobileWorld derives its difficulty from an emphasis on long-horizon, cross-application workflows, requiring nearly twice as many completion steps on average (27.8 vs. 14.3) and featuring a significantly higher proportion of multi-app tasks (62.2% vs. 9.5%) than AndroidWorld. To overcome the limitations of existing environments, MobileWorld achieves a balance between production-grade utility and reproducible evaluation by utilizing open-source alternatives to industry standards (e.g., Mattermost for Slack). This approach enables a fully observable and controlled environment through source code modification and direct backend database access for precise verification. MobileWorld also introduces novel task categories, including agent-user interaction and Model Context Protocol (MCP)-augmented tasks, for evaluating agents in user-aware, hybrid-tool scenarios. To facilitate evaluation, we develop a planner-executor agentic framework with extended action spaces to support user interactions and MCP calls. Our results reveal a sharp performance drop compared to AndroidWorld, with the best agentic framework and end-to-end model achieving 51.7% and 20.9% success rates, respectively, highlighting ample headroom for future research.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item129'>[129]</a>
      <a href ="/abs/2512.20156" title="Abstract" id="2512.20156">
        arXiv:2512.20156
      </a>
          (replaced)

        [<a href="/pdf/2512.20156" title="Download PDF" id="pdf-2512.20156" aria-labelledby="pdf-2512.20156">pdf</a>, <a href="https://arxiv.org/html/2512.20156v3" title="View HTML" id="html-2512.20156" aria-labelledby="html-2512.20156" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.20156" title="Other formats" id="oth-2512.20156" aria-labelledby="oth-2512.20156">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Fun-Audio-Chat Technical Report
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tongyi+Fun+Team">Tongyi Fun Team</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Q">Qian Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+L">Luyao Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+C">Chong Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xiangang Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">Jiaqing Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tan,+C">Chao-Hong Tan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W">Wen Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+J">Junhao Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+J">Jieping Ye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Q">Qinglin Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Q">Qiquan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+J">Jingren Zhou</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Authors are listed in alphabetical order, 21 pages, open-source at <a href="https://github.com/FunAudioLLM/Fun-Audio-Chat" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI); Sound (cs.SD); Audio and Speech Processing (eess.AS)
        </div>

        <p class='mathjax'>
          Recent advancements in joint speech-text models show great potential for seamless voice interactions. However, existing models face critical challenges: temporal resolution mismatch between speech tokens (25Hz) and text tokens (~3Hz) dilutes semantic information, incurs high computational costs, and causes catastrophic forgetting of text LLM knowledge. We introduce Fun-Audio-Chat, a Large Audio Language Model addressing these limitations via two innovations from our previous work DrVoice. First, Dual-Resolution Speech Representations (DRSR): the Shared LLM processes audio at efficient 5Hz (via token grouping), while the Speech Refined Head generates high-quality tokens at 25Hz, balancing efficiency (~50% GPU reduction) and quality. Second, Core-Cocktail Training, a two-stage fine-tuning with intermediate merging that mitigates catastrophic forgetting. We then apply Multi-Task DPO Training to enhance robustness, audio understanding, instruction-following and voice empathy. This multi-stage post-training enables Fun-Audio-Chat to retain text LLM knowledge while gaining powerful audio understanding, reasoning, and generation. Unlike recent LALMs requiring large-scale audio-text pre-training, Fun-Audio-Chat leverages pre-trained models and extensive post-training. Fun-Audio-Chat 8B and MoE 30B-A3B achieve competitive performance on Speech-to-Text and Speech-to-Speech tasks, ranking top among similar-scale models on Spoken QA benchmarks. They also achieve competitive to superior performance on Audio Understanding, Speech Function Calling, Instruction-Following and Voice Empathy. We develop Fun-Audio-Chat-Duplex, a full-duplex variant with strong performance on Spoken QA and full-duplex interactions. We open-source Fun-Audio-Chat-8B with training and inference code, and provide an interactive demo, at <a href="https://github.com/FunAudioLLM/Fun-Audio-Chat" rel="external noopener nofollow" class="link-external link-https">this https URL</a> .
        </p>
      </div>
    </dd>
    <dt>
      <a name='item130'>[130]</a>
      <a href ="/abs/2512.21635" title="Abstract" id="2512.21635">
        arXiv:2512.21635
      </a>
          (replaced)

        [<a href="/pdf/2512.21635" title="Download PDF" id="pdf-2512.21635" aria-labelledby="pdf-2512.21635">pdf</a>, <a href="https://arxiv.org/html/2512.21635v2" title="View HTML" id="html-2512.21635" aria-labelledby="html-2512.21635" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.21635" title="Other formats" id="oth-2512.21635" aria-labelledby="oth-2512.21635">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Heaven-Sent or Hell-Bent? Benchmarking the Intelligence and Defectiveness of LLM Hallucinations
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+C">Chengxu Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+J">Jingling Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+S">Siqi Cai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jiang,+J">Jiawei Jiang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Hu,+C">Chuang Hu</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Published as a conference paper at KDD 2026
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>
        </div>

        <p class='mathjax'>
          Hallucinations in large language models (LLMs) are commonly regarded as errors to be minimized. However, recent perspectives suggest that some hallucinations may encode creative or epistemically valuable content, a dimension that remains underquantified in current literature. Existing hallucination detection methods primarily focus on factual consistency, struggling to handle heterogeneous scientific tasks and balance creativity with accuracy. To address these challenges, we propose HIC-Bench, a novel evaluation framework that categorizes hallucinations into Intelligent Hallucinations (IH) and Defective Hallucinations (DH), enabling systematic investigation of their interplay in LLM creativity. HIC-Bench features three core characteristics: (1) Structured IH/DH Assessment. using a multi-dimensional metric matrix integrating Torrance Tests of Creative Thinking (TTCT) metrics (Originality, Feasibility, Value) with hallucination-specific dimensions (scientific plausibility, factual deviation); (2) Cross-Domain Applicability. spanning ten scientific domains with open-ended innovation tasks; and (3) Dynamic Prompt Optimization. leveraging the Dynamic Hallucination Prompt (DHP) to guide models toward creative and reliable outputs. The evaluation process employs multiple LLM judges, averaging scores to mitigate bias, with human annotators verifying IH/DH classifications. Experimental results reveal a nonlinear relationship between IH and DH, demonstrating that creativity and correctness can be jointly optimized. These insights position IH as a catalyst for creativity and reveal the ability of LLM hallucinations to drive scientific <a href="http://innovation.Additionally" rel="external noopener nofollow" class="link-external link-http">this http URL</a>, the HIC-Bench offers a valuable platform for advancing research into the creative intelligence of LLM hallucinations.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item131'>[131]</a>
      <a href ="/abs/2512.23512" title="Abstract" id="2512.23512">
        arXiv:2512.23512
      </a>
          (replaced)

        [<a href="/pdf/2512.23512" title="Download PDF" id="pdf-2512.23512" aria-labelledby="pdf-2512.23512">pdf</a>, <a href="https://arxiv.org/html/2512.23512v2" title="View HTML" id="html-2512.23512" aria-labelledby="html-2512.23512" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23512" title="Other formats" id="oth-2512.23512" aria-labelledby="oth-2512.23512">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          UniHetero: Could Generation Enhance Understanding for Vision-Language-Model at Large Data Scale?
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+F">Fengjiao Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jing,+M">Minhao Jing</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+W">Weitao Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Feng,+Y">Yan Feng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+X">Xiaoyu Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+X">Xuezhi Cao</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computation and Language (cs.CL)</span>; Artificial Intelligence (cs.AI)
        </div>

        <p class='mathjax'>
          Vision-language large models are moving toward the unification of visual understanding and visual generation tasks. However, whether generation can enhance understanding is still under-explored on large data scale. In this work, we analysis the unified structure with a concise model, UniHetero, under large-scale pretraining (&gt;200M samples). Our key observations are: (1) Generation can improve understanding, but Only if you generate Semantics, Not Pixels. A common assumption in unified vision-language models is that adding generation will naturally strengthen understanding. However, this is not always true at scale. At 200M+ pretraining samples, generation helps understanding only when it operates at the semantic level, i.e. when the model learns to autoregress high-level visual representations inside the LLM. Once pixel-level objectives (e.g., diffusion losses) directly interfere with the LLM, understanding performance often degrades. (2) Generation reveals a superior Data Scaling trend and higher Data Utilization. Unified generation-understanding demonstrates a superior scaling trend compared to understanding alone, revealing a more effective way to learn vision-only knowledge directive from vision modality rather than captioning to text. (3) Autoregression on Input Embedding is effective to capture visual details. Compared to the commonly-used vision encoder, make visual autoregression on input embedding shows less cumulative error and is modality independent, which can be extend to all modalities. The learned semantic representations capture visual information such as objects, locations, shapes, and colors; further enable pixel-level image generation.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item132'>[132]</a>
      <a href ="/abs/2408.07666" title="Abstract" id="2408.07666">
        arXiv:2408.07666
      </a>
          (replaced)

        [<a href="/pdf/2408.07666" title="Download PDF" id="pdf-2408.07666" aria-labelledby="pdf-2408.07666">pdf</a>, <a href="https://arxiv.org/html/2408.07666v5" title="View HTML" id="html-2408.07666" aria-labelledby="html-2408.07666" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2408.07666" title="Other formats" id="oth-2408.07666" aria-labelledby="oth-2408.07666">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Model Merging in LLMs, MLLMs, and Beyond: Methods, Theories, Applications and Opportunities
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+E">Enneng Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shen,+L">Li Shen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+G">Guibing Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X">Xingwei Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cao,+X">Xiaochun Cao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+J">Jie Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tao,+D">Dacheng Tao</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)
        </div>

        <p class='mathjax'>
          Model merging is an efficient empowerment technique in the machine learning community that does not require the collection of raw training data and does not require expensive computation. As model merging becomes increasingly prevalent across various fields, it is crucial to understand the available model merging techniques comprehensively. However, there is a significant gap in the literature regarding a systematic and thorough review of these techniques. This survey provides a comprehensive overview of model merging methods and theories, their applications in various domains and settings, and future research directions. Specifically, we first propose a new taxonomic approach that exhaustively discusses existing model merging methods. Secondly, we discuss the application of model merging techniques in large language models, multimodal large language models, and more than ten machine learning subfields, including continual learning, multi-task learning, few-shot learning, etc. Finally, we highlight the remaining challenges of model merging and discuss future research directions. A comprehensive list of papers about model merging is available at <a href="https://github.com/EnnengYang/Awesome-Model-Merging-Methods-Theories-Applications" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item133'>[133]</a>
      <a href ="/abs/2410.14716" title="Abstract" id="2410.14716">
        arXiv:2410.14716
      </a>
          (replaced)

        [<a href="/pdf/2410.14716" title="Download PDF" id="pdf-2410.14716" aria-labelledby="pdf-2410.14716">pdf</a>, <a href="https://arxiv.org/html/2410.14716v4" title="View HTML" id="html-2410.14716" aria-labelledby="html-2410.14716" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2410.14716" title="Other formats" id="oth-2410.14716" aria-labelledby="oth-2410.14716">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          A Systematic Survey on Large Language Models for Algorithm Design
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+F">Fei Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+Y">Yiming Yao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Guo,+P">Ping Guo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Z">Zhiyuan Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+Z">Zhe Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+X">Xi Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tong,+X">Xialiang Tong</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mao,+K">Kun Mao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+Z">Zhichao Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Z">Zhenkun Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+M">Mingxuan Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Q">Qingfu Zhang</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Algorithm design is crucial for effective problem-solving across various domains. The advent of Large Language Models (LLMs) has notably enhanced the automation and innovation within this field, offering new perspectives and promising solutions. In just a few years, this integration has yielded remarkable progress in areas ranging from combinatorial optimization to scientific discovery. Despite this rapid expansion, a holistic understanding of the field is hindered by the lack of a systematic review, as existing surveys either remain limited to narrow sub-fields or with different objectives. This paper seeks to provide a systematic review of algorithm design with LLMs. We introduce a taxonomy that categorises the roles of LLMs as optimizers, predictors, extractors and designers, analyzing the progress, advantages, and limitations within each category. We further synthesize literature across the three phases of the algorithm design pipeline and across diverse algorithmic applications that define the current landscape. Finally, we outline key open challenges and opportunities to guide future research.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item134'>[134]</a>
      <a href ="/abs/2503.08990" title="Abstract" id="2503.08990">
        arXiv:2503.08990
      </a>
          (replaced)

        [<a href="/pdf/2503.08990" title="Download PDF" id="pdf-2503.08990" aria-labelledby="pdf-2503.08990">pdf</a>, <a href="https://arxiv.org/html/2503.08990v2" title="View HTML" id="html-2503.08990" aria-labelledby="html-2503.08990" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2503.08990" title="Other formats" id="oth-2503.08990" aria-labelledby="oth-2503.08990">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Effective and Efficient Jailbreaks of Black-Box LLMs with Cross-Behavior Attacks
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gohil,+V">Vasudev Gohil</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Code is at <a href="https://github.com/gohil-vasudev/JCB" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Cryptography and Security (cs.CR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Despite recent advancements in Large Language Models (LLMs) and their alignment, they can still be jailbroken, i.e., harmful and toxic content can be elicited from them. While existing red-teaming methods have shown promise in uncovering such vulnerabilities, these methods struggle with limited success and high computational and monetary costs. To address this, we propose a black-box Jailbreak method with Cross-Behavior attacks (JCB), that can automatically and efficiently find successful jailbreak prompts. JCB leverages successes from past behaviors to help jailbreak new behaviors, thereby significantly improving the attack efficiency. Moreover, JCB does not rely on time- and/or cost-intensive calls to auxiliary LLMs to discover/optimize the jailbreak prompts, making it highly efficient and scalable. Comprehensive experimental evaluations show that JCB significantly outperforms related baselines, requiring up to 94% fewer queries while still achieving 12.9% higher average attack success. JCB also achieves a notably high 37% attack success rate on Llama-2-7B, one of the most resilient LLMs, and shows promising zero-shot transferability across different LLMs.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item135'>[135]</a>
      <a href ="/abs/2504.16628" title="Abstract" id="2504.16628">
        arXiv:2504.16628
      </a>
          (replaced)

        [<a href="/pdf/2504.16628" title="Download PDF" id="pdf-2504.16628" aria-labelledby="pdf-2504.16628">pdf</a>, <a href="https://arxiv.org/html/2504.16628v3" title="View HTML" id="html-2504.16628" aria-labelledby="html-2504.16628" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2504.16628" title="Other formats" id="oth-2504.16628" aria-labelledby="oth-2504.16628">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          ParetoHqD: Fast Offline Multiobjective Alignment of Large Language Models using Pareto High-quality Data
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+H">Haoran Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+H">Handing Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Mei,+Y">Yi Mei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+M">Mengjie Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jin,+Y">Yaochu Jin</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Accepted as a main conference paper at AAAI 2026
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          Aligning large language models with multiple human expectations and values is crucial for ensuring that they adequately serve a variety of user needs. To this end, offline multiobjective alignment algorithms such as the Rewards-in-Context algorithm have shown strong performance and efficiency. However, inappropriate preference representations and training with imbalanced reward scores limit the performance of such algorithms. In this work, we introduce ParetoHqD that addresses the above issues by representing human preferences as preference directions in the objective space and regarding data near the Pareto front as &#34;high-quality&#34; data. For each preference, ParetoHqD follows a two-stage supervised fine-tuning process, where each stage uses an individual Pareto high-quality training set that best matches its preference direction. The experimental results have demonstrated the superiority of ParetoHqD over five baselines on two multiobjective alignment tasks.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item136'>[136]</a>
      <a href ="/abs/2505.20730" title="Abstract" id="2505.20730">
        arXiv:2505.20730
      </a>
          (replaced)

        [<a href="/pdf/2505.20730" title="Download PDF" id="pdf-2505.20730" aria-labelledby="pdf-2505.20730">pdf</a>, <a href="https://arxiv.org/html/2505.20730v3" title="View HTML" id="html-2505.20730" aria-labelledby="html-2505.20730" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2505.20730" title="Other formats" id="oth-2505.20730" aria-labelledby="oth-2505.20730">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Do LLMs Understand Collaborative Signals? Diagnosis and Repair
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pouryousef,+S">Shahrooz Pouryousef</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Montazeralghaem,+A">Ali Montazeralghaem</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Information Retrieval (cs.IR)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          Collaborative information from user-item interactions is a fundamental source of signal in successful recommender systems. Recently, researchers have attempted to incorporate this knowledge into large language model-based recommender approaches (LLMRec) to enhance their performance. However, there has been little fundamental analysis of whether LLMs can effectively reason over collaborative information. In this paper, we analyze the ability of LLMs to reason about collaborative information in recommendation tasks, comparing their performance to traditional matrix factorization (MF) models. We propose a simple and effective method to improve LLMs&#39; reasoning capabilities using retrieval-augmented generation (RAG) over the user-item interaction matrix with four different prompting strategies. Our results show that the LLM outperforms the MF model whenever we provide relevant information in a clear and easy-to-follow format, and prompt the LLM to reason based on it. We observe that with this strategy, in almost all cases, the more information we provide, the better the LLM performs.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item137'>[137]</a>
      <a href ="/abs/2506.04245" title="Abstract" id="2506.04245">
        arXiv:2506.04245
      </a>
          (replaced)

        [<a href="/pdf/2506.04245" title="Download PDF" id="pdf-2506.04245" aria-labelledby="pdf-2506.04245">pdf</a>, <a href="https://arxiv.org/html/2506.04245v4" title="View HTML" id="html-2506.04245" aria-labelledby="html-2506.04245" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2506.04245" title="Other formats" id="oth-2506.04245" aria-labelledby="oth-2506.04245">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Contextual Integrity in LLMs via Reasoning and Reinforcement Learning
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lan,+G">Guangchen Lan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Inan,+H+A">Huseyin A. Inan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Abdelnabi,+S">Sahar Abdelnabi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kulkarni,+J">Janardhan Kulkarni</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wutschitz,+L">Lukas Wutschitz</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shokri,+R">Reza Shokri</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Brinton,+C+G">Christopher G. Brinton</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sim,+R">Robert Sim</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          39th Conference on Neural Information Processing Systems (NeurIPS 2025)
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          As the era of autonomous agents making decisions on behalf of users unfolds, ensuring contextual integrity (CI) -- what is the appropriate information to share while carrying out a certain task -- becomes a central question to the field. We posit that CI demands a form of reasoning where the agent needs to reason about the context in which it is operating. To test this, we first prompt LLMs to reason explicitly about CI when deciding what information to disclose. We then extend this approach by developing a reinforcement learning (RL) framework that further instills in models the reasoning necessary to achieve CI. Using a synthetic, automatically created, dataset of only $\sim700$ examples but with diverse contexts and information disclosure norms, we show that our method substantially reduces inappropriate information disclosure while maintaining task performance across multiple model sizes and families. Importantly, improvements transfer from this synthetic dataset to established CI benchmarks such as PrivacyLens that has human annotations and evaluates privacy leakage of AI assistants in actions and tool calls. Our code is available at: <a href="https://github.com/EricGLan/CI-RL" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </p>
      </div>
    </dd>
    <dt>
      <a name='item138'>[138]</a>
      <a href ="/abs/2508.13142" title="Abstract" id="2508.13142">
        arXiv:2508.13142
      </a>
          (replaced)

        [<a href="/pdf/2508.13142" title="Download PDF" id="pdf-2508.13142" aria-labelledby="pdf-2508.13142">pdf</a>, <a href="/format/2508.13142" title="Other formats" id="oth-2508.13142" aria-labelledby="oth-2508.13142">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Holistic Evaluation of Multimodal LLMs on Spatial Intelligence
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cai,+Z">Zhongang Cai</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yubo Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+Q">Qingping Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+R">Ruisi Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+C">Chenyang Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yin,+W">Wanqi Yin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+Z">Zhiqian Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+Z">Zhitao Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wei,+C">Chen Wei</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qian,+O">Oscar Qian</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Pang,+H+E">Hui En Pang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Shi,+X">Xuanke Shi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+K">Kewang Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Han,+X">Xiaoyang Han</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z">Zukai Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+J">Jiaqi Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+X">Xiangyu Fan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+H">Hanming Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lu,+L">Lewei Lu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+B">Bo Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Z">Ziwei Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Q">Quan Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Lin,+D">Dahua Lin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+L">Lei Yang</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Codebase: <a href="https://github.com/EvolvingLMMs-Lab/EASI/" rel="external noopener nofollow" class="link-external link-https">this https URL</a> ; Leaderboard: <a href="https://huggingface.co/spaces/lmms-lab-si/EASI-Leaderboard" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Computer Vision and Pattern Recognition (cs.CV)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG); Multimedia (cs.MM); Robotics (cs.RO)
        </div>

        <p class='mathjax'>
          Multimodal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, the very capability that anchors artificial general intelligence in the physical world. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models (GPT, Gemini, Grok, Seed, Qwen, and Intern) stand on the path toward spatial intelligence (SI). We thus propose EASI for holistic Evaluation of multimodAl LLMs on Spatial Intelligence. EASI conceptualizes a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and a growing collection of newly curated ones, enabling systematic evaluation of state-of-the-art models. In this report, we conduct the study across eight key benchmarks, at a cost exceeding ten billion total tokens. Our empirical study then reveals that (1) GPT-5 demonstrates unprecedented strength in SI, yet (2) still falls short of human performance significantly across a broad spectrum of SI-tasks. Moreover, we (3) show that SI-tasks expose greater model capability deficiency than non-SI tasks, to the extent that (4) proprietary models do not exhibit a decisive advantage when facing the most difficult ones. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans, yet fail the most advanced multimodal models. EASI is an ongoing community effort: we have open-sourced the EASI codebase that provides a one-stop and reproducible solution with standardized interfaces, integrated protocols and prompts that significantly reduce the friction of configuring and running multiple benchmarks; we have also launched an accompanying EASI leaderboard to provide a continually updated snapshot of model performance across the full SI spectrum, accelerating collective progress toward robust SI.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item139'>[139]</a>
      <a href ="/abs/2509.09009" title="Abstract" id="2509.09009">
        arXiv:2509.09009
      </a>
          (replaced)

        [<a href="/pdf/2509.09009" title="Download PDF" id="pdf-2509.09009" aria-labelledby="pdf-2509.09009">pdf</a>, <a href="https://arxiv.org/html/2509.09009v3" title="View HTML" id="html-2509.09009" aria-labelledby="html-2509.09009" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2509.09009" title="Other formats" id="oth-2509.09009" aria-labelledby="oth-2509.09009">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Open-sci-ref-0.01: open and reproducible reference baselines for language model and dataset comparison
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nezhurina,+M">Marianna Nezhurina</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Franke,+J">Jrg Franke</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nakamura,+T">Taishi Nakamura</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Carstensen,+T">Timur Carstensen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ajroldi,+N">Niccol Ajroldi</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Komulainen,+V">Ville Komulainen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Salinas,+D">David Salinas</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Jitsev,+J">Jenia Jitsev</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          v.1.1. AAAI Workshop on Reproducible Artificial Intelligence (RAI, <a href="https://reproducibleai.github.io" rel="external noopener nofollow" class="link-external link-https">this https URL</a>) 2026, camera ready version. Model weights and intermediate training checkpoints are available at <a href="https://huggingface.co/collections/open-sci/open-sci-ref-001;" rel="external noopener nofollow" class="link-external link-https">this https URL</a> code for reproducing training, evaluation and raw experiments data at <a href="https://github.com/LAION-AI/open-sci-ref-0.01" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          We introduce open-sci-ref, a family of dense transformer models trained as research baselines across multiple model (0.13B to 1.7B parameters) and token scales (up to 1T) on 8 recent open reference datasets. Evaluating the models on various standardized benchmarks, our training runs set establishes reference points that enable researchers to assess the sanity and quality of alternative training approaches across scales and datasets. Intermediate checkpoints allow comparison and studying of the training dynamics. The established reference baselines allow training procedures to be compared through their scaling trends, aligning them on a common compute axis. Comparison of open reference datasets reveals that training on NemoTron-CC HQ consistently outperforms other reference datasets, followed by DCLM-baseline and FineWeb-Edu. In addition to intermediate training checkpoints, the release includes logs, code, and downstream evaluations to simplify reproduction, standardize comparison, and facilitate future research.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item140'>[140]</a>
      <a href ="/abs/2510.26745" title="Abstract" id="2510.26745">
        arXiv:2510.26745
      </a>
          (replaced)

        [<a href="/pdf/2510.26745" title="Download PDF" id="pdf-2510.26745" aria-labelledby="pdf-2510.26745">pdf</a>, <a href="https://arxiv.org/html/2510.26745v2" title="View HTML" id="html-2510.26745" aria-labelledby="html-2510.26745" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2510.26745" title="Other formats" id="oth-2510.26745" aria-labelledby="oth-2510.26745">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Deep sequence models tend to memorize geometrically; it is unclear why
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Noroozizadeh,+S">Shahriar Noroozizadeh</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Nagarajan,+V">Vaishnavh Nagarajan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Rosenfeld,+E">Elan Rosenfeld</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kumar,+S">Sanjiv Kumar</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL); Machine Learning (stat.ML)
        </div>

        <p class='mathjax'>
          Deep sequence models are said to store atomic facts predominantly in the form of associative memory: a brute-force lookup of co-occurring entities. We identify a dramatically different form of storage of atomic facts that we term as geometric memory. Here, the model has synthesized embeddings encoding novel global relationships between all entities, including ones that do not co-occur in training. Such storage is powerful: for instance, we show how it transforms a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn $1$-step navigation task.
<br>From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, as against a lookup of local associations, cannot be straightforwardly attributed to typical supervisory, architectural, or optimizational pressures. Counterintuitively, a geometry is learned even when it is more complex than the brute-force lookup.
<br>Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points out to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery, and unlearning.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item141'>[141]</a>
      <a href ="/abs/2512.06205" title="Abstract" id="2512.06205">
        arXiv:2512.06205
      </a>
          (replaced)

        [<a href="/pdf/2512.06205" title="Download PDF" id="pdf-2512.06205" aria-labelledby="pdf-2512.06205">pdf</a>, <a href="https://arxiv.org/html/2512.06205v2" title="View HTML" id="html-2512.06205" aria-labelledby="html-2512.06205" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.06205" title="Other formats" id="oth-2512.06205" aria-labelledby="oth-2512.06205">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          On measuring grounding and generalizing grounding problems
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Quigley,+D">Daniel Quigley</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Maynard,+E">Eric Maynard</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          resubmission: 39 pages, 85 sources, 3 figures
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Machine Learning (cs.LG)
        </div>

        <p class='mathjax'>
          The symbol grounding problem asks how tokens like cat can be about cats, as opposed to mere shapes manipulated in a calculus. We recast grounding from a binary judgment into an audit across desiderata, each indexed by an evaluation tuple (context, meaning type, threat model, reference distribution): authenticity (mechanisms reside inside the agent and, for strong claims, were acquired through learning or evolution); preservation (atomic meanings remain intact); faithfulness, both correlational (realized meanings match intended ones) and etiological (internal mechanisms causally contribute to success); robustness (graceful degradation under declared perturbations); compositionality (the whole is built systematically from the parts). We apply this framework to four grounding modes (symbolic; referential; vectorial; relational) and three case studies: model-theoretic semantics achieves exact composition but lacks etiological warrant; large language models show correlational fit and local robustness for linguistic tasks, yet lack selection-for-success on world tasks without grounded interaction; human language meets the desiderata under strong authenticity through evolutionary and developmental acquisition. By operationalizing a philosophical inquiry about representation, we equip philosophers of science, computer scientists, linguists, and mathematicians with a common language and technical framework for systematic investigation of grounding and meaning.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item142'>[142]</a>
      <a href ="/abs/2512.07805" title="Abstract" id="2512.07805">
        arXiv:2512.07805
      </a>
          (replaced)

        [<a href="/pdf/2512.07805" title="Download PDF" id="pdf-2512.07805" aria-labelledby="pdf-2512.07805">pdf</a>, <a href="/format/2512.07805" title="Other formats" id="oth-2512.07805" aria-labelledby="oth-2512.07805">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Group Representational Position Encoding
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Y">Yifan Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Z">Zixiang Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">Yifeng Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qin,+Z">Zhen Qin</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+H">Huizhuo Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+K">Kangping Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+Y">Yang Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gu,+Q">Quanquan Gu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+A+C">Andrew Chi-Chih Yao</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Project Page: <a href="https://github.com/model-architectures/GRAPE" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Machine Learning (cs.LG)</span>; Artificial Intelligence (cs.AI); Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          We present GRAPE (Group RepresentAtional Position Encoding), a unified framework for positional encoding based on group actions. GRAPE brings together two families of mechanisms: (i) multiplicative rotations (Multiplicative GRAPE) in $\mathrm{SO}(d)$ and (ii) additive logit biases (Additive GRAPE) arising from unipotent actions in the general linear group $\mathrm{GL}$. In Multiplicative GRAPE, a position $n \in \mathbb{Z}$ (or $t \in \mathbb{R}$) acts as $\mathbf{G}(n)=\exp(n\,\omega\,\mathbf{L})$ with a rank-2 skew generator $\mathbf{L} \in \mathbb{R}^{d \times d}$, yielding a relative, compositional, norm-preserving map with a closed-form matrix exponential. RoPE is recovered exactly when the $d/2$ planes are the canonical coordinate pairs with log-uniform spectrum. Learned commuting subspaces and compact non-commuting mixtures strictly extend this geometry to capture cross-subspace feature coupling at $O(d)$ and $O(r d)$ cost per head, respectively. In Additive GRAPE, additive logits arise as rank-1 (or low-rank) unipotent actions, recovering ALiBi and the Forgetting Transformer (FoX) as exact special cases while preserving an exact relative law and streaming cacheability. Altogether, GRAPE supplies a principled design space for positional geometry in long-context models, subsuming RoPE and ALiBi as special cases. Project Page: <a href="https://github.com/model-architectures/GRAPE" rel="external noopener nofollow" class="link-external link-https">this https URL</a>.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item143'>[143]</a>
      <a href ="/abs/2512.22334" title="Abstract" id="2512.22334">
        arXiv:2512.22334
      </a>
          (replaced)

        [<a href="/pdf/2512.22334" title="Download PDF" id="pdf-2512.22334" aria-labelledby="pdf-2512.22334">pdf</a>, <a href="/format/2512.22334" title="Other formats" id="oth-2512.22334" aria-labelledby="oth-2512.22334">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          SciEvalKit: An Open-source Evaluation Toolkit for Scientific General Intelligence
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yiheng Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+Y">Yixin Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Li,+S">Shuo Li</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Y">Yifan Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+B">Bo Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+H">Hengjian Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yuan,+J">Jiakang Yuan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bu,+J">Jia Bu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+W">Wanghan Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Y">Yuhao Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+X">Xiangyu Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+Z">Zhiwang Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+F">Fengxiang Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Duan,+H">Haodong Duan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Songyang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+J">Jun Yao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Deng,+H">Han Deng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+Y">Yizhou Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xiao,+J">Jiabei Xiao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+J">Jiaqi Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Su,+E">Encheng Su</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+Y">Yujie Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+W">Weida Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yao,+J">Junchi Yao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zheng,+S">Shenghe Zheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Sun,+H">Haoran Sun</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ma,+R">Runmin Ma</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yan,+X">Xiangchao Yan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+B">Bo Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+D">Dongzhan Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+S">Shufei Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Ye,+P">Peng Ye</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+X">Xiaosong Wang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Tang,+S">Shixiang Tang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+W">Wenlong Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Bai,+L">Lei Bai</a></div>



        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          We introduce SciEvalKit, a unified benchmarking toolkit designed to evaluate AI models for science across a broad range of scientific disciplines and task capabilities. Unlike general-purpose evaluation platforms, SciEvalKit focuses on the core competencies of scientific intelligence, including Scientific Multimodal Perception, Scientific Multimodal Reasoning, Scientific Multimodal Understanding, Scientific Symbolic Reasoning, Scientific Code Generation, Science Hypothesis Generation and Scientific Knowledge Understanding. It supports six major scientific domains, spanning from physics and chemistry to astronomy and materials science. SciEvalKit builds a foundation of expert-grade scientific benchmarks, curated from real-world, domain-specific datasets, ensuring that tasks reflect authentic scientific challenges. The toolkit features a flexible, extensible evaluation pipeline that enables batch evaluation across models and datasets, supports custom model and dataset integration, and provides transparent, reproducible, and comparable results. By bridging capability-based evaluation and disciplinary diversity, SciEvalKit offers a standardized yet customizable infrastructure to benchmark the next generation of scientific foundation models and intelligent agents. The toolkit is open-sourced and actively maintained to foster community-driven development and progress in AI4Science.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item144'>[144]</a>
      <a href ="/abs/2512.22933" title="Abstract" id="2512.22933">
        arXiv:2512.22933
      </a>
          (replaced)

        [<a href="/pdf/2512.22933" title="Download PDF" id="pdf-2512.22933" aria-labelledby="pdf-2512.22933">pdf</a>, <a href="https://arxiv.org/html/2512.22933v2" title="View HTML" id="html-2512.22933" aria-labelledby="html-2512.22933" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.22933" title="Other formats" id="oth-2512.22933" aria-labelledby="oth-2512.22933">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          Multimodal Fact-Checking: An Agent-based Approach
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Xu,+D">Danni Xu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Fan,+S">Shaojing Fan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Cheng,+H">Harry Cheng</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Kankanhalli,+M">Mohan Kankanhalli</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Code and dataset will be released at <a href="https://github.com/xudanni0927/AgentFact" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL)
        </div>

        <p class='mathjax'>
          The rapid spread of multimodal misinformation poses a growing challenge for automated fact-checking systems. Existing approaches, including large vision language models (LVLMs) and deep multimodal fusion methods, often fall short due to limited reasoning and shallow evidence utilization. A key bottleneck is the lack of dedicated datasets that provide complete real-world multimodal misinformation instances accompanied by annotated reasoning processes and verifiable evidence. To address this limitation, we introduce RW-Post, a high-quality and explainable dataset for real-world multimodal fact-checking. RW-Post aligns real-world multimodal claims with their original social media posts, preserving the rich contextual information in which the claims are made. In addition, the dataset includes detailed reasoning and explicitly linked evidence, which are derived from human written fact-checking articles via a large language model assisted extraction pipeline, enabling comprehensive verification and explanation. Building upon RW-Post, we propose AgentFact, an agent-based multimodal fact-checking framework designed to emulate the human verification workflow. AgentFact consists of five specialized agents that collaboratively handle key fact-checking subtasks, including strategy planning, high-quality evidence retrieval, visual analysis, reasoning, and explanation generation. These agents are orchestrated through an iterative workflow that alternates between evidence searching and task-aware evidence filtering and reasoning, facilitating strategic decision-making and systematic evidence analysis. Extensive experimental results demonstrate that the synergy between RW-Post and AgentFact substantially improves both the accuracy and interpretability of multimodal fact-checking.
        </p>
      </div>
    </dd>
    <dt>
      <a name='item145'>[145]</a>
      <a href ="/abs/2512.23328" title="Abstract" id="2512.23328">
        arXiv:2512.23328
      </a>
          (replaced)

        [<a href="/pdf/2512.23328" title="Download PDF" id="pdf-2512.23328" aria-labelledby="pdf-2512.23328">pdf</a>, <a href="https://arxiv.org/html/2512.23328v2" title="View HTML" id="html-2512.23328" aria-labelledby="html-2512.23328" rel="noopener noreferrer" target="_blank">html</a>, <a href="/format/2512.23328" title="Other formats" id="oth-2512.23328" aria-labelledby="oth-2512.23328">other</a>]
    </dt>
    <dd>
      <div class='meta'>
        <div class='list-title mathjax'><span class='descriptor'>Title:</span>
          CubeBench: Diagnosing Interactive, Long-Horizon Spatial Reasoning Under Partial Observations
        </div>
        <div class='list-authors'><a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Gao,+H">Huan-ang Gao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhang,+Z">Zikang Zhang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Luo,+T">Tianwei Luo</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Yang,+K">Kaisen Yang</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Juan,+X">Xinzhe Juan</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Qiu,+J">Jiahao Qiu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Chen,+T">Tianxing Chen</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=He,+B">Bingxiang He</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhao,+H">Hao Zhao</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Zhou,+H">Hao Zhou</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Liu,+S">Shilong Liu</a>, <a href="https://arxiv.org/search/cs?searchtype=author&amp;query=Wang,+M">Mengdi Wang</a></div>

        <div class='list-comments mathjax'><span class='descriptor'>Comments:</span>
          Webpage: <a href="https://cubebench.c7w.tech/" rel="external noopener nofollow" class="link-external link-https">this https URL</a>
        </div>


        <div class='list-subjects'><span class='descriptor'>Subjects:</span>
          <span class="primary-subject">Artificial Intelligence (cs.AI)</span>; Computation and Language (cs.CL); Computer Vision and Pattern Recognition (cs.CV)
        </div>

        <p class='mathjax'>
          Large Language Model (LLM) agents, while proficient in the digital realm, face a significant gap in physical-world deployment due to the challenge of forming and maintaining a robust spatial mental model. We identify three core cognitive challenges hindering this transition: spatial reasoning, long-horizon state tracking via mental simulation, and active exploration under partial observation. To isolate and evaluate these faculties, we introduce CubeBench, a novel generative benchmark centered on the Rubik&#39;s Cube. CubeBench uses a three-tiered diagnostic framework that progressively assesses agent capabilities, from foundational state tracking with full symbolic information to active exploration with only partial visual data. Our experiments on leading LLMs reveal critical limitations, including a uniform 0.00% pass rate on all long-horizon tasks, exposing a fundamental failure in long-term planning. We also propose a diagnostic framework to isolate these cognitive bottlenecks by providing external solver tools. By analyzing the failure modes, we provide key insights to guide the development of more physically-grounded intelligent agents.
        </p>
      </div>
    </dd>
</dl>




      <div class='paging'>Total of 145 entries 
    </div>
    <div class='morefewer'>Showing up to 2000 entries per page:
          <a href=/list/cs.CL/new?skip=0&amp;show=1000 rel="nofollow">
      fewer</a>
 |
          <span style="color: #454545">more</span>
 |
          <span style="color: #454545">all</span>

    </div>

</div>
</div>      </div>
    </main>

    <footer style="clear: both;">
      <div class="columns is-desktop" role="navigation" aria-label="Secondary" style="margin: -0.75em -0.75em 0.75em -0.75em">
        <!-- Macro-Column 1 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/about">About</a></li>
                <li><a href="https://info.arxiv.org/help">Help</a></li>
              </ul>
            </div>
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
                  <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
                </li>
                <li>
                  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
                  <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
                </li>
              </ul>
            </div>
          </div>
        </div>
        <!-- End Macro-Column 1 -->
        <!-- Macro-Column 2 -->
        <div class="column" style="padding: 0;">
          <div class="columns">
            <div class="column">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
                <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
              </ul>
            </div>
            <div class="column sorry-app-links">
              <ul style="list-style: none; line-height: 2;">
                <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
                <li>
                  <p class="help">
                    <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
                  </p>
                </li>
              </ul>
            </div>
          </div>
        </div> <!-- end MetaColumn 2 -->
        <!-- End Macro-Column 2 -->
      </div>
    </footer>
  </div>

  <script src="/static/base/1.0.1/js/member_acknowledgement.js"></script>

</body>

</html>